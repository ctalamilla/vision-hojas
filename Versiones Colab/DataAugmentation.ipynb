{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "chL8_qaDedEx",
   "metadata": {
    "id": "chL8_qaDedEx"
   },
   "source": [
    "# Inicializaci√≥n collab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "J2Hj0mHSeZWd",
   "metadata": {
    "executionInfo": {
     "elapsed": 7613,
     "status": "ok",
     "timestamp": 1745092969927,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "J2Hj0mHSeZWd"
   },
   "outputs": [],
   "source": [
    "# Importaci√≥n de librer√≠as\n",
    "# Gesti√≥n de archivos y reporte\n",
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "import yaml\n",
    "\n",
    "# Manipulaci√≥n y an√°lisis de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Procesamiento de im√°genes\n",
    "from PIL import Image\n",
    "\n",
    "# Machine Learning\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a38c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa51e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el dataframe desde el .CSV y definimos 'id' como √≠ndice\n",
    "try:\n",
    "    df_split = pd.read_csv('/content/drive/MyDrive/CV2-PlantVillage/dataframe_splitted.csv').set_index('id')\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ö†Ô∏è Error: El archivo 'dataframe.csv' no se encontr√≥ en la ubicaci√≥n actual: {os.getcwd()}\")\n",
    "    print(\"üö® Se crear√° nuevamente al correr las celdas de 'Importaci√≥n de im√°genes' üö®.\")\n",
    "    df_split = None\n",
    "except Exception as e:\n",
    "    print(f\"Ocurri√≥ un error al leer el archivo CSV: {e}\")\n",
    "    df_split = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68fa3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qbCsuWZYdN-x",
   "metadata": {
    "id": "qbCsuWZYdN-x"
   },
   "source": [
    "#### Descarga de dataset de Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DmFevBwtdMNr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6004,
     "status": "ok",
     "timestamp": 1745092979157,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "DmFevBwtdMNr",
    "outputId": "cf2db65e-42eb-4fb1-bbd4-8f52d89f585f"
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "DATASET_PATH = kagglehub.dataset_download(\"abdallahalidev/plantvillage-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3GQwkDS8dTjg",
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1745092979181,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "3GQwkDS8dTjg"
   },
   "outputs": [],
   "source": [
    "# Ruta de acceso al dataset\n",
    "ROOT_DIR = f'{DATASET_PATH}/plantvillage dataset/color'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tJ2g6SaneIRu",
   "metadata": {
    "id": "tJ2g6SaneIRu"
   },
   "source": [
    "# Dataset split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7y7sAZg4-I",
   "metadata": {
    "id": "0f7y7sAZg4-I"
   },
   "source": [
    "### Funciones necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NBH0TuGQeUwU",
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1745092979204,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "NBH0TuGQeUwU"
   },
   "outputs": [],
   "source": [
    "def create_ignore_function(df, train_label, filename_col='filename'):\n",
    "    \"\"\"\n",
    "    Crea y devuelve la funci√≥n 'ignore_files' que tiene acceso al DataFrame\n",
    "    y sabe qu√© archivos mantener.\n",
    "    \"\"\"\n",
    "    # Crea un conjunto (set) con los nombres de archivo que S√ç queremos copiar (ej: split == 'train')\n",
    "    # Usa este conjunto para hacer la b√∫squeda de forma mucho m√°s r√°pida\n",
    "    files_to_keep = set(df[df['split'] == train_label][filename_col])\n",
    "    #print(f\"Archivos a mantener (split='{train_label}'): {files_to_keep}\") # Debugging\n",
    "\n",
    "    def ignore_files(current_dir, files_in_current_dir):\n",
    "        \"\"\"\n",
    "        Funci√≥n que ser√° llamada por shutil.copytree.\n",
    "        Decide qu√© archivos/directorios ignorar en el directorio actual.\n",
    "        \"\"\"\n",
    "        ignore_list = []\n",
    "        for item in files_in_current_dir:\n",
    "            # Construye la ruta completa para verificar si es archivo o directorio\n",
    "            full_path = os.path.join(current_dir, item)\n",
    "\n",
    "            # Aplicar la l√≥gica de ignorar SOLO los ARCHIVOS de la lista\n",
    "            if os.path.isfile(full_path):\n",
    "                # Si el nombre del archivo NO est√° en el conjunto de archivos a mantener,\n",
    "                # entonces lo agrega a la lista de ignorados.\n",
    "                if item not in files_to_keep:\n",
    "                    # print(f\"Ignorando archivo: {item} (en {current_dir})\") # Debugging\n",
    "                    ignore_list.append(item)\n",
    "\n",
    "        # print(f\"Directorio: {current_dir}, Ignorando: {ignore_list}\") # Debugging\n",
    "        return ignore_list\n",
    "\n",
    "    # Devuelve la funci√≥n 'ignore_files' configurada\n",
    "    return ignore_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LCGi7EMeg_Ys",
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1745092979212,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "LCGi7EMeg_Ys"
   },
   "outputs": [],
   "source": [
    "import os, re\n",
    "\n",
    "# Busca la carpeta ra√≠z del dataset en el directorio donde fue descargado\n",
    "def find_path(folder):\n",
    "    match = re.search(fr\"^(.*?)/{folder}/\", DATASET_PATH)\n",
    "    if match:\n",
    "        prefix = match.group(1)\n",
    "        path = os.path.join(prefix, f\"{folder}/\")\n",
    "        return path\n",
    "    else:\n",
    "        print(f'No se ha podido encontrar la carpeta \"{folder}\" en {DATASET_PATH}')\n",
    "        return None\n",
    "# Carga de imagenes en memoria y visualizaci√≥n\n",
    "def load_image(data: pd.DataFrame, index: int, root: str=ROOT_DIR):\n",
    "    \"\"\"\n",
    "    Carga una imagen PIL desde una fila espec√≠fica de un DataFrame.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): El DataFrame que contiene las rutas de las im√°genes.\n",
    "        index (int): El √≠ndice de la fila en el DataFrame para cargar la imagen.\n",
    "        root_dir (str): El directorio ra√≠z donde se encuentran las im√°genes.\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image.Image: La imagen cargada como un objeto PIL.Image, o None si ocurre un error.\n",
    "    \"\"\"\n",
    "    if index < 0 or index >= len(data):\n",
    "        print(\"√çndice fuera de rango.\")\n",
    "        return None\n",
    "\n",
    "    row = data.iloc[index]\n",
    "    relative_path = row['image_path']\n",
    "    filename = row['filename']\n",
    "    full_path = os.path.join(root, relative_path, filename)\n",
    "\n",
    "    try:\n",
    "        img = Image.open(full_path)\n",
    "        return img\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Archivo no encontrado: {full_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar la imagen: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZuUqo9QYg8Uz",
   "metadata": {
    "id": "ZuUqo9QYg8Uz"
   },
   "source": [
    "## Split de archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lPO4I3q2gtkm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1745092979227,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "lPO4I3q2gtkm",
    "outputId": "d4c42ce3-ff22-435f-fe1a-1e1fdc1e6a8a"
   },
   "outputs": [],
   "source": [
    "# Guarda directorio del dataset dividido\n",
    "path = find_path(\"plantvillage-dataset\")\n",
    "DATASETS_ROOT = path\n",
    "SPLITTED_PATH = f\"{path}splitted/\" if path else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4pjsa6cGgh0k",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 92,
     "status": "ok",
     "timestamp": 1745092979320,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "4pjsa6cGgh0k",
    "outputId": "18059699-5dde-4920-df69-d06223eabc9a"
   },
   "outputs": [],
   "source": [
    "splits = df_split['split'].value_counts().index.tolist()\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pjOkWylvgk5W",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 296583,
     "status": "ok",
     "timestamp": 1745093275904,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "pjOkWylvgk5W",
    "outputId": "4f378852-50b5-40b2-da17-738f1f673746"
   },
   "outputs": [],
   "source": [
    "verfication = True # Ejecuta el proceso de verificaci√≥n (punto 2)\n",
    "\n",
    "print(f\"Se inicia proceso de copiado del dataset‚Ä¶\")\n",
    "total_files = len(df_split) # Total de archivos del dataset\n",
    "print(f\" - Total de archivos en el dataset: {total_files}\")\n",
    "\n",
    "# Realiza el proceso de copiado de archivos para cada split\n",
    "succeeded_process = True\n",
    "for split in splits:\n",
    "    # Crea las rutas de origen y destino\n",
    "    # (Ejemplo: 'train', 'test', 'valid')\n",
    "    print(f\"\\n\\nIniciando proceso para '{split}' split ‚Ä¶\")\n",
    "    source_folder = f'{ROOT_DIR}/'\n",
    "    destination_folder = f'{SPLITTED_PATH}{split}/'\n",
    "    total_split = len(df_split[df_split['split'] == split]) # Total de archivos del split\n",
    "    # Se omite verificaci√≥n de existencia del dataset (porque se crea siempre desde cero)\n",
    "    print(f\"üîÑ Procesando split '{split.upper()}' ({(total_split/total_files*100):.2f}):\")\n",
    "    print(f\"  - Total de archivos a copiar: {total_split}\")\n",
    "    succeeded = False\n",
    "\n",
    "    try:\n",
    "        print(f\"1. Creando estructura de subcarpetas:\")\n",
    "        # 1. Crea la funci√≥n para ignorar espec√≠fica para el split a procesar\n",
    "        ignore_function = create_ignore_function(df_split, train_label=split, filename_col='filename')\n",
    "        print(f\"    ‚úî Funci√≥n de filtro creada para el split \")\n",
    "\n",
    "        # 2. Con copytree copia todo el \"√°rbol\" de directorios (careptas y subcarpetas)\n",
    "        # Fitrando con ignore_function todos aquellos archivos que no pertenecen al split deseado\n",
    "        print(f\"    ‚àû Copiando contenido del dataset (puede demorar hasta un minuto).\")\n",
    "        shutil.copytree(source_folder, destination_folder, ignore=ignore_function)\n",
    "        print(f\"    ‚úî Proceso de copiado del split finalizado.\")\n",
    "\n",
    "        if verfication:\n",
    "            # Verifica qu√© se haya copiado adecuadamente (opcional pero √∫til)\n",
    "            print(f\"2. Se inicia proceso de verificaci√≥n‚Ä¶\")\n",
    "            copied_files = []\n",
    "            for root, dirs, files_in_dest in os.walk(destination_folder):\n",
    "                for name in files_in_dest:\n",
    "                    copied_files.append(os.path.join(os.path.relpath(root, destination_folder), name).replace('\\\\', '/')) # Normalizar path\n",
    "                    #print(f\"  - {os.path.join(root, name)}\") # Debuggin\n",
    "            print(f\"    ‚úî Se crearon un total de {len(os.listdir(destination_folder))} carpetas (para las clases).\")\n",
    "            print(f\"    ‚úî Se copiaron un total de {len(copied_files)} archivos ({len(copied_files)/total_split*100:.2f}%)\")\n",
    "            # Agregar confirmaci√≥n de igualdad cantidad split == copiados\n",
    "            if len(copied_files) == total_split:\n",
    "                print(f\"‚úÖ Se complet√≥ satisfactoriamente el subproceso de copiado para el split.\\n\")\n",
    "                succeeded = True\n",
    "            else:\n",
    "                print(f\"‚ùå Error: No se pudo copiar correctamente el split '{split.upper()}'\\n\")\n",
    "                succeeded = False\n",
    "        else:\n",
    "            succeeded = True # Si la verificaci√≥n est√° desactivada, se asume que el proceso fue exitoso\n",
    "\n",
    "    except FileExistsError:\n",
    "        print(f\"Error: La carpeta de destino '{destination_folder}' ya existe.\\n\")\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurri√≥ un error inesperado: {e}\\n\")\n",
    "\n",
    "    succeeded_process *= succeeded # Actualiza el estado del proceso\n",
    "    # (S√≥lo es 'True' si todos los splits se copian correctamente)\n",
    "\n",
    "if succeeded_process:\n",
    "    print(\"\\n\\nüåü El proceso de copiado del dataset ha finalizado con √©xito.\\n\")\n",
    "else:\n",
    "    print(\"\\n\\nüö´ No se pudo completar satisfactoriamente el proceso de copiado del dataset.\\nVerificar que se haya completado la eliminaci√≥n de las carpetas.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oklHK_5oeHUh",
   "metadata": {
    "id": "oklHK_5oeHUh"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6b725e",
   "metadata": {
    "id": "2b6b725e"
   },
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60ec1ca",
   "metadata": {
    "executionInfo": {
     "elapsed": 4861,
     "status": "ok",
     "timestamp": 1745093280766,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "a60ec1ca"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "import random\n",
    "import yaml\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fff79c0",
   "metadata": {
    "id": "1fff79c0"
   },
   "source": [
    "### Funciones necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e91e15",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1745093280770,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "f5e91e15"
   },
   "outputs": [],
   "source": [
    "def find_folder(path):\n",
    "    return os.path.basename(os.path.dirname(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf311112",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1745093280774,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "bf311112"
   },
   "outputs": [],
   "source": [
    "# Carga de imagenes en memoria y visualizaci√≥n\n",
    "def load_image(data: pd.DataFrame, index: int, root: str=ROOT_DIR):\n",
    "    \"\"\"\n",
    "    Carga una imagen PIL desde una fila espec√≠fica de un DataFrame.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): El DataFrame que contiene las rutas de las im√°genes.\n",
    "        index (int): El √≠ndice de la fila en el DataFrame para cargar la imagen.\n",
    "        root_dir (str): El directorio ra√≠z donde se encuentran las im√°genes.\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image.Image: La imagen cargada como un objeto PIL.Image, o None si ocurre un error.\n",
    "    \"\"\"\n",
    "    # if index < 0 or index >= len(data):\n",
    "    #     print(\"√çndice fuera de rango.\")\n",
    "    #     return None\n",
    "\n",
    "    row = data.iloc[index]\n",
    "    relative_path = row['image_path']\n",
    "    filename = row['filename']\n",
    "    full_path = os.path.join(root, relative_path, filename)\n",
    "\n",
    "    try:\n",
    "        img = Image.open(full_path)\n",
    "        return img\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Archivo no encontrado: {full_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar la imagen: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc754e7",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1745093280779,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "0fc754e7"
   },
   "outputs": [],
   "source": [
    "# Carga de imagenes en memoria y visualizaci√≥n\n",
    "def load_image_idx(data, root: str=ROOT_DIR):\n",
    "    \"\"\"\n",
    "    Carga una imagen PIL desde una fila espec√≠fica de un DataFrame.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): El DataFrame que contiene las rutas de las im√°genes.\n",
    "        index (int): El √≠ndice de la fila en el DataFrame para cargar la imagen.\n",
    "        root_dir (str): El directorio ra√≠z donde se encuentran las im√°genes.\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image.Image: La imagen cargada como un objeto PIL.Image, o None si ocurre un error.\n",
    "    \"\"\"\n",
    "    # if index < 0 or index >= len(data):\n",
    "    #     print(\"√çndice fuera de rango.\")\n",
    "    #     return None\n",
    "\n",
    "    row = data\n",
    "    relative_path = row['image_path']\n",
    "    filename = row['filename']\n",
    "    full_path = os.path.join(root, relative_path, filename)\n",
    "\n",
    "    try:\n",
    "        img = Image.open(full_path)\n",
    "        return img\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Archivo no encontrado: {full_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar la imagen: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e687d246",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1745093280785,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "e687d246"
   },
   "outputs": [],
   "source": [
    "def filter_classes(df, target, range: tuple = (0,None)):\n",
    "    lim_min, lim_max = range\n",
    "    counts = df[target].value_counts()\n",
    "    if lim_min == None:\n",
    "        lim_min  = 0\n",
    "    if lim_max == None:\n",
    "        lim_max  = counts.max()\n",
    "    #print(\"Limites:\", lim_min, lim_max)\n",
    "\n",
    "    selection = []\n",
    "    for idx, count in enumerate(counts):\n",
    "        if lim_min < count <= lim_max:\n",
    "            clase = counts.index[idx]\n",
    "            selection.append(clase)\n",
    "    return selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08959452",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1745093280797,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "08959452"
   },
   "outputs": [],
   "source": [
    "def numoji(numero):\n",
    "  \"\"\"\n",
    "  Convierte un n√∫mero entero del 1 al 10 a su emoji correspondiente.\n",
    "\n",
    "  Args:\n",
    "    numero: Un entero entre 1 y 10.\n",
    "\n",
    "  Returns:\n",
    "    Un string con el emoji correspondiente al n√∫mero, o \"0Ô∏è‚É£\" si el n√∫mero\n",
    "    est√° fuera del rango.\n",
    "  \"\"\"\n",
    "  if 1 <= numero <= 10:\n",
    "    emoji_map = {\n",
    "        1: \"1Ô∏è‚É£\",\n",
    "        2: \"2Ô∏è‚É£\",\n",
    "        3: \"3Ô∏è‚É£\",\n",
    "        4: \"4Ô∏è‚É£\",\n",
    "        5: \"5Ô∏è‚É£\",\n",
    "        6: \"6Ô∏è‚É£\",\n",
    "        7: \"7Ô∏è‚É£\",\n",
    "        8: \"8Ô∏è‚É£\",\n",
    "        9: \"9Ô∏è‚É£\",\n",
    "        10: \"üîü\"\n",
    "    }\n",
    "    return emoji_map[numero]\n",
    "  else:\n",
    "    return \"*Ô∏è‚É£\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ztZbyfR7eoBo",
   "metadata": {
    "id": "ztZbyfR7eoBo"
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e623eeec",
   "metadata": {
    "id": "e623eeec"
   },
   "source": [
    "## Transformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fd0978",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1745093280820,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "02fd0978"
   },
   "outputs": [],
   "source": [
    "# 1. Define las transformaciones de Albumentations\n",
    "transform = A.Compose([\n",
    "    A.Rotate(limit=180, p=0.5),          # Rotaci√≥n aleatoria hasta 180 grados con probabilidad 0.5\n",
    "    A.HorizontalFlip(p=0.5),            # Volteo horizontal con probabilidad 0.5\n",
    "    A.VerticalFlip(p=0.5),              # Volteo vertical con probabilidad 0.5\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.3), # Ajuste de brillo y contraste aleatorio\n",
    "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.3), # Ajuste de tono, saturaci√≥n y valor\n",
    "    A.RandomCrop(width=200, height=200, p=0.3), # Recorte aleatorio a 200x200\n",
    "    A.GaussianBlur(blur_limit=(3, 5), p=0.2),   # Desenfoque Gaussiano\n",
    "    A.RandomScale(scale_limit=0.1, p=0.3),      # Escalado aleatorio con un l√≠mite de 10%\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a829c814",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1745093280821,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "a829c814"
   },
   "outputs": [],
   "source": [
    "def apply_transformations(original_image):\n",
    "    # Convertir la imagen original a un array NumPy para la transformaci√≥n\n",
    "    original_image_array = np.array(original_image)\n",
    "\n",
    "    # Aplicar las transformaciones\n",
    "    transformed = transform(image=original_image_array)\n",
    "    return transformed['image']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83e98ce",
   "metadata": {
    "id": "a83e98ce"
   },
   "source": [
    "----\n",
    "# Procesamiento del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3ee624",
   "metadata": {
    "id": "5b3ee624"
   },
   "source": [
    "## Creaci√≥n de directorios y estructuras de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5985a44",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1745093280822,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "e5985a44"
   },
   "outputs": [],
   "source": [
    "# Se ha definido una nueva constante con la ubicaci√≥n del dataset aumentado (ser√° almacenadad en YAML)\n",
    "AUG_PATH = SPLITTED_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9895aa32",
   "metadata": {
    "id": "9895aa32"
   },
   "source": [
    "Luego, se aplican las transformaciones para aumentaci√≥n de datos a la copia del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe57e92",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "executionInfo": {
     "elapsed": 117,
     "status": "ok",
     "timestamp": 1745093280935,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "2fe57e92",
    "outputId": "d41f38cc-8e0b-4747-cb1f-332cc53c6737"
   },
   "outputs": [],
   "source": [
    "# Agrega columnas para indicar procesamiento\n",
    "## original -> imagen sin transofrmaciones\n",
    "## augmented -> existen aumentaciones derivadas\n",
    "df_split['augmented'] = False\n",
    "df_split['is_original'] = True\n",
    "\n",
    "# Crea un nuevo dataframe para gesitonar el procesamiento\n",
    "process_split = df_split[df_split['split']=='train'].copy()\n",
    "process_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71f579d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "executionInfo": {
     "elapsed": 138,
     "status": "ok",
     "timestamp": 1745093281074,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "e71f579d",
    "outputId": "a04b9347-52f4-4d06-be44-f86cc2bfbc9f"
   },
   "outputs": [],
   "source": [
    "# Crea una columna 'ref' que apunta al id de la imagen original\n",
    "process_split['ref'] = process_split.index\n",
    "process_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ab86f5",
   "metadata": {
    "id": "e5ab86f5"
   },
   "source": [
    "## Estrategia de aumentaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fe7baa",
   "metadata": {
    "id": "d4fe7baa"
   },
   "source": [
    "Dado que el dataset original est√° desbalanceado, se aplicar√° la aumentaci√≥n de datos s√≥lo a las clases minoritarias como forma de compensar el desbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b290c26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1745093281090,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "2b290c26",
    "outputId": "1487467d-c9cd-4449-de54-4b1a03464c81"
   },
   "outputs": [],
   "source": [
    "### Resumen distribucion por grupo\n",
    "print(\"Conteo por grupo:\")\n",
    "print(df_split['group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21916a4c",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1745093281095,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "21916a4c"
   },
   "outputs": [],
   "source": [
    "# Clases a las que se le aplicar√° aumentaci√≥n de datos\n",
    "# Entre 4000 y 2000\n",
    "group1 = {\n",
    "    'classes': filter_classes(df_split,'group',(2000,4000)),\n",
    "    'increase': 2 # 2x data augmentation (original + transf)\n",
    "}\n",
    "# Menores a 2000\n",
    "group2 = {\n",
    "    'classes': filter_classes(df_split,'group',(500,2000)),\n",
    "    'increase': 3 # 3x data augmentation (original +  2 transf)\n",
    "}\n",
    "\n",
    "# Clase m√≠nima (Raspberry)\n",
    "group3 = {\n",
    "    'classes': filter_classes(df_split,'group',(0,500)),\n",
    "    'increase': 6 # 6x data augmentation (original +  5 transf)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e4f2f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1745093281105,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "94e4f2f6",
    "outputId": "f8ccd04f-3f0a-402f-b063-bb1443fa425c"
   },
   "outputs": [],
   "source": [
    "print(\"Efecto de la aumentaci√≥n de datos para 'train' split\\n\")\n",
    "counts = df_split[df_split['split']=='train']['group'].value_counts()\n",
    "\n",
    "groups = [group1, group2, group3]\n",
    "for group in groups:\n",
    "    mult = group['increase']\n",
    "    print(f\"Estrategia de aumentaci√≥n {mult}x:\")\n",
    "    for clase in group['classes']:\n",
    "        print(f\" - {clase}: {counts[clase]} -> {counts[clase]*mult}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08d1a02",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1745093281110,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "c08d1a02"
   },
   "outputs": [],
   "source": [
    "def gen_rnd_id():\n",
    "    \"\"\"\n",
    "        Genera un n√∫mero aleatorio de 6 c√≠fras que se usar√° para diferenciar imagenes procesados de las originales\n",
    "    \"\"\"\n",
    "    rnd_seed = round(time.time() * 1e10) # Use clock as seed generator\n",
    "    random.seed(rnd_seed)  # Set this value as a new seed (assure better randomization)\n",
    "    sampled_numbers = random.sample(range(int(1e6)), 1)  # Generates a random number\n",
    "    return sampled_numbers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e100675",
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1745093281135,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "3e100675"
   },
   "outputs": [],
   "source": [
    "import humanize\n",
    "\n",
    "# Traducir humanize al espa√±ol\n",
    "try:\n",
    "    humanize.i18n.activate(\"es\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Paquete de idioma espa√±ol para humanize no encontrado, usando ingl√©s.\")\n",
    "    humanize.i18n.activate(\"en_US\") # Fallback a ingl√©s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G843YMV9vPt9",
   "metadata": {
    "id": "G843YMV9vPt9"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fRLtxPSWokw5",
   "metadata": {
    "id": "fRLtxPSWokw5"
   },
   "source": [
    "## Procesamiento de aumentaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985f8fc6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 415041,
     "status": "ok",
     "timestamp": 1745095167211,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "985f8fc6",
    "outputId": "a09392e9-a8e0-4d70-c4db-d8ce1d785fe3"
   },
   "outputs": [],
   "source": [
    "# PROCESAMIENTO DE AUMENTACIONES CON FILTRADO POR CLASE\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# SETUP INCIAL:\n",
    "verbose = True # Incluye detalle del proceso y estimaci√≥n de tiempo\n",
    "testing = False # No modifica archivos\n",
    "debugging = False # Incluye detalle de archivos y mensajes de error\n",
    "processes = [group1, group2, group3] # Estrategias a aplicar\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "for_processing_classes = [cls for group in groups for cls in group['classes']]\n",
    "for_processing_data = process_split[process_split['group'].isin(for_processing_classes)]\n",
    "n_transformations = 0\n",
    "for process in processes:\n",
    "    classes, increase = process['classes'], process['increase']\n",
    "    images_in_classes = for_processing_data[for_processing_data['group'].isin(classes)]\n",
    "    n_transformations += len(images_in_classes) * (increase - 1)\n",
    "\n",
    "print(f\"Iniciando proceso de aumentaci√≥n de datos‚Ä¶\")\n",
    "folder = f'{AUG_PATH}train/'\n",
    "print(f\"Aumentando im√°genes para:\\n >>\", folder)\n",
    "total_files = len(for_processing_data) # Total de archivos del dataset\n",
    "print(f\" - Total de archivos en el dataset 'train': {len(process_split)}\")\n",
    "print(f\" - Total de archivos en a procesar: {total_files} ({(total_files/len(process_split)*100):.0f}% del dataset)\")\n",
    "print(f\" - Transformaciones totales: {n_transformations} im√°genes ser√°n creadas\\n\")\n",
    "print(f\"Se procesar√°n un total de {len(for_processing_classes)} clases de {len(process_split['group'].value_counts())}:\")\n",
    "for label in for_processing_classes:\n",
    "    print(\" - \",label)\n",
    "print()\n",
    "for i, process in enumerate(processes):\n",
    "    print(f\"¬∑ Estrategia {i+1}: {process['increase']}x aumentation para {process['classes']}\")\n",
    "print(\"\\n¬øDeseas iniciar el procesamiento?\")\n",
    "\n",
    "# Input de confirmaci√≥n del usuario\n",
    "confirmacion = input(f\"‚ö†Ô∏è ATENCI√ìN: El proceso puede demorar varios minutos. Se requiere confirmaci√≥n para continuar [Y/N]: \").strip().lower()\n",
    "if confirmacion != 'y':\n",
    "    print(f\"\\n‚õîÔ∏è La ejecuci√≥n ha sido denegada por el usuario.\")\n",
    "else:\n",
    "    start_time = time.perf_counter() # Medici√≥n del tiempo\n",
    "    partial_time = start_time #?\n",
    "\n",
    "    # PROCESAMIENTO DE AUMENTACIONES\n",
    "    # Leyenda:\n",
    "    #  count -> archivos originales procesados (total)\n",
    "    #  work -> transformaciones por clase (parcial)\n",
    "    #  task -> n√∫mero de transformaciones por archivo\n",
    "\n",
    "    # Duplica dataframe para almacenar el proceso\n",
    "    processed_split = process_split.copy()\n",
    "\n",
    "    # Realiza el proceso de copiado de archivos para cada grupo\n",
    "    succeeded_process = True\n",
    "    count = 0\n",
    "    total_work_done = 0\n",
    "    avg_time = 0\n",
    "    partial_time = 0\n",
    "    previous_count = 0\n",
    "    spacer1 = '   '\n",
    "    spacer2 = '      '\n",
    "\n",
    "    for i, process in enumerate(processes):\n",
    "        mult = process['increase']\n",
    "        n_aumentations = mult - 1 # se resta 1 porque la imagen original ya existe\n",
    "        classes = process['classes']\n",
    "        print(f\"\\n\\n{numoji(i+1)} Iniciando subproceso de aumentaci√≥n:\")\n",
    "        print(f\" - Estrategia de aumentaci√≥n: {mult}x\")\n",
    "        print(f\" - Clases a procesar: {len(classes)} grupos\\n   {classes}\")\n",
    "\n",
    "        for clase in classes:\n",
    "            # Filtrado de clases\n",
    "            subprocess_split = process_split[process_split['group']==clase]\n",
    "            total_work = len(subprocess_split) * n_aumentations # Conteo de archivos para la clase\n",
    "            print(f\"\\n{spacer1}üîÑ Procesando clase '{clase}' ({(total_work/total_files*100):.0f}% del total):\")\n",
    "            print(f\"{spacer1}  - Transformaciones a generar: {total_work}\") if verbose else None\n",
    "            work = 0\n",
    "\n",
    "            for index, image_row in subprocess_split.iterrows():\n",
    "                if work==0 and testing:\n",
    "                    print(f\"{spacer2}(+) TESTING: omite la apertura de la imagen.\")\n",
    "                    print(f\"{spacer2}(+) TESTING: omite las transformaciones.\")\n",
    "                # 1. Apertura del archivo de imagen original\n",
    "                original_image = load_image_idx(data=image_row,root=folder) if not testing else None\n",
    "                #original_image.show() # Debugging\n",
    "\n",
    "                # Genera m√∫ltiples transformaciones para cada imagen\n",
    "                try:\n",
    "                    task = n_aumentations\n",
    "                    for n in range(n_aumentations):\n",
    "\n",
    "                        # 2. Procesamiento de la transformacion\n",
    "                        transformed_image = apply_transformations(original_image) if not testing else None\n",
    "                        try:\n",
    "                            # Convierte el array NumPy a un objeto Image de PIL\n",
    "                            if not testing:\n",
    "                                if transformed_image.ndim == 2:  # Escala de grises\n",
    "                                    processed_image = Image.fromarray(transformed_image.astype(np.uint8), 'L')\n",
    "                                elif transformed_image.ndim == 3:  # RGB\n",
    "                                    if transformed_image.shape[2] == 3:\n",
    "                                        processed_image = Image.fromarray(transformed_image.astype(np.uint8), 'RGB')\n",
    "                                    else:\n",
    "                                        raise ValueError(f\"{spacer2}‚ùóÔ∏èEl array NumPy debe tener 2 (escala de grises) o 3 (RGB) canales.\")\n",
    "                                else:\n",
    "                                    raise ValueError(f\"{spacer2}‚ùóÔ∏è El array NumPy debe tener 2 o 3 dimensiones.\")\n",
    "                                #processed_image.show() # Debugging\n",
    "\n",
    "                            # 3. Almacena el archivo\n",
    "                            # Genera el nombre del nuevo archivo\n",
    "                            # se incluye un int random al final (para permitir m√∫ltiples aumentaciones)\n",
    "                            rnd_num = str(gen_rnd_id()).zfill(6) # string de 6 cifras\n",
    "                            name = list(os.path.splitext(image_row.filename))\n",
    "                            name.insert(1,f'-{rnd_num}')\n",
    "                            filename = \"\".join(name)\n",
    "                            # Guarda la transformaci√≥n como JPG\n",
    "                            processed_image.save(f'{folder}{image_row.image_path}{filename}', \"JPEG\") if not testing else None\n",
    "                            print(f\"{spacer2}üîπ {filename}\") if debugging else None\n",
    "\n",
    "                            # 4. Registra la nueva imagen en el DataFrame\n",
    "                            new_row = image_row.copy() # Copia las etiquetas\n",
    "                            # Actualiza los valores correspondientes\n",
    "                            new_row['filename'] = filename\n",
    "                            new_row['is_original'] = False\n",
    "                            new_row['augmented'] = True\n",
    "                            # Agrega la nueva fila al DataFrame processed_split\n",
    "                            processed_split = pd.concat([processed_split, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "                            task -= 1 # cuenta una transformaci√≥n finalizada\n",
    "                            work += 1 # cuenta una aumentaci√≥n realizada (por clase)\n",
    "                            if work % 100 == 0:\n",
    "                                print(f'{spacer2}üî® ¬ª Progreso subproceso: {work/total_work*100:.2f}%') if verbose else None\n",
    "\n",
    "                        except FileExistsError:\n",
    "                            print(f\"{spacer2}‚ùå Error: El archivo de destino '{destination_folder}' ya existe.\\n\") if debugging else None\n",
    "                            continue\n",
    "                        except Exception as e:\n",
    "                            print(f\"{spacer2}‚ùå Error al guardar la imagen como JPG: {e}\") if debugging else None\n",
    "                            continue\n",
    "                finally:\n",
    "                    # 5. Actualiza el registro de la imagen original\n",
    "                    processed_split.loc[index, 'augmented'] = True\n",
    "                    #print(processed_split.iloc[index])\n",
    "\n",
    "                    count += 1 # cuenta un archivo finalizado\n",
    "                    if task == 0:\n",
    "                        pass\n",
    "                    else:\n",
    "                        print(f\"{spacer2}‚ÄºÔ∏è ALERTA: No se ha completado la tarea para {image_row.filename}\")\n",
    "\n",
    "            print(f'{spacer2}üî® ¬ª Progreso subproceso: 100%') if verbose else None\n",
    "            total_work_done += work # registro del trabajo realizado\n",
    "\n",
    "            # L√ìGICA TEMPORAL\n",
    "            if verbose:\n",
    "                # Medici√≥n de tiempo pasado y estimaci√≥n del faltante\n",
    "                previous_time = partial_time\n",
    "                partial_time = time.perf_counter() # Medici√≥n del tiempo\n",
    "                divisor = count - previous_count\n",
    "                elapsed_time = (partial_time - start_time)\n",
    "                task_time = elapsed_time/divisor if divisor > 0 else 0\n",
    "                #avg_time = elapsed_time/count if count > 0 else 0 # Tiempo medio para estimaci√≥n\n",
    "                avg_time = elapsed_time/total_work_done if total_work_done > 0 else 0 # Tiempo medio para estimaci√≥n\n",
    "\n",
    "                print(f'{spacer2}‚è±Ô∏è ¬∑ Tiempo de suproceso {humanize.naturaldelta(avg_time)}')\n",
    "                if previous_count and verbose:\n",
    "                    if task_time > avg_time*1.1:\n",
    "                        print(f\"{spacer2}üëéüèª La tarea ha demorado m√°s de lo esperado‚Ä¶\")\n",
    "                    elif task_time < avg_time*0.75:\n",
    "                        print(f\"{spacer2}üëçüèª ¬°Tarea completada en tiempo record!\")\n",
    "\n",
    "            if work == total_work:\n",
    "                print(f\"{spacer1}‚úÖ Subproceso de transfromaciones completado.\")\n",
    "                succeeded = True\n",
    "            else:\n",
    "                print(f\"{spacer1}‚ÄºÔ∏è ALERTA: No se pudo completar correctamente el subproceso de transfromaciones.\")\n",
    "                succeeded = False\n",
    "            if verbose:\n",
    "                #print(f'\\n{spacer1}üïó Ejecuci√≥n total {elapsed_time/60:.1f} min')\n",
    "                print(f'\\n{spacer1}üïó Ejecuci√≥n total {humanize.naturaldelta(elapsed_time)}')\n",
    "                #print(f'{spacer1}üîÆ Estimaci√≥n de tiempo restante {avg_time * (n_transformations - total_work_done) / 60:.2f} min')\n",
    "                print(f'{spacer1}üîÆ Estimaci√≥n de tiempo restante {humanize.naturaldelta(avg_time * (n_transformations - total_work_done))}')\n",
    "                print(f'{spacer1}‚ùáÔ∏è ¬ª Progreso general: {count/total_files*100:.3f}%') if (count/total_files) < 0.99 else None\n",
    "            previous_count = count\n",
    "\n",
    "            succeeded_process *= succeeded # Actualiza el estado del proceso\n",
    "            # (S√≥lo es 'True' si todos los subprocesos se completan correctamente)\n",
    "\n",
    "    print(f'{spacer1}‚ùáÔ∏è ¬ª Progreso general: 100%') if not testing else None\n",
    "    end_time = time.perf_counter() # Medici√≥n del tiempo\n",
    "    total_time = end_time - start_time\n",
    "    if succeeded_process:\n",
    "        print(\"\\nüåü El proceso de aumentaci√≥n del dataset ha finalizado con √©xito.\")\n",
    "    else:\n",
    "        print(\"\\nüö´ No se pudo completar satisfactoriamente el proceso de aumentaci√≥n del dataset.\")\n",
    "    if verbose:\n",
    "        print(f'{spacer1}üïó Ejecuci√≥n completada en {humanize.naturaldelta(total_time)}')\n",
    "        print(f'{spacer1}üî¢ ¬∑ Se procesaron {total_work_done} transformaciones para un total de {total_files} im√°genes.')\n",
    "        print(f'{spacer1}‚è±Ô∏è ¬∑ Tiempo medio por transofrmaci√≥n {(total_time/total_work_done):.3f} sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66596957",
   "metadata": {
    "id": "66596957"
   },
   "source": [
    "# Verificaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1cf0a2",
   "metadata": {
    "id": "bb1cf0a2"
   },
   "source": [
    "Ahora, verificamos el proceso de aumentaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12a8ab8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1745095221197,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "b12a8ab8",
    "outputId": "1de0f99a-1e65-4265-b344-8eebf0b150a9"
   },
   "outputs": [],
   "source": [
    "resut_aug_dataframe = len(processed_split) - len(process_split)\n",
    "print(f\"Luego del proceso de aumentaci√≥n de datos se genraron {resut_aug_dataframe} archivos nuevos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7NU3_tv3omdE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 108,
     "status": "ok",
     "timestamp": 1745095227762,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "7NU3_tv3omdE",
    "outputId": "1d18efee-50cd-4f35-a08a-b0e86e3225c6"
   },
   "outputs": [],
   "source": [
    "# prompt: count every file in a folder directory (including subfolders)\n",
    "\n",
    "import os\n",
    "\n",
    "def count_files(directory):\n",
    "  \"\"\"Counts the number of files in a directory, including subdirectories.\n",
    "\n",
    "  Args:\n",
    "    directory: The path to the directory.\n",
    "\n",
    "  Returns:\n",
    "    The total number of files in the directory and its subdirectories.\n",
    "  \"\"\"\n",
    "  total_files = 0\n",
    "  for root, _, files in os.walk(directory):\n",
    "    total_files += len(files)\n",
    "  return total_files\n",
    "\n",
    "# Example usage: Replace 'your_directory' with the actual directory path\n",
    "directory_path = '/content/Nonetest'\n",
    "file_count = count_files(directory_path)\n",
    "print(f\"Test dataset '{directory_path}': {file_count}\")\n",
    "\n",
    "directory_path = '/content/Nonetrain'\n",
    "file_count = count_files(directory_path)\n",
    "print(f\"Train dataset '{directory_path}': {file_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1581e090",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1745095231466,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "1581e090",
    "outputId": "e8fcf973-5006-4203-91c3-9ab3d32fb978"
   },
   "outputs": [],
   "source": [
    "processed_split.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c315ad6b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1745095233897,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "c315ad6b",
    "outputId": "0f5b6376-7902-421a-b5a3-1dc4ab340ffd"
   },
   "outputs": [],
   "source": [
    "processed_split.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f37952e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1745095236544,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "8f37952e",
    "outputId": "15e28782-e9db-4ea5-f2fe-408a3a75c80d"
   },
   "outputs": [],
   "source": [
    "processed_split[(processed_split['is_original'] == False)].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3eb441",
   "metadata": {
    "executionInfo": {
     "elapsed": 142,
     "status": "ok",
     "timestamp": 1745095238977,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "9b3eb441"
   },
   "outputs": [],
   "source": [
    "# MERGE DEL DATAFRAME AUMENTADO CON EL ORIGINAL\n",
    "# 1. Se pararan los datos de archivos procesados\n",
    "for_update = processed_split[processed_split['is_original']==True]\n",
    "new_rows = processed_split[processed_split['is_original']==False]\n",
    "\n",
    "# 2. Se actualiza el Dataframe original con los nuevos valores\n",
    "df_split.update(for_update)\n",
    "\n",
    "# 3. Se agregan las aumentaciones con el dataframe original\n",
    "df_augmented = pd.concat([df_split, new_rows], axis=0)\n",
    "df_augmented.update(for_update)\n",
    "\n",
    "# 4. (Opcional) Reordenar por √≠ndice si quer√©s mantener orden\n",
    "df_augmented['ref'] = df_augmented['ref'].astype('Int64')\n",
    "# df_merged = df_merged.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_-8qC43zk8th",
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1745095263796,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "_-8qC43zk8th"
   },
   "outputs": [],
   "source": [
    "#df_augmented.to_csv('df_augmented.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e846b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1745095265597,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "79e846b9",
    "outputId": "e68ec9ca-230b-434f-9bf3-13d8aa252a7f"
   },
   "outputs": [],
   "source": [
    "df_augmented.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e5da61",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1745095267571,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "00e5da61",
    "outputId": "009df6ce-2d53-4d1d-bcc7-3a43d54edd14"
   },
   "outputs": [],
   "source": [
    "# Verificamos que el Dataset original + Aumentaci√≥n coincidan con los registros del dataframe final\n",
    "len_entrada = len(df_split)\n",
    "len_augmentado = len_entrada + resut_aug_dataframe\n",
    "len_salida = len(df_augmented)\n",
    "if len_salida != len_augmentado:\n",
    "    raise ValueError(f'‚ÄºÔ∏è ALERTA: Se detect√≥ una discrepancia en el dataframe:\\n - Datos originales {len_entrada}\\n - Datos aumentados {resut_aug_dataframe}\\n - Datos de salida {len_salida}')\n",
    "else:\n",
    "    print(f\"‚úÖ Verificaci√≥n exitosa. Se verificaron {len_salida} archivos en dataframe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58810c05",
   "metadata": {
    "id": "58810c05"
   },
   "source": [
    "## Distribuci√≥n por clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ffa4e5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1745095270067,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "d2ffa4e5",
    "outputId": "d3962542-e664-485e-e9c2-66bcb78d1df0"
   },
   "outputs": [],
   "source": [
    "### Resumen distribucion por grupo\n",
    "print(\"Conteo por grupo:\")\n",
    "print(df_augmented['group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f9027e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1745095272559,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "c9f9027e",
    "outputId": "0a458773-3c69-4d53-8027-40951c71b067"
   },
   "outputs": [],
   "source": [
    "print(\"Efecto de la aumentaci√≥n de datos por clase:\\n\")\n",
    "counts_entrada = df_split['group'].value_counts()\n",
    "counts_salida = df_augmented['group'].value_counts()\n",
    "\n",
    "classes = counts_salida.index\n",
    "for class_ in classes:\n",
    "    print(f\" - {class_}: {counts_entrada[class_]} -> {counts_salida[class_]} (+{(counts_salida[class_]/counts_entrada[class_]-1)*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zuJLwiu3kQTy",
   "metadata": {
    "id": "zuJLwiu3kQTy"
   },
   "source": [
    "----\n",
    "# Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lYYhbtz_kbqm",
   "metadata": {
    "id": "lYYhbtz_kbqm"
   },
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R5KPXKQ9lzs5",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1745095279480,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "R5KPXKQ9lzs5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vaH4dNL-kRZe",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1745095281635,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "vaH4dNL-kRZe"
   },
   "outputs": [],
   "source": [
    "# Data laoders setup\n",
    "def load_from_directory(data_folder):\n",
    "    \"\"\"\n",
    "    Carga un dataset de im√°genes desde un directorio espec√≠fico.\n",
    "\n",
    "    Args:\n",
    "        data_folder (str): Ruta al directorio que contiene las im√°genes.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: Dataset de TensorFlow con las im√°genes y etiquetas.\n",
    "    \"\"\"\n",
    "    dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        data_folder,  # Ruta al directorio de datos\n",
    "        labels=\"inferred\",  # Las etiquetas se infieren autom√°ticamente desde los nombres de las carpetas\n",
    "        label_mode=\"categorical\",  # Las etiquetas se codifican como categor√≠as (one-hot encoding)\n",
    "        class_names=None,  # Las clases se detectan autom√°ticamente\n",
    "        color_mode=\"rgb\",  # Las im√°genes se cargan en modo RGB\n",
    "        batch_size=128,  # Tama√±o de lote para el entrenamiento\n",
    "        image_size=(256, 256),  # Redimensiona las im√°genes a 128x128 p√≠xeles\n",
    "        shuffle=True,  # Mezcla las im√°genes aleatoriamente\n",
    "        seed=42,  # No se utiliza una semilla espec√≠fica para la aleatorizaci√≥n\n",
    "        validation_split=None,  # No se realiza una divisi√≥n de validaci√≥n aqu√≠\n",
    "        subset=None,  # No se especifica un subconjunto (train/validation)\n",
    "        interpolation=\"bilinear\",  # M√©todo de interpolaci√≥n para redimensionar las im√°genes\n",
    "        follow_links=False,  # No sigue enlaces simb√≥licos\n",
    "        crop_to_aspect_ratio=False  # No recorta las im√°genes para ajustar la relaci√≥n de aspecto\n",
    "    )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k21oXc7-kTqf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6567,
     "status": "ok",
     "timestamp": 1745095290753,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "k21oXc7-kTqf",
    "outputId": "15b6d7cd-efe8-420d-b0dd-e6a9ddd872d5"
   },
   "outputs": [],
   "source": [
    "# Carga el dataset de im√°genes desde el directorio especificado\n",
    "train_images = \"\"; test_images = \"\"; valid_images = \"\"\n",
    "\n",
    "print(\"Cargando datasets desde el directorio‚Ä¶\\n\")\n",
    "for split in splits:\n",
    "    data_folder = f'{SPLITTED_PATH}{split}/'\n",
    "\n",
    "    # Carga el conjunto de datos desde el directorio especificado\n",
    "    # Utiliza la funci√≥n de TensorFlow para crear un dataset de im√°genes\n",
    "    match split:\n",
    "        case 'train':\n",
    "            print(f\"Cargando dataset de entrenamiento desde:\\n > {data_folder}\")\n",
    "            train_images = load_from_directory(data_folder)\n",
    "        case 'test':\n",
    "            print(f\"Cargando dataset de test desde:\\n > {data_folder}\")\n",
    "            test_images = load_from_directory(data_folder)\n",
    "        case 'valid':\n",
    "            print(f\"Cargando dataset de validaci√≥n desde:\\n > {data_folder}\")\n",
    "            valid_images = load_from_directory(data_folder)\n",
    "        case _: # En caso de no coincidir con ninguno de los splits\n",
    "            print(f\"‚ö†Ô∏è El split '{split}' no es reconocido. No se cargar√° ning√∫n dataset.\")\n",
    "            continue # Salta al siguiente split\n",
    "    print(f\"‚úÖ Dataset cargado exitosamente.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S5wkBadHlOcN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1745095293885,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "S5wkBadHlOcN",
    "outputId": "cf05db50-eeca-4ff0-ee7e-a6a60726756e"
   },
   "outputs": [],
   "source": [
    "print(\"Resumen de los datasets cargados:\")\n",
    "print(f\" - Total de im√°genes en el dataset de entrenamiento: {len(train_images)}\")\n",
    "print(f\" - Total de im√°genes en el dataset de validaci√≥n: {len(valid_images)}\")\n",
    "print(f\" - Total de im√°genes en el dataset de test: {len(test_images)}\")\n",
    "print(f\"Total de im√°genes cargadas: {len(train_images) + len(test_images) + len(valid_images)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vXPimzgmm78F",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1745095297636,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "vXPimzgmm78F",
    "outputId": "b2615fb1-470b-4a82-e47d-4f817becd804"
   },
   "outputs": [],
   "source": [
    "print(f\"Clases detectadas:\")\n",
    "[print(\" -\",clase) for clase in train_images.class_names]\n",
    "print(f\"Total de clases: {len(train_images.class_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b82dbb",
   "metadata": {},
   "source": [
    "----\n",
    "## Arquitectura del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d64a42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "# Bloque 1\n",
    "model.add(Input(shape=(256, 256, 3)))\n",
    "model.add(layers.Rescaling(1./255))\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.1))\n",
    "\n",
    "# Bloque 2\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.1))\n",
    "\n",
    "# Bloque 3\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.1))\n",
    "\n",
    "# Bloque 4\n",
    "model.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.1))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Capa densa intermedia\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dropout(0.25))\n",
    "\n",
    "# Capa de salida con 38 neuronas y softmax para multiclase\n",
    "model.add(layers.Dense(38, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d790b348",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225e0b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "# Definimos el callback para guardar el mejor modelo seg√∫n la m√©trica elegida\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='best_model.keras',   # Se generar√° una carpeta con este nombre\n",
    "    monitor='val_loss',            # M√©trica a monitorear ('val_accuracy' es otra opci√≥n)\n",
    "    save_best_only=True,           # Guarda solo si hay mejora\n",
    "    save_weights_only=False,       # Guarda la arquitectura + pesos\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Ajusta el modelo a tu criterio\n",
    "with tf.device('/GPU:0'):\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "history = model.fit(\n",
    "    train_images,\n",
    "    validation_data=test_images,\n",
    "    epochs=10,\n",
    "    callbacks=[checkpoint_callback]  # Incorporamos el callback\n",
    ")\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Tiempo de entrenamiento: {elapsed_time:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5OiC0aJnnLtu",
   "metadata": {
    "id": "5OiC0aJnnLtu"
   },
   "source": [
    "## Guardando resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iy4o3hiNnNjx",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1745097247402,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "iy4o3hiNnNjx"
   },
   "outputs": [],
   "source": [
    "#Recording History in json & pickle\n",
    "import json\n",
    "with open('training_hist.json','w') as f:\n",
    "  json.dump(history.history,f)\n",
    "\n",
    "import pickle\n",
    "with open('training_hist.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HHUmDV4HnQ_Z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1326,
     "status": "ok",
     "timestamp": 1745097250729,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "HHUmDV4HnQ_Z",
    "outputId": "7d2f4247-1f3f-42ed-d8c9-bb99b3f979c7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "experiment = 'experimento_n' # Completar n√∫mero de experimento\n",
    "files = ['best_model.keras','training_hist.json','training_hist.pkl']\n",
    "destino=f\"/content/drive/MyDrive/CV2-PlantVillage/{experiment}/\"\n",
    "\n",
    "def check_folder(folder):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "        print(f\"Folder '{folder}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Folder '{folder}' already exists.\")\n",
    "\n",
    "check_folder(destino)\n",
    "\n",
    "for file in files:\n",
    "    try:\n",
    "        origen=f\"/content/{file}\"\n",
    "        !cp -r \"$origen\" \"$destino\"\n",
    "    except:\n",
    "        print(f\"Error al copiar el archivo '{file}'\")\n",
    "    finally:\n",
    "        print(f\"Archivo '{file}' copiado exitosamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Cf-rl6y25u5U",
   "metadata": {
    "id": "Cf-rl6y25u5U"
   },
   "source": [
    "[Pesos del modelo entrenado (en Drive)](https://drive.google.com/file/d/17rHX7PTvd0RC0cZ4ZtiPOQy1KcsRdgUX/view?usp=drive_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0MCZ-KXTnrn3",
   "metadata": {
    "id": "0MCZ-KXTnrn3"
   },
   "source": [
    "---\n",
    "# Gr√°ficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Aa1ve2usnucW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 207,
     "status": "ok",
     "timestamp": 1745097257181,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "Aa1ve2usnucW",
    "outputId": "2d3a97b6-0449-483d-880c-11853b99c05a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = [i for i in range(1,11)]\n",
    "plt.plot(epochs,history.history['accuracy'],color='red',label='Training Accuracy')\n",
    "plt.plot(epochs,history.history['val_accuracy'],color='blue',label='Validation Accuracy')\n",
    "plt.xlabel('No. of Epochs')\n",
    "plt.title('Visualization of Accuracy Result')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "J-GidBpGnxCm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12495,
     "status": "ok",
     "timestamp": 1745097273728,
     "user": {
      "displayName": "Diego Mart√≠n M√©ndez",
      "userId": "06828156837652954680"
     },
     "user_tz": 180
    },
    "id": "J-GidBpGnxCm",
    "outputId": "c61af531-a7e1-4e5e-f0c6-6331c6015db4"
   },
   "outputs": [],
   "source": [
    "#Validation set Accuracy\n",
    "model = tf.keras.models.load_model('best_model.keras')\n",
    "val_loss, val_acc = model.evaluate(test_images)\n",
    "print('Validation accuracy:', val_acc)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
