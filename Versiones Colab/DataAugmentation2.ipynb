{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d57e12cb",
      "metadata": {},
      "source": [
        "# Inicialización collab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "444d527f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importación de librerías\n",
        "# Gestión de archivos y reporte\n",
        "import os\n",
        "import shutil\n",
        "import logging\n",
        "import yaml\n",
        "\n",
        "# Manipulación y análisis de datos\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Procesamiento de imágenes\n",
        "from PIL import Image\n",
        "\n",
        "# Machine Learning\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fd59a00",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d51d331",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargamos el dataframe desde el .CSV y definimos 'id' como índice\n",
        "try:\n",
        "    df_split = pd.read_csv('/content/drive/MyDrive/CV2-PlantVillage/dataframe_splitted.csv').set_index('id')\n",
        "except FileNotFoundError:\n",
        "    print(f\"⚠️ Error: El archivo 'dataframe.csv' no se encontró en la ubicación actual: {os.getcwd()}\")\n",
        "    print(\"🚨 Se creará nuevamente al correr las celdas de 'Importación de imágenes' 🚨.\")\n",
        "    df_split = None\n",
        "except Exception as e:\n",
        "    print(f\"Ocurrió un error al leer el archivo CSV: {e}\")\n",
        "    df_split = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71e7b021",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_split.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4d72e69",
      "metadata": {},
      "source": [
        "#### Descarga de dataset de Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98c0f2a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "DATASET_PATH = kagglehub.dataset_download(\"abdallahalidev/plantvillage-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", DATASET_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30e5cd67",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ruta de acceso al dataset\n",
        "ROOT_DIR = f'{DATASET_PATH}/plantvillage dataset/color'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5b865de",
      "metadata": {},
      "source": [
        "# Dataset split"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad4e12c9",
      "metadata": {},
      "source": [
        "### Funciones necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7339f34c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_ignore_function(df, train_label, filename_col='filename'):\n",
        "    \"\"\"\n",
        "    Crea y devuelve la función 'ignore_files' que tiene acceso al DataFrame\n",
        "    y sabe qué archivos mantener.\n",
        "    \"\"\"\n",
        "    # Crea un conjunto (set) con los nombres de archivo que SÍ queremos copiar (ej: split == 'train')\n",
        "    # Usa este conjunto para hacer la búsqueda de forma mucho más rápida\n",
        "    files_to_keep = set(df[df['split'] == train_label][filename_col])\n",
        "    #print(f\"Archivos a mantener (split='{train_label}'): {files_to_keep}\") # Debugging\n",
        "\n",
        "    def ignore_files(current_dir, files_in_current_dir):\n",
        "        \"\"\"\n",
        "        Función que será llamada por shutil.copytree.\n",
        "        Decide qué archivos/directorios ignorar en el directorio actual.\n",
        "        \"\"\"\n",
        "        ignore_list = []\n",
        "        for item in files_in_current_dir:\n",
        "            # Construye la ruta completa para verificar si es archivo o directorio\n",
        "            full_path = os.path.join(current_dir, item)\n",
        "\n",
        "            # Aplicar la lógica de ignorar SOLO los ARCHIVOS de la lista\n",
        "            if os.path.isfile(full_path):\n",
        "                # Si el nombre del archivo NO está en el conjunto de archivos a mantener,\n",
        "                # entonces lo agrega a la lista de ignorados.\n",
        "                if item not in files_to_keep:\n",
        "                    # print(f\"Ignorando archivo: {item} (en {current_dir})\") # Debugging\n",
        "                    ignore_list.append(item)\n",
        "\n",
        "        # print(f\"Directorio: {current_dir}, Ignorando: {ignore_list}\") # Debugging\n",
        "        return ignore_list\n",
        "\n",
        "    # Devuelve la función 'ignore_files' configurada\n",
        "    return ignore_files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f21693a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, re\n",
        "\n",
        "# Busca la carpeta raíz del dataset en el directorio donde fue descargado\n",
        "def find_path(folder):\n",
        "    match = re.search(fr\"^(.*?)/{folder}/\", DATASET_PATH)\n",
        "    if match:\n",
        "        prefix = match.group(1)\n",
        "        path = os.path.join(prefix, f\"{folder}/\")\n",
        "        return path\n",
        "    else:\n",
        "        print(f'No se ha podido encontrar la carpeta \"{folder}\" en {DATASET_PATH}')\n",
        "        return None\n",
        "# Carga de imagenes en memoria y visualización\n",
        "def load_image(data: pd.DataFrame, index: int, root: str=ROOT_DIR):\n",
        "    \"\"\"\n",
        "    Carga una imagen PIL desde una fila específica de un DataFrame.\n",
        "\n",
        "    Args:\n",
        "        dataframe (pandas.DataFrame): El DataFrame que contiene las rutas de las imágenes.\n",
        "        index (int): El índice de la fila en el DataFrame para cargar la imagen.\n",
        "        root_dir (str): El directorio raíz donde se encuentran las imágenes.\n",
        "\n",
        "    Returns:\n",
        "        PIL.Image.Image: La imagen cargada como un objeto PIL.Image, o None si ocurre un error.\n",
        "    \"\"\"\n",
        "    if index < 0 or index >= len(data):\n",
        "        print(\"Índice fuera de rango.\")\n",
        "        return None\n",
        "\n",
        "    row = data.iloc[index]\n",
        "    relative_path = row['image_path']\n",
        "    filename = row['filename']\n",
        "    full_path = os.path.join(root, relative_path, filename)\n",
        "\n",
        "    try:\n",
        "        img = Image.open(full_path)\n",
        "        return img\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Archivo no encontrado: {full_path}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error al cargar la imagen: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78948f1f",
      "metadata": {},
      "source": [
        "## Split de archivos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf5382a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guarda directorio del dataset dividido\n",
        "path = find_path(\"plantvillage-dataset\")\n",
        "DATASETS_ROOT = path\n",
        "SPLITTED_PATH = f\"{path}splitted/\" if path else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f3dab83",
      "metadata": {},
      "outputs": [],
      "source": [
        "splits = df_split['split'].value_counts().index.tolist()\n",
        "splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8db6bfc7",
      "metadata": {},
      "outputs": [],
      "source": [
        "verfication = True # Ejecuta el proceso de verificación (punto 2)\n",
        "\n",
        "print(f\"Se inicia proceso de copiado del dataset…\")\n",
        "total_files = len(df_split) # Total de archivos del dataset\n",
        "print(f\" - Total de archivos en el dataset: {total_files}\")\n",
        "\n",
        "# Realiza el proceso de copiado de archivos para cada split\n",
        "succeeded_process = True\n",
        "for split in splits:\n",
        "    # Crea las rutas de origen y destino\n",
        "    # (Ejemplo: 'train', 'test', 'valid')\n",
        "    print(f\"\\n\\nIniciando proceso para '{split}' split …\")\n",
        "    source_folder = f'{ROOT_DIR}/'\n",
        "    destination_folder = f'{SPLITTED_PATH}{split}/'\n",
        "    total_split = len(df_split[df_split['split'] == split]) # Total de archivos del split\n",
        "    # Se omite verificación de existencia del dataset (porque se crea siempre desde cero)\n",
        "    print(f\"🔄 Procesando split '{split.upper()}' ({(total_split/total_files*100):.2f}):\")\n",
        "    print(f\"  - Total de archivos a copiar: {total_split}\")\n",
        "    succeeded = False\n",
        "\n",
        "    try:\n",
        "        print(f\"1. Creando estructura de subcarpetas:\")\n",
        "        # 1. Crea la función para ignorar específica para el split a procesar\n",
        "        ignore_function = create_ignore_function(df_split, train_label=split, filename_col='filename')\n",
        "        print(f\"    ✔ Función de filtro creada para el split \")\n",
        "\n",
        "        # 2. Con copytree copia todo el \"árbol\" de directorios (careptas y subcarpetas)\n",
        "        # Fitrando con ignore_function todos aquellos archivos que no pertenecen al split deseado\n",
        "        print(f\"    ∞ Copiando contenido del dataset (puede demorar hasta un minuto).\")\n",
        "        shutil.copytree(source_folder, destination_folder, ignore=ignore_function)\n",
        "        print(f\"    ✔ Proceso de copiado del split finalizado.\")\n",
        "\n",
        "        if verfication:\n",
        "            # Verifica qué se haya copiado adecuadamente (opcional pero útil)\n",
        "            print(f\"2. Se inicia proceso de verificación…\")\n",
        "            copied_files = []\n",
        "            for root, dirs, files_in_dest in os.walk(destination_folder):\n",
        "                for name in files_in_dest:\n",
        "                    copied_files.append(os.path.join(os.path.relpath(root, destination_folder), name).replace('\\\\', '/')) # Normalizar path\n",
        "                    #print(f\"  - {os.path.join(root, name)}\") # Debuggin\n",
        "            print(f\"    ✔ Se crearon un total de {len(os.listdir(destination_folder))} carpetas (para las clases).\")\n",
        "            print(f\"    ✔ Se copiaron un total de {len(copied_files)} archivos ({len(copied_files)/total_split*100:.2f}%)\")\n",
        "            # Agregar confirmación de igualdad cantidad split == copiados\n",
        "            if len(copied_files) == total_split:\n",
        "                print(f\"✅ Se completó satisfactoriamente el subproceso de copiado para el split.\\n\")\n",
        "                succeeded = True\n",
        "            else:\n",
        "                print(f\"❌ Error: No se pudo copiar correctamente el split '{split.upper()}'\\n\")\n",
        "                succeeded = False\n",
        "        else:\n",
        "            succeeded = True # Si la verificación está desactivada, se asume que el proceso fue exitoso\n",
        "\n",
        "    except FileExistsError:\n",
        "        print(f\"Error: La carpeta de destino '{destination_folder}' ya existe.\\n\")\n",
        "        pass\n",
        "    except Exception as e:\n",
        "        print(f\"Ocurrió un error inesperado: {e}\\n\")\n",
        "\n",
        "    succeeded_process *= succeeded # Actualiza el estado del proceso\n",
        "    # (Sólo es 'True' si todos los splits se copian correctamente)\n",
        "\n",
        "if succeeded_process:\n",
        "    print(\"\\n\\n🌟 El proceso de copiado del dataset ha finalizado con éxito.\\n\")\n",
        "else:\n",
        "    print(\"\\n\\n🚫 No se pudo completar satisfactoriamente el proceso de copiado del dataset.\\nVerificar que se haya completado la eliminación de las carpetas.\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26fbf36a",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b6b725e",
      "metadata": {
        "id": "2b6b725e"
      },
      "source": [
        "# Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a60ec1ca",
      "metadata": {
        "id": "a60ec1ca"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import albumentations as A\n",
        "import random\n",
        "import yaml\n",
        "import shutil\n",
        "from PIL import Image\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fff79c0",
      "metadata": {
        "id": "1fff79c0"
      },
      "source": [
        "### Funciones necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5e91e15",
      "metadata": {
        "id": "f5e91e15"
      },
      "outputs": [],
      "source": [
        "def find_folder(path):\n",
        "    return os.path.basename(os.path.dirname(path))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a8fedd1",
      "metadata": {
        "id": "9a8fedd1"
      },
      "source": [
        "### Carga de datos almacenados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "464be198",
      "metadata": {
        "id": "464be198"
      },
      "outputs": [],
      "source": [
        "def dataset_already_exists(path_to_check: str) -> bool | None:\n",
        "    \"\"\"\n",
        "    Verifica si el directorio especificado existe y está vacío.\n",
        "\n",
        "    Args:\n",
        "        path_to_check (str): Ruta del directorio a verificar.\n",
        "\n",
        "    Returns:\n",
        "        bool: True si el directorio existe y está vacío, False en caso contrario.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path_to_check):\n",
        "        # El directorio no existe -> Crea el directorio\n",
        "        #print(f\"☑️ El directorio no existe, aún no ha sido creado:\\n > {path_to_check}\") # Debugging\n",
        "        return False # No realiza ninguna acción\n",
        "    else:\n",
        "        # Verificar si el directorio está vacío\n",
        "        try:\n",
        "            # Explora el contenido del directorio\n",
        "            content = os.listdir(path_to_check)\n",
        "            #print(content) # Debugging\n",
        "\n",
        "            # Si el directorio está vacío, se puede eliminar directamente\n",
        "            #       -> Elimina sin confirmación\n",
        "            if not content:\n",
        "                os.rmdir(path_to_check) # Elimina el directorio vacío\n",
        "                print(f\"☑️ El directorio estaba vacío y se ha eliminado de forma automática:\\n > {path_to_check}\\n\")\n",
        "                return False\n",
        "\n",
        "            # Si el directorio contiene sólo archivos ocultos (de sistema)\n",
        "            #       -> Elimina sin confirmación\n",
        "            elif all([file.startswith('.') for file in content]):\n",
        "                shutil.rmtree(path_to_check) # Elimina el directorio y su contenido\n",
        "                print(f\"☑️ El directorio sólo contenía archivos ocutlos, por lo que se ha eliminado de forma automática:\\n > {path_to_check}\\n\")\n",
        "                return False\n",
        "\n",
        "            # Si hay archivos visibles en el directorio (dataset ya existe)\n",
        "            #       -> Solicita permiso para eliminarlos\n",
        "            else:\n",
        "                # Input de confirmación del usuario\n",
        "                confirmacion = input(f\"⚠️ El directorio especificado ya existe y contiene archivos. ¿Deseas eliminar todo su contenido y el directorio en sí? [Y/N]: '{path_to_check}'\").strip().lower()\n",
        "                # Verifica la respuesta del usuario\n",
        "                if confirmacion == 'y':\n",
        "                    shutil.rmtree(path_to_check) # Elimina el directorio y su contenido\n",
        "                    print(f\"✅ El directorio y su contenido han sido eliminados exitosamente:\\n > {path_to_check}\\n\")\n",
        "                    return False\n",
        "                else:\n",
        "                    print(f\"⛔️ La eliminación del directorio ha sido denegada por el usuario:\\n  > {path_to_check}\")\n",
        "                    return True\n",
        "\n",
        "        except OSError as e:\n",
        "            print(f\"❌ Error al eliminar el directorio vacío en {path_to_check}: {e}\\n\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"‼️ Ocurrió un error inesperado al intentar eliminar el directorio vacío en {path_to_check}: {e}\\n\")\n",
        "            return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf311112",
      "metadata": {
        "id": "bf311112"
      },
      "outputs": [],
      "source": [
        "# Carga de imagenes en memoria y visualización\n",
        "def load_image(data: pd.DataFrame, index: int, root: str=ROOT_DIR):\n",
        "    \"\"\"\n",
        "    Carga una imagen PIL desde una fila específica de un DataFrame.\n",
        "\n",
        "    Args:\n",
        "        dataframe (pandas.DataFrame): El DataFrame que contiene las rutas de las imágenes.\n",
        "        index (int): El índice de la fila en el DataFrame para cargar la imagen.\n",
        "        root_dir (str): El directorio raíz donde se encuentran las imágenes.\n",
        "\n",
        "    Returns:\n",
        "        PIL.Image.Image: La imagen cargada como un objeto PIL.Image, o None si ocurre un error.\n",
        "    \"\"\"\n",
        "    # if index < 0 or index >= len(data):\n",
        "    #     print(\"Índice fuera de rango.\")\n",
        "    #     return None\n",
        "\n",
        "    row = data.iloc[index]\n",
        "    relative_path = row['image_path']\n",
        "    filename = row['filename']\n",
        "    full_path = os.path.join(root, relative_path, filename)\n",
        "\n",
        "    try:\n",
        "        img = Image.open(full_path)\n",
        "        return img\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Archivo no encontrado: {full_path}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error al cargar la imagen: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fc754e7",
      "metadata": {
        "id": "0fc754e7"
      },
      "outputs": [],
      "source": [
        "# Carga de imagenes en memoria y visualización\n",
        "def load_image_idx(data, root: str=ROOT_DIR):\n",
        "    \"\"\"\n",
        "    Carga una imagen PIL desde una fila específica de un DataFrame.\n",
        "\n",
        "    Args:\n",
        "        dataframe (pandas.DataFrame): El DataFrame que contiene las rutas de las imágenes.\n",
        "        index (int): El índice de la fila en el DataFrame para cargar la imagen.\n",
        "        root_dir (str): El directorio raíz donde se encuentran las imágenes.\n",
        "\n",
        "    Returns:\n",
        "        PIL.Image.Image: La imagen cargada como un objeto PIL.Image, o None si ocurre un error.\n",
        "    \"\"\"\n",
        "    # if index < 0 or index >= len(data):\n",
        "    #     print(\"Índice fuera de rango.\")\n",
        "    #     return None\n",
        "\n",
        "    row = data\n",
        "    relative_path = row['image_path']\n",
        "    filename = row['filename']\n",
        "    full_path = os.path.join(root, relative_path, filename)\n",
        "\n",
        "    try:\n",
        "        img = Image.open(full_path)\n",
        "        return img\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Archivo no encontrado: {full_path}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error al cargar la imagen: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ztZbyfR7eoBo",
      "metadata": {
        "id": "ztZbyfR7eoBo"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e623eeec",
      "metadata": {
        "id": "e623eeec"
      },
      "source": [
        "## Transformaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0234e2b",
      "metadata": {
        "id": "e0234e2b"
      },
      "outputs": [],
      "source": [
        "# 1. Define las transformaciones de Albumentations\n",
        "transform = A.Compose([\n",
        "    # Espaciales\n",
        "    A.Rotate(limit=180, p=0.5),          # Rotación aleatoria hasta 180 grados con probabilidad 0.5\n",
        "    A.HorizontalFlip(p=0.5),            # Volteo horizontal con probabilidad 0.5\n",
        "    A.VerticalFlip(p=0.5),              # Volteo vertical con probabilidad 0.5\n",
        "    A.RandomScale(scale_limit=0.1, p=0.3),      # Escalado aleatorio con un límite de 30%\n",
        "    A.RandomCrop(width=200, height=200, p=0.3), # Recorte aleatorio a 200x200 (original 256px)\n",
        "    # Visuales\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.3), # Ajuste de brillo y contraste aleatorio\n",
        "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.3), # Ajuste de tono, saturación y valor\n",
        "    A.GaussianBlur(blur_limit=(3, 5), p=0.2),   # Desenfoque Gaussiano\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c3f87a1",
      "metadata": {
        "id": "5c3f87a1"
      },
      "outputs": [],
      "source": [
        "# 2. Transformaciones alternativas (más agresivas) para clases minoritarias\n",
        "proba_mult = 2\n",
        "scaler = 1.5\n",
        "transform2 = A.Compose([\n",
        "    A.Rotate(limit=180, p=0.5*proba_mult),          # Rotación aleatoria hasta 180 grados con probabilidad 0.5\n",
        "    A.HorizontalFlip(p=0.5*proba_mult),            # Volteo horizontal con probabilidad 0.5\n",
        "    A.VerticalFlip(p=0.5*proba_mult),              # Volteo vertical con probabilidad 0.5\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.2*scaler, contrast_limit=0.2*scaler, p=0.3*proba_mult), # Ajuste de brillo y contraste aleatorio\n",
        "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30*scaler, val_shift_limit=20*scaler, p=0.3*proba_mult), # Ajuste de tono, saturación y valor\n",
        "    A.RandomCrop(width=200, height=200, p=0.3*proba_mult), # Recorte aleatorio a 200x200\n",
        "    A.GaussianBlur(blur_limit=(3, 5), p=0.2),   # Desenfoque Gaussiano\n",
        "    A.RandomScale(scale_limit=0.1, p=0.3*proba_mult),      # Escalado aleatorio con un límite de 30%\n",
        "    A.CLAHE(clip_limit=2.0, p=0.8),             # Ecualización CLAHE\n",
        "    # Se agrega CLAHE para compensar algunos valores extremos (brillo, contraste) fruto del 'scaler' y el aumento de la probabilidad\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "956e89eb",
      "metadata": {
        "id": "956e89eb"
      },
      "outputs": [],
      "source": [
        "def apply_transformations(original_image, algo: int = 1) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Applies transformations to an input image using Albumentations.\n",
        "\n",
        "    Args:\n",
        "        original_image (PIL.Image.Image): The original image to transform.\n",
        "        algo (int, optional): The transformation strategy to use. Defaults to 1.\n",
        "            - 1: Applies the first set of transformations (`transform`).\n",
        "            - 2: Applies the second set of transformations (`transform2`).\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: The transformed image as a NumPy array.\n",
        "    \"\"\"\n",
        "    # Convertir la imagen original a un array NumPy para la transformación\n",
        "    original_image_array = np.array(original_image)\n",
        "\n",
        "    algo_opt = [1, 2]\n",
        "    # Aplicar las transformaciones\n",
        "    if algo == algo_opt[0]:\n",
        "        transformed = transform(image=original_image_array)\n",
        "    elif algo == algo_opt[1]:\n",
        "        transformed = transform2(image=original_image_array)\n",
        "    else:\n",
        "        print(\"Debe seleccionar entre las siguientes opciones:\")\n",
        "        for opt in algo_opt:\n",
        "            print(f'Opción: `algo={opt}`')\n",
        "            if opt == algo_opt[0]:\n",
        "                print(transform)\n",
        "            elif opt == algo_opt[1]:\n",
        "                print(transform2)\n",
        "            print()\n",
        "    return transformed['image']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a83e98ce",
      "metadata": {
        "id": "a83e98ce"
      },
      "source": [
        "----\n",
        "## Procesamiento del dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afa6d546",
      "metadata": {
        "id": "afa6d546"
      },
      "source": [
        "### Funciones necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fca5f8af",
      "metadata": {
        "id": "fca5f8af"
      },
      "outputs": [],
      "source": [
        "import humanize\n",
        "\n",
        "# Traducir humanize al español\n",
        "try:\n",
        "    humanize.i18n.activate(\"es\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Paquete de idioma español para humanize no encontrado, usando inglés.\")\n",
        "    humanize.i18n.activate(\"en_US\") # Fallback a inglés"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18ad052a",
      "metadata": {
        "id": "18ad052a"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "#from collections import defaultdict\n",
        "\n",
        "def cluster_by_count_distribution(data_string, std_dev_multiplier=1.0, verbose = True):\n",
        "    \"\"\"\n",
        "    Agrupa categorías basándose en la distribución de sus conteos.\n",
        "\n",
        "    Agrupa elementos con conteos similares utilizando diferencias logarítmicas.\n",
        "    Identifica valores atípicos con conteos significativamente diferentes como clústeres separados.\n",
        "\n",
        "    Args:\n",
        "        data_string (str): Una cadena multilínea donde cada línea contiene\n",
        "                           un nombre de categoría y su conteo, separados por espacios.\n",
        "        std_dev_multiplier (float): Multiplicador para la desviación estándar utilizado\n",
        "                                    en el cálculo del umbral para identificar\n",
        "                                    rupturas de clúster. Valores más altos generan menos\n",
        "                                    clústeres más grandes. Por defecto es 1.0.\n",
        "\n",
        "    Returns:\n",
        "        list: Una lista de clústeres, donde cada clúster es una lista de\n",
        "              tuplas (nombre_categoria, conteo).\n",
        "              Devuelve una lista vacía si falla el análisis o no se encuentra ningún dato.\n",
        "    \"\"\"\n",
        "    # --- 1. Analizar los datos de entrada ---\n",
        "    data = {}\n",
        "    # Expresión regular para capturar el nombre de la categoría (que puede contener espacios/caracteres especiales)\n",
        "    # y el conteo al final\n",
        "    # Maneja posibles espacios adicionales.\n",
        "    pattern = re.compile(r'^(.*?)\\s+(\\d+)$')\n",
        "    lines = data_string.strip().split('\\n')\n",
        "\n",
        "    if not lines or (len(lines) == 1 and not lines[0].strip()):\n",
        "        print(\"Error: La cadena de datos de entrada está vacía o no es válida.\")\n",
        "        return []\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue  # Saltar líneas vacías\n",
        "        match = pattern.match(line)\n",
        "        if match:\n",
        "            # El grupo 1 captura todo antes del último bloque de dígitos\n",
        "            name = match.group(1).strip()\n",
        "            # El grupo 2 captura el último bloque de dígitos\n",
        "            count = int(match.group(2))\n",
        "            data[name] = count\n",
        "        else:\n",
        "            print(f\"Advertencia: No se pudo analizar la línea: '{line}'\")\n",
        "\n",
        "    if not data:\n",
        "        print(\"Error: No se analizaron datos válidos de la cadena de entrada.\")\n",
        "        return []\n",
        "\n",
        "    # --- 2. Ordenar los datos ---\n",
        "    # Convertir a una lista de tuplas y ordenar por conteo en orden descendente\n",
        "    # Esto facilita la comparación de elementos consecutivos.\n",
        "    sorted_items = sorted(data.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "    # Manejar el caso con solo un elemento\n",
        "    if len(sorted_items) <= 1:\n",
        "        print(\"Solo se encontró un punto de datos, devolviéndolo como un único clúster.\")\n",
        "        return [sorted_items]\n",
        "\n",
        "    # --- 3. Calcular diferencias logarítmicas ---\n",
        "    # Extraer los conteos en un array de numpy\n",
        "    counts = np.array([item[1] for item in sorted_items])\n",
        "\n",
        "    # Calcular el logaritmo de los conteos. La transformación logarítmica ayuda a normalizar\n",
        "    # grandes diferencias y centrarse en cambios relativos.\n",
        "    # Se agrega un pequeño epsilon para evitar log(0) si los conteos pueden ser cero.\n",
        "    epsilon = 1e-9\n",
        "    log_counts = np.log(counts + epsilon)\n",
        "\n",
        "    # Calcular las diferencias entre los logaritmos consecutivos de los conteos.\n",
        "    # Dado que los datos están ordenados de forma descendente, una gran diferencia positiva aquí\n",
        "    # indica una gran caída en el valor del conteo.\n",
        "    # diff[i] = log_counts[i] - log_counts[i+1]\n",
        "    log_diffs = -np.diff(log_counts)\n",
        "\n",
        "    # --- 4. Determinar rupturas de clúster ---\n",
        "    # Identificar saltos significativos en los conteos logarítmicos utilizando un umbral basado en\n",
        "    # la media y la desviación estándar de las diferencias logarítmicas.\n",
        "    # Las diferencias mayores que el umbral sugieren un punto de ruptura para un nuevo clúster.\n",
        "    if len(log_diffs) > 0:\n",
        "        mean_log_diff = np.mean(log_diffs)\n",
        "        std_log_diff = np.std(log_diffs)\n",
        "        # Establecer el umbral: media + multiplicador * desviación estándar\n",
        "        # Ajustar el multiplicador para cambiar la sensibilidad (mayor = menos sensible)\n",
        "        threshold = mean_log_diff + std_dev_multiplier * std_log_diff\n",
        "        if verbose:\n",
        "            print(f\"- Diferencias logarítmicas: {np.round(log_diffs, 2)}\")\n",
        "            print(f\"- Diferencia logarítmica media: {mean_log_diff:.2f}\")\n",
        "            print(f\"- Desviación estándar de las diferencias logarítmicas: {std_log_diff:.2f}\")\n",
        "            print(f\"- Umbral (Media + {std_dev_multiplier:.1f}*SD): {threshold:.2f}\")\n",
        "\n",
        "        # Encontrar índices *donde* la diferencia excede el umbral.\n",
        "        # Esto indica una ruptura *después* de este índice en la lista ordenada.\n",
        "        # np.where devuelve una tupla de arrays, necesitamos el primer array.\n",
        "        break_after_indices = np.where(log_diffs >= threshold)[0]\n",
        "        # El nuevo clúster comienza en el índice *siguiente*.\n",
        "        break_start_indices = break_after_indices + 1\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"- Puntos de quiembre detectados después de los elementos en los índices: {break_after_indices}\")\n",
        "            print(f\"- Los nuevos clústeres comienzan en los índices: {break_start_indices}\")\n",
        "    else:\n",
        "        # No hay diferencias para calcular si hay <= 1 elemento\n",
        "        break_start_indices = []\n",
        "        print(\"No hay suficientes puntos de datos para calcular diferencias.\")\n",
        "\n",
        "    # --- 5. Agrupar en clústeres ---\n",
        "    clusters = []\n",
        "    start_index = 0\n",
        "    # Iterar a través de los puntos de ruptura identificados (donde comienzan nuevos clústeres)\n",
        "    for break_idx in break_start_indices:\n",
        "        # Agregar el segmento antes de la ruptura como un clúster\n",
        "        clusters.append(sorted_items[start_index:break_idx])\n",
        "        # Actualizar el índice de inicio para el próximo clúster\n",
        "        start_index = break_idx\n",
        "    # Agregar los elementos restantes (desde el último punto de ruptura hasta el final) como el clúster final\n",
        "    clusters.append(sorted_items[start_index:])\n",
        "\n",
        "    return clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f5657f0",
      "metadata": {
        "id": "1f5657f0"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def plot_clusters(df, target, found_clusters):\n",
        "    df_graph = df.copy()\n",
        "\n",
        "    # 1. Crear un mapeo del nombre de la categoría al ID del clúster\n",
        "    category_to_cluster_id = {}\n",
        "    for cluster_id, cluster_list in enumerate(found_clusters):\n",
        "        for category_name, _ in cluster_list:  # Solo necesitamos el nombre aquí\n",
        "            category_to_cluster_id[category_name] = cluster_id  # Asignar 0, 1, 2...\n",
        "\n",
        "    # 2. Usar este mapeo para crear la columna 'cluster' en df_graph\n",
        "    df_graph['cluster'] = df_graph[target].map(category_to_cluster_id)\n",
        "\n",
        "\n",
        "    # Opcional: Convertir los IDs de clúster a tipo string para etiquetas de leyenda más claras\n",
        "    n_clusters = len(df_graph['cluster'].unique())\n",
        "    # Map numerical cluster IDs to textual labels (e.g., \"Cluster 1\", \"Cluster 2\", ...)\n",
        "    cluster_id_to_text = {cluster_id: f\"Cluster {cluster_id + 1}\" for cluster_id in range(n_clusters)}\n",
        "    df_graph['cluster'] = df_graph['cluster'].map(cluster_id_to_text)\n",
        "\n",
        "    # Sort the clusters in descending order based on their IDs\n",
        "    df_graph['cluster'] = pd.Categorical(df_graph['cluster'],\n",
        "                                        categories=[f\"Cluster {i + 1}\" for i in range(n_clusters)],\n",
        "                                        ordered=True)\n",
        "\n",
        "    # Obtener el conteo de cada clase\n",
        "    class_counts = df_graph[target].value_counts().sort_values(ascending=False)\n",
        "\n",
        "    # Obtener la lista ordenada de clases basada en el conteo\n",
        "    ordered_classes_by_count = class_counts.index.tolist()\n",
        "\n",
        "    # Configurar el tamaño de la figura\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Crear un gráfico de barras (usando el argumento data= para mayor claridad)\n",
        "    sns.countplot(y=target, order=ordered_classes_by_count, hue='cluster',\n",
        "                data=df_graph, palette=\"Spectral\", dodge=False)\n",
        "\n",
        "    plt.title(\"Gráfico de barras de la distribución de clases objetivo coloreado por clústeres (basado en el conteo)\")\n",
        "    plt.ylabel(\"Clases objetivo\")\n",
        "    plt.xlabel(\"Conteo\")\n",
        "    plt.legend(title='ID del clúster')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ba74686",
      "metadata": {
        "id": "4ba74686"
      },
      "outputs": [],
      "source": [
        "def filter_classes(df, target, range: tuple = (0,None)):\n",
        "    lim_min, lim_max = range\n",
        "    counts = df[target].value_counts()\n",
        "    if lim_min == None:\n",
        "        lim_min  = 0\n",
        "    if lim_max == None:\n",
        "        lim_max  = counts.max()\n",
        "    #print(\"Limites:\", lim_min, lim_max)\n",
        "\n",
        "    selection = []\n",
        "    for idx, count in enumerate(counts):\n",
        "        if lim_min < count <= lim_max:\n",
        "            clase = counts.index[idx]\n",
        "            selection.append(clase)\n",
        "    return selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6927b8c",
      "metadata": {
        "id": "b6927b8c"
      },
      "outputs": [],
      "source": [
        "def gen_rnd_id():\n",
        "    \"\"\"\n",
        "        Genera un número aleatorio de 6 cífras que se usará para diferenciar imagenes procesados de las originales\n",
        "    \"\"\"\n",
        "    rnd_seed = round(time.time() * 1e10) # Use clock as seed generator\n",
        "    random.seed(rnd_seed)  # Set this value as a new seed (assure better randomization)\n",
        "    sampled_numbers = random.sample(range(int(1e6)), 1)  # Generates a random number\n",
        "    return str(sampled_numbers[0]).zfill(6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b206ffb",
      "metadata": {
        "id": "8b206ffb"
      },
      "outputs": [],
      "source": [
        "def numoji(numero):\n",
        "  \"\"\"\n",
        "  Convierte un número entero del 1 al 10 a su emoji correspondiente.\n",
        "\n",
        "  Args:\n",
        "    numero: Un entero entre 1 y 10.\n",
        "\n",
        "  Returns:\n",
        "    Un string con el emoji correspondiente al número, o \"0️⃣\" si el número\n",
        "    está fuera del rango.\n",
        "  \"\"\"\n",
        "  if 1 <= numero <= 10:\n",
        "    emoji_map = {\n",
        "        1: \"1️⃣\",\n",
        "        2: \"2️⃣\",\n",
        "        3: \"3️⃣\",\n",
        "        4: \"4️⃣\",\n",
        "        5: \"5️⃣\",\n",
        "        6: \"6️⃣\",\n",
        "        7: \"7️⃣\",\n",
        "        8: \"8️⃣\",\n",
        "        9: \"9️⃣\",\n",
        "        10: \"🔟\"\n",
        "    }\n",
        "    return emoji_map[numero]\n",
        "  else:\n",
        "    return \"*️⃣\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93d6e93b",
      "metadata": {
        "id": "93d6e93b"
      },
      "outputs": [],
      "source": [
        "def show_test_mode_alert(spacer2):\n",
        "    print()\n",
        "    print(spacer2,(\"⚠️\"*31))\n",
        "    print(spacer2,\"⚠️\",(\" \"*46),\"⚠️\")\n",
        "    print(f\"{spacer2} ⚠️      🚨     MODO TESTING ACTIVADO      🚨       ⚠️\")\n",
        "    print(spacer2,\"⚠️\",(\" \"*46),\"⚠️\")\n",
        "    print(f\"{spacer2} ⚠️ (+) Se omite la apertura de imágenes           ⚠️\")\n",
        "    print(f\"{spacer2} ⚠️ (+) Se omite las creación de transformaciones  ⚠️\")\n",
        "    print(spacer2,\"⚠️\",(\" \"*46),\"⚠️\")\n",
        "    print(f\"{spacer2} ⚠️    ESTE PROCESO NO GENERARÁ ARCHIVOS REALES    ⚠️\")\n",
        "    print(spacer2,\"⚠️\",(\" \"*46),\"⚠️\")\n",
        "    print(spacer2,(\"⚠️\"*31))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b3ee624",
      "metadata": {
        "id": "5b3ee624"
      },
      "source": [
        "## Creación de directorios y estructuras de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5985a44",
      "metadata": {
        "id": "e5985a44"
      },
      "outputs": [],
      "source": [
        "# Se ha definido una nueva constante con la ubicación del dataset aumentado (será almacenadad en YAML)\n",
        "AUG_PATH = SPLITTED_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9895aa32",
      "metadata": {
        "id": "9895aa32"
      },
      "source": [
        "Luego, se aplican las transformaciones para aumentación de datos a la copia del dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fe57e92",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "id": "2fe57e92",
        "outputId": "4f9c3c96-b5bf-485b-fba1-1aa4a098e1a7"
      },
      "outputs": [],
      "source": [
        "# Agrega columnas para indicar procesamiento\n",
        "## original -> imagen sin transofrmaciones\n",
        "## augmented -> existen aumentaciones derivadas\n",
        "df_split['augmented'] = False\n",
        "df_split['is_original'] = True\n",
        "\n",
        "# Crea un nuevo dataframe para gesitonar el procesamiento\n",
        "process_split = df_split[df_split['split']=='train'].copy()\n",
        "process_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e71f579d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 976
        },
        "id": "e71f579d",
        "outputId": "de1b5240-7899-4769-ac4a-5ccabab02fcb"
      },
      "outputs": [],
      "source": [
        "# Crea una columna 'ref' que apunta al id de la imagen original\n",
        "process_split['ref'] = process_split.index\n",
        "process_split"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5ab86f5",
      "metadata": {
        "id": "e5ab86f5"
      },
      "source": [
        "## Estrategia de aumentación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b04ebf55",
      "metadata": {
        "id": "b04ebf55"
      },
      "outputs": [],
      "source": [
        "# Se define el feature a partir del cual se aplicarán las aumentaciones\n",
        "target = 'class' # class / group\n",
        "#data = df.copy()\n",
        "data = process_split.copy()\n",
        "#data = df_augmented.copy()\n",
        "#data = processed_split.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54b1a017",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54b1a017",
        "outputId": "d39fd59c-2a39-497c-b63f-5c4381641858"
      },
      "outputs": [],
      "source": [
        "# Resumen distribucion por grupo\n",
        "print(f\"Conteo por '{target}':\")\n",
        "counts = data[target].value_counts()\n",
        "for class_name, count in counts.items():\n",
        "    print(f\" - {class_name}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64604549",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64604549",
        "outputId": "da1fd330-f646-435c-a31d-3ab29768dd01"
      },
      "outputs": [],
      "source": [
        "def find_clusters(data):\n",
        "    # Convierte el diccionario a una cadena multilínea\n",
        "    data_string = dict(data.value_counts())\n",
        "    data_string = \"\\n\".join(f\"{key} {value}\" for key, value in data_string.items())\n",
        "\n",
        "    # --- Ejecuta la función de agrupamiento ---\n",
        "    # Puedes ajustar el std_dev_multiplier (por ejemplo, 0.5, 1.0, 1.5)\n",
        "    # para hacer que el agrupamiento sea más o menos sensible a las brechas de conteo.\n",
        "    # Un valor más bajo podría crear más clústeres.\n",
        "    multiplier = 0.15 # 0.5 / 0.25 / 0.1 proba distintos valores\n",
        "    found_clusters = cluster_by_count_distribution(data_string, std_dev_multiplier=multiplier,verbose=False)\n",
        "\n",
        "    # --- Muestra los resultados ---\n",
        "    print(f\"Clústeres identificados:\")\n",
        "    print(f\" · Multiplicador Desviación Estándar {multiplier}\\n\")\n",
        "    if found_clusters:\n",
        "        for i, cluster in enumerate(found_clusters):\n",
        "            # Filtrar clústeres potencialmente vacíos si el corte resultó en uno\n",
        "            if cluster:\n",
        "                print(f\"Clúster {i + 1}:\")\n",
        "                # Obtener conteos para calcular el rango (opcional para mostrar)\n",
        "                cluster_counts = [count for _, count in cluster]\n",
        "                print(f\"  (Rango de conteo: {min(cluster_counts)} - {max(cluster_counts)})\")\n",
        "                # Imprimir elementos en el clúster\n",
        "                for name, count in cluster:\n",
        "                    print(f\"  - {name}: {count}\")\n",
        "                print()\n",
        "        return found_clusters\n",
        "    else:\n",
        "        print(\"No se identificaron clústeres.\")\n",
        "        return None\n",
        "\n",
        "found_clusters = find_clusters(data[target])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac9a905b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        },
        "id": "ac9a905b",
        "outputId": "46d2c10d-2529-452c-cbf3-cdabe2f25c4c"
      },
      "outputs": [],
      "source": [
        "plot_clusters(data, target, found_clusters)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e93d82e3",
      "metadata": {
        "id": "e93d82e3"
      },
      "source": [
        "Se identifican las siguientes distribuciones:\n",
        "- **Para target `class`:**\n",
        "    1. **Clase mayoritaria** 'Soybean___healthy' con ~4.5k\n",
        "    1. Grupo 1 con rango de > 4k *(3 clases)*\n",
        "    1. Grupo 2 con rango de ~1k - 2k ***ascendente***\n",
        "    1. Grupo 3 con rango de ~500 - 1k\n",
        "        1. Mitad sueprior ***ascendente*** con rango de >800k\n",
        "        1. Mitad inferior ***constante*** ~800 *(8 clases)*\n",
        "    1. Grupo 4 con rango de ~400 - 500 *(2 clases)*\n",
        "    1. Grupo 5 con rango de ~400 *(3 clases)* ***ascendente***\n",
        "    1. Grupo 6 con rango de ~300 *(3 clases)* ***constante***\n",
        "    1. Clase aislada de ~200\n",
        "    1. **Clase minoritaria** 'Potato___healthy' con ~100\n",
        "\n",
        "- **Para target `group`:**\n",
        "    1. **Clase mayoritaria** 'Tomato' con ~14.5k\n",
        "    1. Grupo 1 con rango de ~4k *(2 clases)*\n",
        "    1. Grupo 2 con rango de ~3k *(2 clases)*\n",
        "    1. Grupo 3 con rango de ~2k *(4 clases)*\n",
        "    1. Grupo 4 con rango de ~1.5k *(2 clases)*\n",
        "    1. Grupo 5 con rango de ~1.2k *(2 clases)*\n",
        "    1. **Clase minoritaria** 'Raspberry' con ~300"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "195ed49b",
      "metadata": {
        "id": "195ed49b"
      },
      "source": [
        "**Estrategia 1:**\n",
        "- Se dejan sin procesar los primeros 3 grupos *(>4k)* **⇢ =**\n",
        "- Para los grupos 1 y 2 *(2k–4k)* se aplica **⇢ x2**\n",
        "- Para los grupos 3 a 6 *(500–2k)* se aplica **⇢ x3**\n",
        "- Para la clase minotirtaria *(<500)* se aplica **⇢ x6**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d5c242a",
      "metadata": {
        "id": "2d5c242a"
      },
      "outputs": [],
      "source": [
        "# ESTRATEGIA PARA AUMENTACIÓN POR 'GRUPO' (estrategia 1)\n",
        "# Clases a las que se le aplicará aumentación de datos\n",
        "# Entre 4000 y 2000\n",
        "group1 = {\n",
        "    'classes': filter_classes(df_split,target,(2000,4000)),\n",
        "    'increase': 2 # 2x data augmentation (original + transf)\n",
        "}\n",
        "# Menores a 2000\n",
        "group2 = {\n",
        "    'classes': filter_classes(df_split,target,(500,2000)),\n",
        "    'increase': 3 # 3x data augmentation (original +  2 transf)\n",
        "}\n",
        "\n",
        "# Clase mínima (Raspberry)\n",
        "group3 = {\n",
        "    'classes': filter_classes(df_split,target,(0,500)),\n",
        "    'increase': 6 # 6x data augmentation (original +  5 transf)\n",
        "}\n",
        "\n",
        "estrategy1 = [group1, group2, group3]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88457edd",
      "metadata": {
        "id": "88457edd"
      },
      "source": [
        "**Estrategia 2:**\n",
        "\n",
        "A diferencia de la estrategia anterior, además de la aumentanción, se aplicará la técnica de  submuestreo (*undersampling*) para las clases mayoritarias.\n",
        "\n",
        "Se intentará balancear todas las clases entorno a 2k samples.\n",
        "\n",
        "- Submuestreo del grupo 1 *(>4k)* **⇢ ÷2**\n",
        "- Para el grupo 2 *(1k-2k)* se aumentará **⇢ x2** y luego undersampling **⇢ =2k**\n",
        "- Para el grupos 3 *(500–1k)* se aplica **⇢ x2**\n",
        "- Para el grupos 4 *(400-500)* se aplica **⇢ x4**\n",
        "- Para el grupos 5 *(~400)* se aplica **⇢ x5**\n",
        "- Para el grupos 6 *(~300)* se aplica **⇢ x6**\n",
        "- Para las clases minoritarias:\n",
        "    - Cluster 7 *(~200)* se aplica **⇢ x10** *(aumentando variabilidad)*\n",
        "    - Cluster 8 *(~100)* se aplica **⇢ x20** *(aumentando variabilidad)*\n",
        "\n",
        "Para las clases minoritarias se modificará la función de transformación incrementando la probabilidad para aumentar la variabilidad en la generación y evitar así el overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de5cf8c4",
      "metadata": {
        "id": "de5cf8c4"
      },
      "outputs": [],
      "source": [
        "# ESTRATEGIA PARA AUMENTACIÓN POR 'CLASE' (estrategia 2)\n",
        "# Clases a las que se le aplicará aumentación de datos\n",
        "# Entre 4000 y 2000\n",
        "\n",
        "# Grupo 1 (>4k): Requiere submuestreo (÷2), no aumentación. Se maneja por separado.\n",
        "# Rango original: 1000 a 2000 muestras\n",
        "group1 = {\n",
        "    'classes': filter_classes(process_split, target, (3000, 10000)), # Clases por encima de 3000\n",
        "    'increase': 0,\n",
        "    'limit': 2000 # ~ undersampling ÷2\n",
        "}\n",
        "\n",
        "# Grupo 2: Aumentación x2 (target original * 2)\n",
        "# Rango original: 1000 a 2000 muestras\n",
        "group2 = {\n",
        "    'classes': filter_classes(process_split, target, (1000, 3000)), # Clases entre 1000 y 3000 (exclusive el 2000)\n",
        "    'increase': 2, # 2x data augmentation (original + 1 transf).\n",
        "    'limit': 2000 # Post-tope a 2k\n",
        "}\n",
        "\n",
        "# Grupo 3: Aumentación x2\n",
        "# Rango original: 500 a 1000 muestras\n",
        "group3 = {\n",
        "    'classes': filter_classes(process_split, target, (550, 1000)), # Clases entre 500 y 1000 (exclusive el 1000)\n",
        "    'increase': 2 # 2x data augmentation (original + 1 transf)\n",
        "}\n",
        "\n",
        "# Grupo 4: Aumentación x4\n",
        "# Rango original: 400 a 500 muestras\n",
        "group4 = {\n",
        "    'classes': filter_classes(process_split, target, (450, 550)), # Clases entre 400 y 500 (exclusive el 500)\n",
        "    'increase': 4 # 4x data augmentation (original + 3 transf)\n",
        "}\n",
        "\n",
        "# Grupo 5: Aumentación x5\n",
        "# Rango original: ~400 muestras (interpretado como 300 a 400)\n",
        "group5 = {\n",
        "    'classes': filter_classes(process_split, target, (300, 450)), # Clases entre 300 y 400 (exclusive el 400)\n",
        "    'increase': 5 # 5x data augmentation (original + 4 transf)\n",
        "}\n",
        "\n",
        "# Grupo 6: Aumentación x6\n",
        "# Rango original: ~300 muestras (interpretado como 200 a 300)\n",
        "group6 = {\n",
        "    'classes': filter_classes(process_split, target, (250, 300)), # Clases entre 200 y 300 (exclusive el 300)\n",
        "    'increase': 6 # 6x data augmentation (original + 5 transf)\n",
        "}\n",
        "\n",
        "# Grupo 7 (Minoritaria 1): Aumentación x10\n",
        "# Rango original: ~200 muestras (interpretado como 100 a 200)\n",
        "group7 = {\n",
        "    'classes': filter_classes(process_split, target, (150, 250)), # Clases entre 100 y 200 (exclusive el 200)\n",
        "    'increase': 10, # 10x data augmentation (original + 9 transf)\n",
        "    'algo': 2   # Algoritmo alternativo buscando variabilidad\n",
        "}\n",
        "\n",
        "# Grupo 8 (Minoritaria 2): Aumentación x20\n",
        "# Rango original: ~100 muestras (interpretado como 0 a 100)\n",
        "group8 = {\n",
        "    'classes': filter_classes(process_split, target, (0, 150)), # Clases entre 0 y 100 (exclusive el 100)\n",
        "    'increase': 20, # 20x data augmentation (original + 19 transf)\n",
        "    'algo': 2   # Algoritmo alternativo buscando variabilidad\n",
        "}\n",
        "\n",
        "estrategy2 = [group1, group2, group3, group4, group5, group6, group7, group8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de94c83a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de94c83a",
        "outputId": "f8b3b5c8-b235-474e-eecc-f9c37cbc8e38"
      },
      "outputs": [],
      "source": [
        "print(\"Efecto de la aumentación de datos para 'train' split\\n\")\n",
        "counts = process_split[target].value_counts()\n",
        "\n",
        "groups = estrategy2 # Estrategia 2\n",
        "for group in groups:\n",
        "    mult = group['increase']\n",
        "    print(f\"Estrategia de aumentación {mult}x:\")\n",
        "    for clase in group['classes']:\n",
        "        print(f\" - {clase}: {counts[clase]} -> {counts[clase]*mult}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd858a04",
      "metadata": {
        "id": "bd858a04"
      },
      "source": [
        "## Procesamiento de aumentaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94cf6517",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94cf6517",
        "outputId": "e71cfa2d-eab5-4e72-c485-5df8b341558b"
      },
      "outputs": [],
      "source": [
        "# PROCESAMIENTO DE AUMENTACIONES CON FILTRADO POR CLASE\n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "# SETUP INCIAL:\n",
        "verbose = True # Incluye detalle del proceso y estimación de tiempo\n",
        "testing = False # No modifica archivos\n",
        "debugging = False # Incluye detalle de archivos y mensajes de error\n",
        "estrategy = estrategy2 # Estrategia a aplicar: estrategy1 | estrategy2\n",
        "processes = estrategy.copy() # Para evitar modificación cruzada\n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "for_processing_classes = [cls for group in groups for cls in group['classes']]\n",
        "for_processing_data = process_split[process_split[target].isin(for_processing_classes)]\n",
        "n_transformations = 0\n",
        "without_transfromations = False\n",
        "for process in processes:\n",
        "    if process['increase'] !=0:\n",
        "        classes, increase = process['classes'], process['increase']\n",
        "        images_in_classes = for_processing_data[for_processing_data[target].isin(classes)]\n",
        "        n_transformations += len(images_in_classes) * (increase - 1)\n",
        "    else:\n",
        "        print(f\"NO AUGMENTATIONS NEEDED FOR:\\n{process}\\n\") if debugging else None\n",
        "        processes.remove(process)\n",
        "\n",
        "print(f\"Iniciando proceso de aumentación de datos…\")\n",
        "folder = f'{AUG_PATH}train/'\n",
        "print(f\"Aumentando imágenes para:\\n >>\", folder)\n",
        "total_files = len(for_processing_data) # Total de archivos del dataset\n",
        "print(f\" - Total de archivos en el dataset 'train': {len(process_split)}\")\n",
        "print(f\" - Total de archivos a procesar: {total_files} ({(total_files/len(process_split)*100):.0f}% del dataset)\")\n",
        "print(f\" - Transformaciones totales: {n_transformations} imágenes serán creadas\\n\")\n",
        "\n",
        "if n_transformations > 0:\n",
        "    print(f\"Se procesarán un total de {len(for_processing_classes)} clases de {len(process_split[target].value_counts())}:\")\n",
        "    for label in for_processing_classes:\n",
        "        print(\" - \",label)\n",
        "    print()\n",
        "    for i, process in enumerate(processes):\n",
        "        print(f\"· Estrategia {i+1}: {process['increase']}x augmentation para {process['classes']}\")\n",
        "    print(\"\\n¿Deseas iniciar el procesamiento?\")\n",
        "    time.sleep(1) # Para asegurar que se muestren los prints\n",
        "\n",
        "    # Input de confirmación del usuario\n",
        "    confirmacion = input(f\"⚠️ ATENCIÓN: El proceso puede demorar varios minutos. Se requiere confirmación para continuar [Y/N]: \").strip().lower()\n",
        "    if confirmacion != 'y':\n",
        "        print(f\"\\n⛔️ La ejecución ha sido denegada por el usuario.\")\n",
        "    else:\n",
        "        start_time = time.perf_counter() # Medición del tiempo\n",
        "        partial_time = start_time #?\n",
        "\n",
        "        # PROCESAMIENTO DE AUMENTACIONES\n",
        "        # Leyenda:\n",
        "        #  count -> archivos originales procesados (total)\n",
        "        #  work -> transformaciones por clase (parcial)\n",
        "        #  task -> número de transformaciones por archivo\n",
        "\n",
        "        # Duplica dataframe para almacenar el proceso\n",
        "        processed_split = process_split.copy()\n",
        "\n",
        "        # Realiza el proceso de copiado de archivos para cada grupo\n",
        "        succeeded_process = True\n",
        "        count = 0\n",
        "        total_work_done = 0\n",
        "        avg_time = 0\n",
        "        partial_time = 0\n",
        "        previous_count = 0\n",
        "        spacer1 = '   '\n",
        "        spacer2 = '      '\n",
        "\n",
        "        for i, process in enumerate(processes):\n",
        "            mult = process['increase']\n",
        "            n_augmentations = mult - 1 # se resta 1 porque la imagen original ya existe\n",
        "            classes = process['classes']\n",
        "            print(f\"\\n\\n{numoji(i+1)} Iniciando subproceso de aumentación:\")\n",
        "            print(f\" - Estrategia de aumentación: {mult}x\")\n",
        "            if process.get('algo'):\n",
        "                algo = process['algo']\n",
        "                print(f\" - Aplicando el algoritmo {algo}\")\n",
        "            else:\n",
        "                algo = 1\n",
        "            print(f\" - Clases a procesar: {len(classes)} grupos\\n   {classes}\")\n",
        "\n",
        "            for clase in classes:\n",
        "                # Filtrado de clases\n",
        "                subprocess_split = process_split[process_split[target]==clase]\n",
        "                total_work = len(subprocess_split) * n_augmentations # Conteo de archivos para la clase\n",
        "                print(f\"\\n{spacer1}🔄 Procesando clase '{clase}' ({(total_work/total_files*100):.0f}% del total):\")\n",
        "                print(f\"{spacer1}  - Transformaciones a generar: {total_work}\") if verbose else None\n",
        "                work = 0\n",
        "\n",
        "                for index, image_row in subprocess_split.iterrows():\n",
        "                    show_test_mode_alert(spacer2) if work==0 and testing else None\n",
        "                    # 1. Apertura del archivo de imagen original\n",
        "                    original_image = load_image_idx(data=image_row,root=folder) if not testing else None\n",
        "                    #original_image.show() # Debugging\n",
        "\n",
        "                    # Genera múltiples transformaciones para cada imagen\n",
        "                    try:\n",
        "                        task = n_augmentations\n",
        "                        for n in range(n_augmentations):\n",
        "\n",
        "                            # 2. Procesamiento de la transformacion\n",
        "                            transformed_image = apply_transformations(original_image,algo=algo) if not testing else None\n",
        "                            try:\n",
        "                                # Convierte el array NumPy a un objeto Image de PIL\n",
        "                                if not testing:\n",
        "                                    if transformed_image.ndim == 2:  # Escala de grises\n",
        "                                        processed_image = Image.fromarray(transformed_image.astype(np.uint8), 'L')\n",
        "                                    elif transformed_image.ndim == 3:  # RGB\n",
        "                                        if transformed_image.shape[2] == 3:\n",
        "                                            processed_image = Image.fromarray(transformed_image.astype(np.uint8), 'RGB')\n",
        "                                        else:\n",
        "                                            raise ValueError(f\"{spacer2}❗️ El array NumPy debe tener 2 (escala de grises) o 3 (RGB) canales.\")\n",
        "                                    else:\n",
        "                                        raise ValueError(f\"{spacer2}❗️ El array NumPy debe tener 2 o 3 dimensiones.\")\n",
        "                                    #processed_image.show() # Debugging\n",
        "\n",
        "                                # 3. Almacena el archivo\n",
        "                                # Genera el nombre del nuevo archivo\n",
        "                                # se incluye un int random al final (para permitir múltiples aumentaciones)\n",
        "                                rnd_num = gen_rnd_id() # string de 6 cifras\n",
        "                                name = list(os.path.splitext(image_row.filename))\n",
        "                                name.insert(1,f'-{rnd_num}')\n",
        "                                filename = \"\".join(name)\n",
        "                                # Guarda la transformación como JPG\n",
        "                                processed_image.save(f'{folder}{image_row.image_path}{filename}', \"JPEG\") if not testing else None\n",
        "                                print(f\"{spacer2}🔹 {filename}\") if debugging else None\n",
        "\n",
        "                                # 4. Registra la nueva imagen en el DataFrame\n",
        "                                new_row = image_row.copy() # Copia las etiquetas\n",
        "                                # Actualiza los valores correspondientes\n",
        "                                new_row['filename'] = filename\n",
        "                                new_row['is_original'] = False\n",
        "                                new_row['augmented'] = True\n",
        "                                # Agrega la nueva fila al DataFrame processed_split\n",
        "                                processed_split = pd.concat([processed_split, pd.DataFrame([new_row])], ignore_index=True)\n",
        "\n",
        "                                task -= 1 # cuenta una transformación finalizada\n",
        "                                work += 1 # cuenta una aumentación realizada (por clase)\n",
        "                                if work % 100 == 0:\n",
        "                                    print(f'{spacer2}🔨 » Progreso subproceso: {work/total_work*100:.2f}%') if verbose else None\n",
        "\n",
        "                            except FileExistsError:\n",
        "                                print(f\"{spacer2}❌ Error: El archivo de destino '{destination_folder}' ya existe.\\n\") if debugging else None\n",
        "                                continue\n",
        "                            except Exception as e:\n",
        "                                print(f\"{spacer2}❌ Error al guardar la imagen como JPG: {e}\") if debugging else None\n",
        "                                continue\n",
        "                    finally:\n",
        "                        # 5. Actualiza el registro de la imagen original\n",
        "                        processed_split.loc[index, 'augmented'] = True\n",
        "                        #print(processed_split.iloc[index])\n",
        "\n",
        "                        count += 1 # cuenta un archivo finalizado\n",
        "                        if task == 0:\n",
        "                            pass\n",
        "                        else:\n",
        "                            print(f\"{spacer2}‼️ ALERTA: No se ha completado la tarea para {image_row.filename}\", \"[Activar opción 'debugging' para más detalle]\" if not debugging else None)\n",
        "\n",
        "                print(f'{spacer2}🔨 » Progreso subproceso: 100%') if verbose else None\n",
        "                total_work_done += work # registro del trabajo realizado\n",
        "\n",
        "                # LÓGICA TEMPORAL\n",
        "                if verbose:\n",
        "                    # Medición de tiempo pasado y estimación del faltante\n",
        "                    previous_time = partial_time\n",
        "                    partial_time = time.perf_counter() # Medición del tiempo\n",
        "                    divisor = count - previous_count\n",
        "                    elapsed_time = (partial_time - start_time)\n",
        "                    task_time = elapsed_time/divisor if divisor > 0 else 0\n",
        "                    avg_time = elapsed_time/total_work_done if total_work_done > 0 else 0 # Tiempo medio para estimación\n",
        "\n",
        "                    print(f'{spacer2}⏱️ · Tiempo de subproceso {humanize.naturaldelta(task_time)}')\n",
        "                    if previous_count and verbose:\n",
        "                        if task_time > avg_time*1.1:\n",
        "                            print(f\"{spacer2}👎🏻 La tarea ha demorado más de lo esperado…\")\n",
        "                        elif task_time < avg_time*0.75:\n",
        "                            print(f\"{spacer2}👍🏻 ¡Tarea completada en tiempo record!\")\n",
        "\n",
        "                if work == total_work:\n",
        "                    print(f\"{spacer1}✅ Subproceso de transformaciones completado.\")\n",
        "                    succeeded = True\n",
        "                else:\n",
        "                    print(f\"{spacer1}‼️ ALERTA: No se pudo completar correctamente el subproceso de transformaciones.\", \"[Activar opción 'debugging' para más detalle]\" if not debugging else None)\n",
        "                    succeeded = False\n",
        "                if verbose:\n",
        "                    #print(f'\\n{spacer1}🕗 Ejecución total {elapsed_time/60:.1f} min')\n",
        "                    print(f'\\n{spacer1}🕗 Ejecución total {humanize.naturaldelta(elapsed_time)}')\n",
        "                    #print(f'{spacer1}🔮 Estimación de tiempo restante {avg_time * (n_transformations - total_work_done) / 60:.2f} min')\n",
        "                    print(f'{spacer1}🔮 Estimación de tiempo restante {humanize.naturaldelta(avg_time * (n_transformations - total_work_done))}')\n",
        "                    print(f'{spacer1}❇️ » Progreso general: {count/total_files*100:.3f}%') if (count/total_files) < 0.99 else None\n",
        "                previous_count = count\n",
        "\n",
        "                succeeded_process *= succeeded # Actualiza el estado del proceso\n",
        "                # (Sólo es 'True' si todos los subprocesos se completan correctamente)\n",
        "\n",
        "        print(f'{spacer1}❇️ » Progreso general: 100%') if not testing else None\n",
        "        end_time = time.perf_counter() # Medición del tiempo\n",
        "        total_time = end_time - start_time\n",
        "        if succeeded_process:\n",
        "            print(\"\\n🌟 El proceso de aumentación del dataset ha finalizado con éxito.\")\n",
        "        else:\n",
        "            print(\"\\n🚫 No se pudo completar satisfactoriamente el proceso de aumentación del dataset.\")\n",
        "        if verbose:\n",
        "            print(f'{spacer1}🕗 Ejecución completada en {humanize.naturaldelta(total_time)}')\n",
        "            print(f'{spacer1}🔢 · Se procesaron {total_work_done} transformaciones para un total de {total_files} imágenes.')\n",
        "            print(f'{spacer1}⏱️ · Tiempo medio por transofrmación {(total_time/total_work_done):.3f} sec')\n",
        "\n",
        "else:\n",
        "    print(\"🌟 La estrategia actual no requiere realizar ninguna transformación.\")\n",
        "    without_transfromations = True\n",
        "    processed_split = process_split.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66596957",
      "metadata": {
        "id": "66596957"
      },
      "source": [
        "# Verificación"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc73c695",
      "metadata": {
        "id": "cc73c695"
      },
      "source": [
        "### Archivos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "462cf624",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "462cf624",
        "outputId": "d99ab73b-a4e2-4aa1-fbe4-09d8e3894f02"
      },
      "outputs": [],
      "source": [
        "resut_aug_dataframe = len(processed_split) - len(process_split)\n",
        "print(f\"Luego del proceso de aumentación de datos se genraron {resut_aug_dataframe} archivos nuevos.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4aee39a7",
      "metadata": {
        "id": "4aee39a7"
      },
      "outputs": [],
      "source": [
        "def contar_archivos(ruta):\n",
        "    total = 0\n",
        "    with os.scandir(ruta) as it:\n",
        "        for entry in it:\n",
        "            if entry.is_file():\n",
        "                total += 1\n",
        "            elif entry.is_dir():\n",
        "                total += contar_archivos(entry.path)\n",
        "    return total"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb7a4f37",
      "metadata": {
        "id": "cb7a4f37"
      },
      "source": [
        "### Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37817aab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37817aab",
        "outputId": "2049de46-7a21-4319-8f9c-32fb8c840bad"
      },
      "outputs": [],
      "source": [
        "processed_split.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab5e118d",
      "metadata": {
        "id": "ab5e118d"
      },
      "outputs": [],
      "source": [
        "if not without_transfromations:\n",
        "    processed_split[(processed_split['is_original'] == False)].sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd3c1e23",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "cd3c1e23",
        "outputId": "516cec36-9ebe-4e54-9e98-07ae75f2e6db"
      },
      "outputs": [],
      "source": [
        "# Verificamos el último archivo procesado para asegurar que el dataframe fue editado correctamente\n",
        "processed_split[(processed_split['group'] == 'Raspberry') & (processed_split['ref'] == 27181)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6b76619",
      "metadata": {
        "id": "b6b76619"
      },
      "source": [
        "## Distribución por clase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb42147b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb42147b",
        "outputId": "4e50742e-c93e-4ea9-9211-4da7a9dc1d8a"
      },
      "outputs": [],
      "source": [
        "### Resumen distribucion por grupo\n",
        "print(f\"Conteo por {target}:\")\n",
        "print(processed_split[target].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f94ff79",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f94ff79",
        "outputId": "07c10678-5516-44cf-b46d-c334be271895"
      },
      "outputs": [],
      "source": [
        "print(\"Efecto de la aumentación de datos por clase:\\n\")\n",
        "counts_entrada = df_split[target].value_counts()\n",
        "counts_salida = processed_split[target].value_counts()\n",
        "\n",
        "classes = counts_salida.index\n",
        "for class_ in classes:\n",
        "    print(f\" - {class_}: {counts_entrada[class_]} -> {counts_salida[class_]} (+{(counts_salida[class_]/counts_entrada[class_]-1)*100:.0f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d61944fa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d61944fa",
        "outputId": "93b31895-fe01-4f9f-af35-7a8599627c66"
      },
      "outputs": [],
      "source": [
        "# Se define el feature a partir del cual se aplicarán las aumentaciones\n",
        "#target = 'class' # class / group\n",
        "data = processed_split.copy()\n",
        "found_clusters = find_clusters(data[target])\n",
        "plot_clusters(data, target, found_clusters)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd368525",
      "metadata": {
        "id": "dd368525"
      },
      "source": [
        "## Merge dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c206ea89",
      "metadata": {
        "id": "c206ea89"
      },
      "outputs": [],
      "source": [
        "# MERGE DEL DATAFRAME AUMENTADO CON EL ORIGINAL\n",
        "# 1. Se pararan los datos de archivos procesados\n",
        "for_update = processed_split[processed_split['is_original']==True]\n",
        "new_rows = processed_split[processed_split['is_original']==False]\n",
        "\n",
        "# 2. Se actualiza el Dataframe original con los nuevos valores\n",
        "df_split.update(for_update)\n",
        "\n",
        "# 3. Se agregan las aumentaciones con el dataframe original\n",
        "df_augmented = pd.concat([df_split, new_rows], axis=0)\n",
        "df_augmented.update(for_update)\n",
        "\n",
        "# 4. Ajuste de formato\n",
        "df_augmented['ref'] = df_augmented['ref'].astype('Int64')\n",
        "\n",
        "# 5. Eliminación de posibles entrads duplicadas\n",
        "df_augmented.drop_duplicates(subset='filename', keep='first', inplace=True)\n",
        "\n",
        "# 6. (Opcional) Reordenar por índice si querés mantener orden\n",
        "# df_merged = df_merged.sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d2f9f33",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "7d2f9f33",
        "outputId": "36710701-3499-451e-a43f-107eef31b77d"
      },
      "outputs": [],
      "source": [
        "df_augmented.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1aac14e3",
      "metadata": {
        "id": "1aac14e3"
      },
      "source": [
        "# Undersampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8078910a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8078910a",
        "outputId": "a69feed5-0fe2-47e3-9671-f06f66cb6b07"
      },
      "outputs": [],
      "source": [
        "counts = df_augmented[target].value_counts()\n",
        "undersample_works = []\n",
        "undersample_count = 0\n",
        "undersample_count_group = 0\n",
        "\n",
        "for i, process in enumerate(estrategy):\n",
        "    if process.get('limit'):\n",
        "        print(f\"{numoji(i+1)} Se aplicará submuestreo hasta {process['limit']} imágenes para las siguientes clases:\")\n",
        "        for clase in process['classes']:\n",
        "            print(f\" - {clase}: {counts[clase]} -> {process['limit']}\")\n",
        "            if counts[clase] > process['limit']:\n",
        "                undersample_count_group += counts[clase] - process['limit']\n",
        "            else:\n",
        "                print(f\"    ‼️ La clase '{clase}' posee {counts[clase]} imágenes, por lo que no es necesario aplicar submuestreo.\")\n",
        "        if undersample_count_group > 0:\n",
        "            print(\"Reducción para el grupo: \",undersample_count_group)\n",
        "            undersample_count += undersample_count_group\n",
        "            undersample_works.append(process)\n",
        "        else:\n",
        "            print(\"    🚨 CUIDADO: Aunque está configurado un proceso de undersampling, la cantidad de muestras para estas clases es menor al límite establecido.\")\n",
        "        print()\n",
        "if undersample_count:\n",
        "    print(f\"✅ Luego del proceso de undersampling se eliminarán un total de {undersample_count} imágenes.\")\n",
        "else:\n",
        "    print(f\"🌟 No se ha configurado ningún proceso de undersampling para la estrategia seleccionada.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d0518a5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d0518a5",
        "outputId": "f9518634-1518-485c-8186-b36136dc2a34"
      },
      "outputs": [],
      "source": [
        "# PROCESAMIENTO DE UNDERSAMPLING CON FILTRADO POR CLASE\n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "# SETUP INCIAL:\n",
        "#verbose = True # Incluye detalle del proceso y estimación de tiempo\n",
        "testing = False # No modifica archivos\n",
        "debugging = False # Incluye detalle de archivos y mensajes de error\n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "undersample_split = df_augmented[df_augmented['split']=='train']\n",
        "undersample_split['for_remove'] = False\n",
        "spacer1 = '    '\n",
        "spacer2 = '      '\n",
        "remove_data_work = pd.DataFrame(columns=undersample_split.columns)\n",
        "indices_to_remove_all = []\n",
        "\n",
        "for i, work in enumerate(undersample_works):\n",
        "    print(f'{numoji(i+1)} Iniciando trabajo de submuestreo para {len(work)} clases:')\n",
        "    limit = work['limit']\n",
        "    print(f' · Límite establecido para undersampling: {limit}\\n')\n",
        "\n",
        "    for clase in work['classes']:\n",
        "        indices_to_remove_class = []\n",
        "        class_indices = undersample_split[undersample_split['class'] == clase].index\n",
        "\n",
        "        removing_message = lambda remove_data: f\"{spacer1}> Serán eliminadas: {len(remove_data)} imágenes\"\n",
        "        data = undersample_split.loc[class_indices]\n",
        "        data_orig = data[data['is_original'] == True]\n",
        "        data_aug = data[data['is_original'] == False]\n",
        "        print(f'  - {clase}: contiene {len(data)} imágenes ({len(data_orig)}+{len(data_aug)})')\n",
        "\n",
        "        if len(data_orig) > limit:\n",
        "            print(f\"{spacer1}Data original supera el límite\") if debugging else None\n",
        "            print(removing_message(data_orig.iloc[limit:]))\n",
        "            remove_orig_indices = data_orig.index[limit:].tolist()\n",
        "            indices_to_remove_class.extend(remove_orig_indices)\n",
        "            if len(data_aug) != 0:\n",
        "                # Verifica que no se hayan generado aumentaciones inncesarias\n",
        "                # En caso afirmativo, las elimina también\n",
        "                remove_aug_indices = data_aug.index.tolist()\n",
        "                indices_to_remove_class.extend(remove_aug_indices)\n",
        "        elif len(data_aug) > limit:\n",
        "            print(f\"{spacer1}Data aumentada supera el límite\") if debugging else None\n",
        "            print(removing_message(data_aug.iloc[limit:]))\n",
        "            remove_orig_indices = data_aug.index[limit:].tolist()\n",
        "            indices_to_remove_class.extend(remove_orig_indices)\n",
        "        elif len(data_orig)+len(data_aug) > limit:\n",
        "            print(f\"{spacer1}Al unir ambos conjuntos se supera el límite\") if debugging else None\n",
        "            keep_aug_count = limit - len(data_orig)\n",
        "            print(f\"{spacer1}Se deben incluir\", keep_aug_count) if debugging else None\n",
        "            print(removing_message(data_aug.iloc[keep_aug_count:]))\n",
        "            remove_aug_indices = data_aug.index[keep_aug_count:].tolist()\n",
        "            indices_to_remove_class.extend(remove_aug_indices)\n",
        "        else:\n",
        "            print(f\"No es necesario aplicar undersampling. La cantidad de imágenes {len(data)} no supera el límite establecido.\")\n",
        "\n",
        "        if indices_to_remove_class:\n",
        "             print(f\"{spacer1}Total marcado para remover en clase '{clase}': {len(indices_to_remove_class)}\") if debugging else None\n",
        "             indices_to_remove_all.extend(indices_to_remove_class)\n",
        "    print()\n",
        "\n",
        "#====================================================================================================\n",
        "# --- Post-procesamiento después de iterar a través de todos los trabajos ---\n",
        "# Elimina duplicados entre diferentes trabajos/clases\n",
        "indices_to_remove_all = list(set(indices_to_remove_all))\n",
        "print(f\"Total de imágenes marcadas para eliminación: {len(indices_to_remove_all)}\")\n",
        "print(\"¿Deseas iniciar el procesamiento?\\n\\n\")\n",
        "time.sleep(1) # Para asegurar que se muestren los prints\n",
        "\n",
        "# Input de confirmación del usuario\n",
        "confirmacion = input(f\"⚠️ ATENCIÓN: El proceso puede demorar varios minutos. Se requiere confirmación para continuar [Y/N]: \").strip().lower()\n",
        "if confirmacion != 'y':\n",
        "    print(f\"\\n⛔️ La ejecución ha sido denegada por el usuario.\")\n",
        "else:\n",
        "\n",
        "    folder = f'{AUG_PATH}train/'\n",
        "    error_log = []\n",
        "    # --- Aplica los cambios al DataFrame principal ---\n",
        "    print(\"🔄 INICIANDO PROCESO DE ELIMINACIÓN (para interrumpir presione: Ctrl+Q)\")\n",
        "    if not testing and indices_to_remove_all:  # Solo modifica si no está en modo de prueba y hay índices\n",
        "        print(f\"Actualizando el dataframe...\")\n",
        "        time.sleep(5) # Pausa por precaución para que el usuario pueda interrumpir\n",
        "        undersample_split.loc[indices_to_remove_all, 'for_remove'] = True\n",
        "        print(\"¡Actualización del dataframe completada!\") if debugging else None\n",
        "    elif testing:\n",
        "        show_test_mode_alert(spacer1)\n",
        "    else:\n",
        "        print(\"‼️ No se encontraron filas marcadas para eliminación. [ERROR 1]\")\n",
        "\n",
        "\n",
        "    # --- Crea el DataFrame final de filas marcadas para eliminación ---\n",
        "    print(\"Preparando archivos para su eliminación...\")\n",
        "    remove_data_work = undersample_split[undersample_split['for_remove'] == True].copy()\n",
        "    deleted_count = 0\n",
        "    not_found_count = 0\n",
        "    error_count = 0\n",
        "    alert_counter = 0\n",
        "\n",
        "    # Verificar si las columnas requeridas existen antes de iterar\n",
        "    required_cols = ['filename', 'image_path']\n",
        "    if not all(col in remove_data_work.columns for col in required_cols):\n",
        "        print(f\"❌ ERROR: El DataFrame 'remove_data_work' no contiene las columnas requeridas: {required_cols}\")\n",
        "        raise ValueError\n",
        "    else:\n",
        "\n",
        "        # Iterar a través de las filas del DataFrame\n",
        "        for index, row in remove_data_work.iterrows():\n",
        "            try:\n",
        "                # Extraer el nombre del archivo y la ruta del directorio de la fila\n",
        "                # Usar str() para manejar posibles tipos no cadena de manera segura, aunque deberían ser cadenas\n",
        "                filename = str(row['filename'])\n",
        "                image_dir = str(row['image_path'])\n",
        "\n",
        "                # Verificación básica para valores vacíos/invalidos\n",
        "                if not filename or not image_dir or pd.isna(row['filename']) or pd.isna(row['image_path']):\n",
        "                    print(f\"  ❗️ Fila {index} omitida - 'filename' o 'image_path' inválido/vacío (Filename: '{filename}', Path: '{image_dir}')\")\n",
        "                    error_count += 1\n",
        "                    continue  # Saltar a la siguiente fila\n",
        "\n",
        "                # Construir la ruta completa al archivo\n",
        "                full_file_path = os.path.join(folder, image_dir, filename)\n",
        "\n",
        "                # Verificar si el archivo existe antes de intentar eliminarlo\n",
        "                if os.path.exists(full_file_path):\n",
        "                    os.remove(full_file_path) if not testing else None\n",
        "                    if debugging & (index % 1000 == 0):\n",
        "                        print(f\"{spacer2}⚠️ Muestra ejemplos de archivos eliminados (cada 1000 outputs)\") if alert_counter % 50 == 0 else None\n",
        "                        print(f\"{spacer2}🔹 ELIMINADO: {filename}\")\n",
        "                        alert_counter += 1\n",
        "                    deleted_count += 1\n",
        "                else:\n",
        "                    # La ruta del archivo derivada del DataFrame no existe\n",
        "                    print(f\"{spacer2}🔸 NO ENCONTRADO: {filename}\") if debugging else None\n",
        "                    error_log.append((folder, image_dir, filename, full_file_path))\n",
        "                    not_found_count += 1\n",
        "\n",
        "            except OSError as e:\n",
        "                # Manejar posibles errores del sistema operativo durante la eliminación (por ejemplo, permisos)\n",
        "                print(f\"{spacer2}❗️ ERROR eliminando {full_file_path}: {e}\")\n",
        "                error_count += 1\n",
        "                continue\n",
        "            except KeyError as e:\n",
        "                # Manejar el caso donde la columna 'filename' o 'image_path' no existe (debería ser detectado antes)\n",
        "                print(f\"{spacer2}❗️ ERROR: Falta la columna {e} en la fila {index}.\")\n",
        "                error_count += 1\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                # Capturar cualquier otro error inesperado para esta fila\n",
        "                print(f\"{spacer2}❗️ ERROR INESPERADO procesando la fila {index} ({full_file_path}): {e}\")\n",
        "                error_count += 1\n",
        "                continue\n",
        "\n",
        "    work_done = len(remove_data_work)\n",
        "    undersampled_split = undersample_split[undersample_split['for_remove'] == False].copy()\n",
        "    pending_for_undersample = undersample_split[undersample_split['for_remove'] == True].copy()\n",
        "    if debugging or not_found_count or error_count:\n",
        "        print(\"\\n--- Resumen de Eliminación ---\")\n",
        "        print(f\"Archivos eliminados exitosamente: {deleted_count}\")\n",
        "        print(f\"Archivos no encontrados en la ruta especificada: {not_found_count}\")\n",
        "        print(f\"Errores durante el proceso: {error_count}\")\n",
        "        print(f\"Total de filas procesadas: {work_done}\")\n",
        "        print(\"-----------------------------\")\n",
        "\n",
        "    if error_count:\n",
        "        print(f\"\\n{spacer1}‼️ ERROR: No se pudo completar la eliminación para {error_count} archivos\", \"[Activar opción 'debugging' para más detalle]\" if not debugging else None)\n",
        "    elif not_found_count:\n",
        "        print(\"\\n‼️ No se encontraron archivos indicados para eliminación. [ERROR 2]\", \"[Activar opción 'debugging' para más detalle]\" if not debugging else None)\n",
        "    else:\n",
        "        # Se crea el conjunto de datos submuestreado final:\n",
        "        if work_done != deleted_count:\n",
        "            print(\"\\n‼️ La cantidad de archivos para eliminar y efectivamente eliminados no coincide.\")\n",
        "            if debugging:\n",
        "                print(\"Debían removerse:\",len(indices_to_remove_all))\n",
        "                print(\"Se procesaron:\", deleted_count)\n",
        "                print(\"Se registraron en el dataframe:\",len(undersampled_split))\n",
        "        else:\n",
        "            print(\"\\n✅ Eliminación de archivos completada exitosamente.\")\n",
        "        print(f\"El conjunto de datos submuestreado final tiene {len(df_augmented)} imágenes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18d9dbad",
      "metadata": {
        "id": "18d9dbad"
      },
      "source": [
        "## Verificación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fe1eb17",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fe1eb17",
        "outputId": "606fdcfa-1a16-4a67-8f2a-3f8ea266b6cc"
      },
      "outputs": [],
      "source": [
        "print(f'Quedaron {len(error_log)} sin eliminar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b00bce9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "1b00bce9",
        "outputId": "53bf919f-3200-4eed-9d25-ea16c0501f17"
      },
      "outputs": [],
      "source": [
        "# Verificación de entradas duplicadas\n",
        "filename_counts = df_augmented['filename'].value_counts()\n",
        "filename_counts.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ff7c3bd",
      "metadata": {
        "id": "0ff7c3bd"
      },
      "outputs": [],
      "source": [
        "for log in error_log:\n",
        "    if filename_counts.loc[log[2]]-1:\n",
        "        pass\n",
        "    else:\n",
        "        print(False)\n",
        "        print(log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b144e987",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b144e987",
        "outputId": "68ad0b10-e5cc-49f4-f6d6-d750598352b8"
      },
      "outputs": [],
      "source": [
        "error_log[-10:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "584cb713",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "584cb713",
        "outputId": "43fc467c-d110-44f0-855b-e95485e5a7ef"
      },
      "outputs": [],
      "source": [
        "# Se  verifica la distribucion\n",
        "data = undersampled_split.copy()\n",
        "found_clusters = find_clusters(data[target])\n",
        "plot_clusters(data, target, found_clusters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a776f90a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "a776f90a",
        "outputId": "62a997eb-95a7-46a5-9aaf-266f6e4e7e9d"
      },
      "outputs": [],
      "source": [
        "undersampled_split[undersampled_split['for_remove']==True]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c33da8c",
      "metadata": {
        "id": "1c33da8c"
      },
      "outputs": [],
      "source": [
        "del undersampled_split['for_remove']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46b638dc",
      "metadata": {
        "id": "46b638dc"
      },
      "outputs": [],
      "source": [
        "# MERGE DEL DATAFRAME REDUCIDO CON EL AUMENTADO\n",
        "# 1. Obtén las filas de df_augmented donde 'split' NO sea 'train'\n",
        "df_non_train = df_augmented[df_augmented['split'] != 'train'].copy()\n",
        "\n",
        "# 2. Combina ambos dataframes\n",
        "df_undersampled = pd.concat([df_non_train, undersampled_split])\n",
        "\n",
        "# Opcional: Restablece el índice del nuevo dataframe para un índice limpio y secuencial\n",
        "#df_undersampled = df_undersampled.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44b4b72e",
      "metadata": {
        "id": "44b4b72e"
      },
      "source": [
        "## Distribución por clase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98f36875",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98f36875",
        "outputId": "b2974e8d-2c0d-430a-d5c4-fe520576e1f0"
      },
      "outputs": [],
      "source": [
        "### Resumen distribucion por grupo\n",
        "print(f\"Conteo por {target}:\")\n",
        "print(df_undersampled[target].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "554650a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "554650a6",
        "outputId": "390a31f2-0bb0-4290-968f-f35bcccfc203"
      },
      "outputs": [],
      "source": [
        "print(\"Efecto de la aumentación de datos por clase:\\n\")\n",
        "counts_entrada = df_split[target].value_counts()\n",
        "counts_salida = df_undersampled[target].value_counts()\n",
        "\n",
        "classes = counts_salida.index\n",
        "for class_ in classes:\n",
        "    prop = counts_salida[class_]/counts_entrada[class_]\n",
        "    print(f\" - {class_}: {counts_entrada[class_]} -> {counts_salida[class_]} ({'+' if prop >1 else ''}{(prop-1)*100:.0f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "494b3ef2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "494b3ef2",
        "outputId": "3b7592d9-1ab5-4cc0-f5dc-b45c2f45a8d9"
      },
      "outputs": [],
      "source": [
        "# Se  verifica la distribucion\n",
        "data = df_undersampled.copy()\n",
        "found_clusters = find_clusters(data[target])\n",
        "plot_clusters(data, target, found_clusters)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zuJLwiu3kQTy",
      "metadata": {
        "id": "zuJLwiu3kQTy"
      },
      "source": [
        "----\n",
        "# Training model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lYYhbtz_kbqm",
      "metadata": {
        "id": "lYYhbtz_kbqm"
      },
      "source": [
        "## Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R5KPXKQ9lzs5",
      "metadata": {
        "id": "R5KPXKQ9lzs5"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vaH4dNL-kRZe",
      "metadata": {
        "id": "vaH4dNL-kRZe"
      },
      "outputs": [],
      "source": [
        "# Data laoders setup\n",
        "def load_from_directory(data_folder):\n",
        "    \"\"\"\n",
        "    Carga un dataset de imágenes desde un directorio específico.\n",
        "\n",
        "    Args:\n",
        "        data_folder (str): Ruta al directorio que contiene las imágenes.\n",
        "\n",
        "    Returns:\n",
        "        tf.data.Dataset: Dataset de TensorFlow con las imágenes y etiquetas.\n",
        "    \"\"\"\n",
        "    dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        data_folder,  # Ruta al directorio de datos\n",
        "        labels=\"inferred\",  # Las etiquetas se infieren automáticamente desde los nombres de las carpetas\n",
        "        label_mode=\"categorical\",  # Las etiquetas se codifican como categorías (one-hot encoding)\n",
        "        class_names=None,  # Las clases se detectan automáticamente\n",
        "        color_mode=\"rgb\",  # Las imágenes se cargan en modo RGB\n",
        "        batch_size=128,  # Tamaño de lote para el entrenamiento\n",
        "        image_size=(256, 256),  # Redimensiona las imágenes a 128x128 píxeles\n",
        "        shuffle=True,  # Mezcla las imágenes aleatoriamente\n",
        "        seed=42,  # No se utiliza una semilla específica para la aleatorización\n",
        "        validation_split=None,  # No se realiza una división de validación aquí\n",
        "        subset=None,  # No se especifica un subconjunto (train/validation)\n",
        "        interpolation=\"bilinear\",  # Método de interpolación para redimensionar las imágenes\n",
        "        follow_links=False,  # No sigue enlaces simbólicos\n",
        "        crop_to_aspect_ratio=False  # No recorta las imágenes para ajustar la relación de aspecto\n",
        "    )\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k21oXc7-kTqf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k21oXc7-kTqf",
        "outputId": "e9adf73a-ef67-4399-99c4-dcf523a9fb37"
      },
      "outputs": [],
      "source": [
        "# Carga el dataset de imágenes desde el directorio especificado\n",
        "train_images = \"\"; test_images = \"\"; valid_images = \"\"\n",
        "\n",
        "print(\"Cargando datasets desde el directorio…\\n\")\n",
        "for split in splits:\n",
        "    data_folder = f'{SPLITTED_PATH}{split}/'\n",
        "\n",
        "    # Carga el conjunto de datos desde el directorio especificado\n",
        "    # Utiliza la función de TensorFlow para crear un dataset de imágenes\n",
        "    match split:\n",
        "        case 'train':\n",
        "            print(f\"Cargando dataset de entrenamiento desde:\\n > {data_folder}\")\n",
        "            train_images = load_from_directory(data_folder)\n",
        "        case 'test':\n",
        "            print(f\"Cargando dataset de test desde:\\n > {data_folder}\")\n",
        "            test_images = load_from_directory(data_folder)\n",
        "        case 'valid':\n",
        "            print(f\"Cargando dataset de validación desde:\\n > {data_folder}\")\n",
        "            valid_images = load_from_directory(data_folder)\n",
        "        case _: # En caso de no coincidir con ninguno de los splits\n",
        "            print(f\"⚠️ El split '{split}' no es reconocido. No se cargará ningún dataset.\")\n",
        "            continue # Salta al siguiente split\n",
        "    print(f\"✅ Dataset cargado exitosamente.\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S5wkBadHlOcN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5wkBadHlOcN",
        "outputId": "7578c5f5-1209-40aa-8ff5-8b0dbf4d6ffb"
      },
      "outputs": [],
      "source": [
        "print(\"Resumen de los datasets cargados:\")\n",
        "print(f\" - Total de imágenes en el dataset de entrenamiento: {len(train_images)}\")\n",
        "print(f\" - Total de imágenes en el dataset de validación: {len(valid_images)}\")\n",
        "print(f\" - Total de imágenes en el dataset de test: {len(test_images)}\")\n",
        "print(f\"Total de imágenes cargadas: {len(train_images) + len(test_images) + len(valid_images)}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vXPimzgmm78F",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXPimzgmm78F",
        "outputId": "41a6232b-e762-4fb6-872a-850abb4a33f4"
      },
      "outputs": [],
      "source": [
        "print(f\"Clases detectadas:\")\n",
        "[print(\" -\",clase) for clase in train_images.class_names]\n",
        "print(f\"Total de clases: {len(train_images.class_names)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UGKhzCLJm-gx",
      "metadata": {
        "id": "UGKhzCLJm-gx"
      },
      "source": [
        "----\n",
        "## Arquitectura del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xTKxLilrnADt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 974
        },
        "id": "xTKxLilrnADt",
        "outputId": "819bf763-a48a-43bf-ff1e-b47283aaf054"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras import Input\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "# Bloque 1\n",
        "model.add(Input(shape=(256, 256, 3)))\n",
        "model.add(layers.Rescaling(1./255))\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.1))\n",
        "\n",
        "# Bloque 2\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.1))\n",
        "\n",
        "# Bloque 3\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.1))\n",
        "\n",
        "# Bloque 4\n",
        "model.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.1))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "# Capa densa intermedia\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dropout(0.25))\n",
        "\n",
        "# Capa de salida con 38 neuronas y softmax para multiclase\n",
        "model.add(layers.Dense(38, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5lJh9l29nEg9",
      "metadata": {
        "id": "5lJh9l29nEg9"
      },
      "source": [
        "## Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tWE8J2-snD2L",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWE8J2-snD2L",
        "outputId": "7b10f3ed-655e-498b-aa97-d529d59ac85c"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "start_time = time.perf_counter()\n",
        "\n",
        "# Definimos el callback para guardar el mejor modelo según la métrica elegida\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath='best_model.keras',   # Se generará una carpeta con este nombre\n",
        "    monitor='val_loss',            # Métrica a monitorear ('val_accuracy' es otra opción)\n",
        "    save_best_only=True,           # Guarda solo si hay mejora\n",
        "    save_weights_only=False,       # Guarda la arquitectura + pesos\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Ajusta el modelo a tu criterio\n",
        "with tf.device('/GPU:0'):\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "history = model.fit(\n",
        "    train_images,\n",
        "    validation_data=test_images,\n",
        "    epochs=10,\n",
        "    callbacks=[checkpoint_callback]  # Incorporamos el callback\n",
        ")\n",
        "\n",
        "end_time = time.perf_counter()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Tiempo de entrenamiento: {elapsed_time:.2f} segundos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jQnG78SFnJ10",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQnG78SFnJ10",
        "outputId": "c9f478d1-c7a7-43b3-a574-3412eb1cb861"
      },
      "outputs": [],
      "source": [
        "print(f\"El entrenamiento tomó {elapsed_time:.2f} segundos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5OiC0aJnnLtu",
      "metadata": {
        "id": "5OiC0aJnnLtu"
      },
      "source": [
        "## Guardando resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iy4o3hiNnNjx",
      "metadata": {
        "id": "iy4o3hiNnNjx"
      },
      "outputs": [],
      "source": [
        "#Recording History in json & pickle\n",
        "import json\n",
        "with open('training_hist.json','w') as f:\n",
        "  json.dump(history.history,f)\n",
        "\n",
        "import pickle\n",
        "with open('training_hist.pkl', 'wb') as f:\n",
        "    pickle.dump(history.history, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HHUmDV4HnQ_Z",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHUmDV4HnQ_Z",
        "outputId": "80175c66-9ec5-49b9-e6e6-6b678467f322"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "experiment = 'experimento_n' # Completar número de experimento\n",
        "files = ['best_model.keras','training_hist.json','training_hist.pkl']\n",
        "destino=f\"/content/drive/MyDrive/CV2-PlantVillage/{experiment}/\"\n",
        "\n",
        "def check_folder(folder):\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "        print(f\"Folder '{folder}' created successfully.\")\n",
        "    else:\n",
        "        print(f\"Folder '{folder}' already exists.\")\n",
        "\n",
        "check_folder(destino)\n",
        "\n",
        "for file in files:\n",
        "    try:\n",
        "        origen=f\"/content/{file}\"\n",
        "        !cp -r \"$origen\" \"$destino\"\n",
        "    except:\n",
        "        print(f\"Error al copiar el archivo '{file}'\")\n",
        "    finally:\n",
        "        print(f\"Archivo '{file}' copiado exitosamente.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0MCZ-KXTnrn3",
      "metadata": {
        "id": "0MCZ-KXTnrn3"
      },
      "source": [
        "---\n",
        "# Gráficos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Aa1ve2usnucW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "Aa1ve2usnucW",
        "outputId": "d4c4d58b-f0dc-4e1c-8746-6550d5aadf18"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = [i for i in range(1,11)]\n",
        "plt.plot(epochs,history.history['accuracy'],color='red',label='Training Accuracy')\n",
        "plt.plot(epochs,history.history['val_accuracy'],color='blue',label='Validation Accuracy')\n",
        "plt.xlabel('No. of Epochs')\n",
        "plt.title('Visualization of Accuracy Result')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J-GidBpGnxCm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-GidBpGnxCm",
        "outputId": "93dc61b5-9c31-409b-e73b-03e3d3ffab1b"
      },
      "outputs": [],
      "source": [
        "#Validation set Accuracy\n",
        "model = tf.keras.models.load_model('best_model.keras')\n",
        "val_loss, val_acc = model.evaluate(test_images)\n",
        "print('Validation accuracy:', val_acc)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
