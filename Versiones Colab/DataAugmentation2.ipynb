{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d57e12cb",
      "metadata": {},
      "source": [
        "# Inicializaci√≥n collab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "444d527f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importaci√≥n de librer√≠as\n",
        "# Gesti√≥n de archivos y reporte\n",
        "import os\n",
        "import shutil\n",
        "import logging\n",
        "import yaml\n",
        "\n",
        "# Manipulaci√≥n y an√°lisis de datos\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Procesamiento de im√°genes\n",
        "from PIL import Image\n",
        "\n",
        "# Machine Learning\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fd59a00",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d51d331",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargamos el dataframe desde el .CSV y definimos 'id' como √≠ndice\n",
        "try:\n",
        "    df_split = pd.read_csv('/content/drive/MyDrive/CV2-PlantVillage/dataframe_splitted.csv').set_index('id')\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ö†Ô∏è Error: El archivo 'dataframe.csv' no se encontr√≥ en la ubicaci√≥n actual: {os.getcwd()}\")\n",
        "    print(\"üö® Se crear√° nuevamente al correr las celdas de 'Importaci√≥n de im√°genes' üö®.\")\n",
        "    df_split = None\n",
        "except Exception as e:\n",
        "    print(f\"Ocurri√≥ un error al leer el archivo CSV: {e}\")\n",
        "    df_split = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71e7b021",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_split.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4d72e69",
      "metadata": {},
      "source": [
        "#### Descarga de dataset de Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98c0f2a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "DATASET_PATH = kagglehub.dataset_download(\"abdallahalidev/plantvillage-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", DATASET_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30e5cd67",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ruta de acceso al dataset\n",
        "ROOT_DIR = f'{DATASET_PATH}/plantvillage dataset/color'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5b865de",
      "metadata": {},
      "source": [
        "# Dataset split"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad4e12c9",
      "metadata": {},
      "source": [
        "### Funciones necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7339f34c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_ignore_function(df, train_label, filename_col='filename'):\n",
        "    \"\"\"\n",
        "    Crea y devuelve la funci√≥n 'ignore_files' que tiene acceso al DataFrame\n",
        "    y sabe qu√© archivos mantener.\n",
        "    \"\"\"\n",
        "    # Crea un conjunto (set) con los nombres de archivo que S√ç queremos copiar (ej: split == 'train')\n",
        "    # Usa este conjunto para hacer la b√∫squeda de forma mucho m√°s r√°pida\n",
        "    files_to_keep = set(df[df['split'] == train_label][filename_col])\n",
        "    #print(f\"Archivos a mantener (split='{train_label}'): {files_to_keep}\") # Debugging\n",
        "\n",
        "    def ignore_files(current_dir, files_in_current_dir):\n",
        "        \"\"\"\n",
        "        Funci√≥n que ser√° llamada por shutil.copytree.\n",
        "        Decide qu√© archivos/directorios ignorar en el directorio actual.\n",
        "        \"\"\"\n",
        "        ignore_list = []\n",
        "        for item in files_in_current_dir:\n",
        "            # Construye la ruta completa para verificar si es archivo o directorio\n",
        "            full_path = os.path.join(current_dir, item)\n",
        "\n",
        "            # Aplicar la l√≥gica de ignorar SOLO los ARCHIVOS de la lista\n",
        "            if os.path.isfile(full_path):\n",
        "                # Si el nombre del archivo NO est√° en el conjunto de archivos a mantener,\n",
        "                # entonces lo agrega a la lista de ignorados.\n",
        "                if item not in files_to_keep:\n",
        "                    # print(f\"Ignorando archivo: {item} (en {current_dir})\") # Debugging\n",
        "                    ignore_list.append(item)\n",
        "\n",
        "        # print(f\"Directorio: {current_dir}, Ignorando: {ignore_list}\") # Debugging\n",
        "        return ignore_list\n",
        "\n",
        "    # Devuelve la funci√≥n 'ignore_files' configurada\n",
        "    return ignore_files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f21693a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, re\n",
        "\n",
        "# Busca la carpeta ra√≠z del dataset en el directorio donde fue descargado\n",
        "def find_path(folder):\n",
        "    match = re.search(fr\"^(.*?)/{folder}/\", DATASET_PATH)\n",
        "    if match:\n",
        "        prefix = match.group(1)\n",
        "        path = os.path.join(prefix, f\"{folder}/\")\n",
        "        return path\n",
        "    else:\n",
        "        print(f'No se ha podido encontrar la carpeta \"{folder}\" en {DATASET_PATH}')\n",
        "        return None\n",
        "# Carga de imagenes en memoria y visualizaci√≥n\n",
        "def load_image(data: pd.DataFrame, index: int, root: str=ROOT_DIR):\n",
        "    \"\"\"\n",
        "    Carga una imagen PIL desde una fila espec√≠fica de un DataFrame.\n",
        "\n",
        "    Args:\n",
        "        dataframe (pandas.DataFrame): El DataFrame que contiene las rutas de las im√°genes.\n",
        "        index (int): El √≠ndice de la fila en el DataFrame para cargar la imagen.\n",
        "        root_dir (str): El directorio ra√≠z donde se encuentran las im√°genes.\n",
        "\n",
        "    Returns:\n",
        "        PIL.Image.Image: La imagen cargada como un objeto PIL.Image, o None si ocurre un error.\n",
        "    \"\"\"\n",
        "    if index < 0 or index >= len(data):\n",
        "        print(\"√çndice fuera de rango.\")\n",
        "        return None\n",
        "\n",
        "    row = data.iloc[index]\n",
        "    relative_path = row['image_path']\n",
        "    filename = row['filename']\n",
        "    full_path = os.path.join(root, relative_path, filename)\n",
        "\n",
        "    try:\n",
        "        img = Image.open(full_path)\n",
        "        return img\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Archivo no encontrado: {full_path}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error al cargar la imagen: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78948f1f",
      "metadata": {},
      "source": [
        "## Split de archivos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf5382a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guarda directorio del dataset dividido\n",
        "path = find_path(\"plantvillage-dataset\")\n",
        "DATASETS_ROOT = path\n",
        "SPLITTED_PATH = f\"{path}splitted/\" if path else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f3dab83",
      "metadata": {},
      "outputs": [],
      "source": [
        "splits = df_split['split'].value_counts().index.tolist()\n",
        "splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8db6bfc7",
      "metadata": {},
      "outputs": [],
      "source": [
        "verfication = True # Ejecuta el proceso de verificaci√≥n (punto 2)\n",
        "\n",
        "print(f\"Se inicia proceso de copiado del dataset‚Ä¶\")\n",
        "total_files = len(df_split) # Total de archivos del dataset\n",
        "print(f\" - Total de archivos en el dataset: {total_files}\")\n",
        "\n",
        "# Realiza el proceso de copiado de archivos para cada split\n",
        "succeeded_process = True\n",
        "for split in splits:\n",
        "    # Crea las rutas de origen y destino\n",
        "    # (Ejemplo: 'train', 'test', 'valid')\n",
        "    print(f\"\\n\\nIniciando proceso para '{split}' split ‚Ä¶\")\n",
        "    source_folder = f'{ROOT_DIR}/'\n",
        "    destination_folder = f'{SPLITTED_PATH}{split}/'\n",
        "    total_split = len(df_split[df_split['split'] == split]) # Total de archivos del split\n",
        "    # Se omite verificaci√≥n de existencia del dataset (porque se crea siempre desde cero)\n",
        "    print(f\"üîÑ Procesando split '{split.upper()}' ({(total_split/total_files*100):.2f}):\")\n",
        "    print(f\"  - Total de archivos a copiar: {total_split}\")\n",
        "    succeeded = False\n",
        "\n",
        "    try:\n",
        "        print(f\"1. Creando estructura de subcarpetas:\")\n",
        "        # 1. Crea la funci√≥n para ignorar espec√≠fica para el split a procesar\n",
        "        ignore_function = create_ignore_function(df_split, train_label=split, filename_col='filename')\n",
        "        print(f\"    ‚úî Funci√≥n de filtro creada para el split \")\n",
        "\n",
        "        # 2. Con copytree copia todo el \"√°rbol\" de directorios (careptas y subcarpetas)\n",
        "        # Fitrando con ignore_function todos aquellos archivos que no pertenecen al split deseado\n",
        "        print(f\"    ‚àû Copiando contenido del dataset (puede demorar hasta un minuto).\")\n",
        "        shutil.copytree(source_folder, destination_folder, ignore=ignore_function)\n",
        "        print(f\"    ‚úî Proceso de copiado del split finalizado.\")\n",
        "\n",
        "        if verfication:\n",
        "            # Verifica qu√© se haya copiado adecuadamente (opcional pero √∫til)\n",
        "            print(f\"2. Se inicia proceso de verificaci√≥n‚Ä¶\")\n",
        "            copied_files = []\n",
        "            for root, dirs, files_in_dest in os.walk(destination_folder):\n",
        "                for name in files_in_dest:\n",
        "                    copied_files.append(os.path.join(os.path.relpath(root, destination_folder), name).replace('\\\\', '/')) # Normalizar path\n",
        "                    #print(f\"  - {os.path.join(root, name)}\") # Debuggin\n",
        "            print(f\"    ‚úî Se crearon un total de {len(os.listdir(destination_folder))} carpetas (para las clases).\")\n",
        "            print(f\"    ‚úî Se copiaron un total de {len(copied_files)} archivos ({len(copied_files)/total_split*100:.2f}%)\")\n",
        "            # Agregar confirmaci√≥n de igualdad cantidad split == copiados\n",
        "            if len(copied_files) == total_split:\n",
        "                print(f\"‚úÖ Se complet√≥ satisfactoriamente el subproceso de copiado para el split.\\n\")\n",
        "                succeeded = True\n",
        "            else:\n",
        "                print(f\"‚ùå Error: No se pudo copiar correctamente el split '{split.upper()}'\\n\")\n",
        "                succeeded = False\n",
        "        else:\n",
        "            succeeded = True # Si la verificaci√≥n est√° desactivada, se asume que el proceso fue exitoso\n",
        "\n",
        "    except FileExistsError:\n",
        "        print(f\"Error: La carpeta de destino '{destination_folder}' ya existe.\\n\")\n",
        "        pass\n",
        "    except Exception as e:\n",
        "        print(f\"Ocurri√≥ un error inesperado: {e}\\n\")\n",
        "\n",
        "    succeeded_process *= succeeded # Actualiza el estado del proceso\n",
        "    # (S√≥lo es 'True' si todos los splits se copian correctamente)\n",
        "\n",
        "if succeeded_process:\n",
        "    print(\"\\n\\nüåü El proceso de copiado del dataset ha finalizado con √©xito.\\n\")\n",
        "else:\n",
        "    print(\"\\n\\nüö´ No se pudo completar satisfactoriamente el proceso de copiado del dataset.\\nVerificar que se haya completado la eliminaci√≥n de las carpetas.\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26fbf36a",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b6b725e",
      "metadata": {
        "id": "2b6b725e"
      },
      "source": [
        "# Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a60ec1ca",
      "metadata": {
        "id": "a60ec1ca"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import albumentations as A\n",
        "import random\n",
        "import yaml\n",
        "import shutil\n",
        "from PIL import Image\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fff79c0",
      "metadata": {
        "id": "1fff79c0"
      },
      "source": [
        "### Funciones necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5e91e15",
      "metadata": {
        "id": "f5e91e15"
      },
      "outputs": [],
      "source": [
        "def find_folder(path):\n",
        "    return os.path.basename(os.path.dirname(path))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a8fedd1",
      "metadata": {
        "id": "9a8fedd1"
      },
      "source": [
        "### Carga de datos almacenados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "464be198",
      "metadata": {
        "id": "464be198"
      },
      "outputs": [],
      "source": [
        "def dataset_already_exists(path_to_check: str) -> bool | None:\n",
        "    \"\"\"\n",
        "    Verifica si el directorio especificado existe y est√° vac√≠o.\n",
        "\n",
        "    Args:\n",
        "        path_to_check (str): Ruta del directorio a verificar.\n",
        "\n",
        "    Returns:\n",
        "        bool: True si el directorio existe y est√° vac√≠o, False en caso contrario.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path_to_check):\n",
        "        # El directorio no existe -> Crea el directorio\n",
        "        #print(f\"‚òëÔ∏è El directorio no existe, a√∫n no ha sido creado:\\n > {path_to_check}\") # Debugging\n",
        "        return False # No realiza ninguna acci√≥n\n",
        "    else:\n",
        "        # Verificar si el directorio est√° vac√≠o\n",
        "        try:\n",
        "            # Explora el contenido del directorio\n",
        "            content = os.listdir(path_to_check)\n",
        "            #print(content) # Debugging\n",
        "\n",
        "            # Si el directorio est√° vac√≠o, se puede eliminar directamente\n",
        "            #       -> Elimina sin confirmaci√≥n\n",
        "            if not content:\n",
        "                os.rmdir(path_to_check) # Elimina el directorio vac√≠o\n",
        "                print(f\"‚òëÔ∏è El directorio estaba vac√≠o y se ha eliminado de forma autom√°tica:\\n > {path_to_check}\\n\")\n",
        "                return False\n",
        "\n",
        "            # Si el directorio contiene s√≥lo archivos ocultos (de sistema)\n",
        "            #       -> Elimina sin confirmaci√≥n\n",
        "            elif all([file.startswith('.') for file in content]):\n",
        "                shutil.rmtree(path_to_check) # Elimina el directorio y su contenido\n",
        "                print(f\"‚òëÔ∏è El directorio s√≥lo conten√≠a archivos ocutlos, por lo que se ha eliminado de forma autom√°tica:\\n > {path_to_check}\\n\")\n",
        "                return False\n",
        "\n",
        "            # Si hay archivos visibles en el directorio (dataset ya existe)\n",
        "            #       -> Solicita permiso para eliminarlos\n",
        "            else:\n",
        "                # Input de confirmaci√≥n del usuario\n",
        "                confirmacion = input(f\"‚ö†Ô∏è El directorio especificado ya existe y contiene archivos. ¬øDeseas eliminar todo su contenido y el directorio en s√≠? [Y/N]: '{path_to_check}'\").strip().lower()\n",
        "                # Verifica la respuesta del usuario\n",
        "                if confirmacion == 'y':\n",
        "                    shutil.rmtree(path_to_check) # Elimina el directorio y su contenido\n",
        "                    print(f\"‚úÖ El directorio y su contenido han sido eliminados exitosamente:\\n > {path_to_check}\\n\")\n",
        "                    return False\n",
        "                else:\n",
        "                    print(f\"‚õîÔ∏è La eliminaci√≥n del directorio ha sido denegada por el usuario:\\n  > {path_to_check}\")\n",
        "                    return True\n",
        "\n",
        "        except OSError as e:\n",
        "            print(f\"‚ùå Error al eliminar el directorio vac√≠o en {path_to_check}: {e}\\n\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"‚ÄºÔ∏è Ocurri√≥ un error inesperado al intentar eliminar el directorio vac√≠o en {path_to_check}: {e}\\n\")\n",
        "            return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf311112",
      "metadata": {
        "id": "bf311112"
      },
      "outputs": [],
      "source": [
        "# Carga de imagenes en memoria y visualizaci√≥n\n",
        "def load_image(data: pd.DataFrame, index: int, root: str=ROOT_DIR):\n",
        "    \"\"\"\n",
        "    Carga una imagen PIL desde una fila espec√≠fica de un DataFrame.\n",
        "\n",
        "    Args:\n",
        "        dataframe (pandas.DataFrame): El DataFrame que contiene las rutas de las im√°genes.\n",
        "        index (int): El √≠ndice de la fila en el DataFrame para cargar la imagen.\n",
        "        root_dir (str): El directorio ra√≠z donde se encuentran las im√°genes.\n",
        "\n",
        "    Returns:\n",
        "        PIL.Image.Image: La imagen cargada como un objeto PIL.Image, o None si ocurre un error.\n",
        "    \"\"\"\n",
        "    # if index < 0 or index >= len(data):\n",
        "    #     print(\"√çndice fuera de rango.\")\n",
        "    #     return None\n",
        "\n",
        "    row = data.iloc[index]\n",
        "    relative_path = row['image_path']\n",
        "    filename = row['filename']\n",
        "    full_path = os.path.join(root, relative_path, filename)\n",
        "\n",
        "    try:\n",
        "        img = Image.open(full_path)\n",
        "        return img\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Archivo no encontrado: {full_path}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error al cargar la imagen: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fc754e7",
      "metadata": {
        "id": "0fc754e7"
      },
      "outputs": [],
      "source": [
        "# Carga de imagenes en memoria y visualizaci√≥n\n",
        "def load_image_idx(data, root: str=ROOT_DIR):\n",
        "    \"\"\"\n",
        "    Carga una imagen PIL desde una fila espec√≠fica de un DataFrame.\n",
        "\n",
        "    Args:\n",
        "        dataframe (pandas.DataFrame): El DataFrame que contiene las rutas de las im√°genes.\n",
        "        index (int): El √≠ndice de la fila en el DataFrame para cargar la imagen.\n",
        "        root_dir (str): El directorio ra√≠z donde se encuentran las im√°genes.\n",
        "\n",
        "    Returns:\n",
        "        PIL.Image.Image: La imagen cargada como un objeto PIL.Image, o None si ocurre un error.\n",
        "    \"\"\"\n",
        "    # if index < 0 or index >= len(data):\n",
        "    #     print(\"√çndice fuera de rango.\")\n",
        "    #     return None\n",
        "\n",
        "    row = data\n",
        "    relative_path = row['image_path']\n",
        "    filename = row['filename']\n",
        "    full_path = os.path.join(root, relative_path, filename)\n",
        "\n",
        "    try:\n",
        "        img = Image.open(full_path)\n",
        "        return img\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Archivo no encontrado: {full_path}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error al cargar la imagen: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ztZbyfR7eoBo",
      "metadata": {
        "id": "ztZbyfR7eoBo"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e623eeec",
      "metadata": {
        "id": "e623eeec"
      },
      "source": [
        "## Transformaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0234e2b",
      "metadata": {
        "id": "e0234e2b"
      },
      "outputs": [],
      "source": [
        "# 1. Define las transformaciones de Albumentations\n",
        "transform = A.Compose([\n",
        "    # Espaciales\n",
        "    A.Rotate(limit=180, p=0.5),          # Rotaci√≥n aleatoria hasta 180 grados con probabilidad 0.5\n",
        "    A.HorizontalFlip(p=0.5),            # Volteo horizontal con probabilidad 0.5\n",
        "    A.VerticalFlip(p=0.5),              # Volteo vertical con probabilidad 0.5\n",
        "    A.RandomScale(scale_limit=0.1, p=0.3),      # Escalado aleatorio con un l√≠mite de 30%\n",
        "    A.RandomCrop(width=200, height=200, p=0.3), # Recorte aleatorio a 200x200 (original 256px)\n",
        "    # Visuales\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.3), # Ajuste de brillo y contraste aleatorio\n",
        "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.3), # Ajuste de tono, saturaci√≥n y valor\n",
        "    A.GaussianBlur(blur_limit=(3, 5), p=0.2),   # Desenfoque Gaussiano\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c3f87a1",
      "metadata": {
        "id": "5c3f87a1"
      },
      "outputs": [],
      "source": [
        "# 2. Transformaciones alternativas (m√°s agresivas) para clases minoritarias\n",
        "proba_mult = 2\n",
        "scaler = 1.5\n",
        "transform2 = A.Compose([\n",
        "    A.Rotate(limit=180, p=0.5*proba_mult),          # Rotaci√≥n aleatoria hasta 180 grados con probabilidad 0.5\n",
        "    A.HorizontalFlip(p=0.5*proba_mult),            # Volteo horizontal con probabilidad 0.5\n",
        "    A.VerticalFlip(p=0.5*proba_mult),              # Volteo vertical con probabilidad 0.5\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.2*scaler, contrast_limit=0.2*scaler, p=0.3*proba_mult), # Ajuste de brillo y contraste aleatorio\n",
        "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30*scaler, val_shift_limit=20*scaler, p=0.3*proba_mult), # Ajuste de tono, saturaci√≥n y valor\n",
        "    A.RandomCrop(width=200, height=200, p=0.3*proba_mult), # Recorte aleatorio a 200x200\n",
        "    A.GaussianBlur(blur_limit=(3, 5), p=0.2),   # Desenfoque Gaussiano\n",
        "    A.RandomScale(scale_limit=0.1, p=0.3*proba_mult),      # Escalado aleatorio con un l√≠mite de 30%\n",
        "    A.CLAHE(clip_limit=2.0, p=0.8),             # Ecualizaci√≥n CLAHE\n",
        "    # Se agrega CLAHE para compensar algunos valores extremos (brillo, contraste) fruto del 'scaler' y el aumento de la probabilidad\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "956e89eb",
      "metadata": {
        "id": "956e89eb"
      },
      "outputs": [],
      "source": [
        "def apply_transformations(original_image, algo: int = 1) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Applies transformations to an input image using Albumentations.\n",
        "\n",
        "    Args:\n",
        "        original_image (PIL.Image.Image): The original image to transform.\n",
        "        algo (int, optional): The transformation strategy to use. Defaults to 1.\n",
        "            - 1: Applies the first set of transformations (`transform`).\n",
        "            - 2: Applies the second set of transformations (`transform2`).\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: The transformed image as a NumPy array.\n",
        "    \"\"\"\n",
        "    # Convertir la imagen original a un array NumPy para la transformaci√≥n\n",
        "    original_image_array = np.array(original_image)\n",
        "\n",
        "    algo_opt = [1, 2]\n",
        "    # Aplicar las transformaciones\n",
        "    if algo == algo_opt[0]:\n",
        "        transformed = transform(image=original_image_array)\n",
        "    elif algo == algo_opt[1]:\n",
        "        transformed = transform2(image=original_image_array)\n",
        "    else:\n",
        "        print(\"Debe seleccionar entre las siguientes opciones:\")\n",
        "        for opt in algo_opt:\n",
        "            print(f'Opci√≥n: `algo={opt}`')\n",
        "            if opt == algo_opt[0]:\n",
        "                print(transform)\n",
        "            elif opt == algo_opt[1]:\n",
        "                print(transform2)\n",
        "            print()\n",
        "    return transformed['image']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a83e98ce",
      "metadata": {
        "id": "a83e98ce"
      },
      "source": [
        "----\n",
        "## Procesamiento del dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afa6d546",
      "metadata": {
        "id": "afa6d546"
      },
      "source": [
        "### Funciones necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fca5f8af",
      "metadata": {
        "id": "fca5f8af"
      },
      "outputs": [],
      "source": [
        "import humanize\n",
        "\n",
        "# Traducir humanize al espa√±ol\n",
        "try:\n",
        "    humanize.i18n.activate(\"es\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Paquete de idioma espa√±ol para humanize no encontrado, usando ingl√©s.\")\n",
        "    humanize.i18n.activate(\"en_US\") # Fallback a ingl√©s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18ad052a",
      "metadata": {
        "id": "18ad052a"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "#from collections import defaultdict\n",
        "\n",
        "def cluster_by_count_distribution(data_string, std_dev_multiplier=1.0, verbose = True):\n",
        "    \"\"\"\n",
        "    Agrupa categor√≠as bas√°ndose en la distribuci√≥n de sus conteos.\n",
        "\n",
        "    Agrupa elementos con conteos similares utilizando diferencias logar√≠tmicas.\n",
        "    Identifica valores at√≠picos con conteos significativamente diferentes como cl√∫steres separados.\n",
        "\n",
        "    Args:\n",
        "        data_string (str): Una cadena multil√≠nea donde cada l√≠nea contiene\n",
        "                           un nombre de categor√≠a y su conteo, separados por espacios.\n",
        "        std_dev_multiplier (float): Multiplicador para la desviaci√≥n est√°ndar utilizado\n",
        "                                    en el c√°lculo del umbral para identificar\n",
        "                                    rupturas de cl√∫ster. Valores m√°s altos generan menos\n",
        "                                    cl√∫steres m√°s grandes. Por defecto es 1.0.\n",
        "\n",
        "    Returns:\n",
        "        list: Una lista de cl√∫steres, donde cada cl√∫ster es una lista de\n",
        "              tuplas (nombre_categoria, conteo).\n",
        "              Devuelve una lista vac√≠a si falla el an√°lisis o no se encuentra ning√∫n dato.\n",
        "    \"\"\"\n",
        "    # --- 1. Analizar los datos de entrada ---\n",
        "    data = {}\n",
        "    # Expresi√≥n regular para capturar el nombre de la categor√≠a (que puede contener espacios/caracteres especiales)\n",
        "    # y el conteo al final\n",
        "    # Maneja posibles espacios adicionales.\n",
        "    pattern = re.compile(r'^(.*?)\\s+(\\d+)$')\n",
        "    lines = data_string.strip().split('\\n')\n",
        "\n",
        "    if not lines or (len(lines) == 1 and not lines[0].strip()):\n",
        "        print(\"Error: La cadena de datos de entrada est√° vac√≠a o no es v√°lida.\")\n",
        "        return []\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue  # Saltar l√≠neas vac√≠as\n",
        "        match = pattern.match(line)\n",
        "        if match:\n",
        "            # El grupo 1 captura todo antes del √∫ltimo bloque de d√≠gitos\n",
        "            name = match.group(1).strip()\n",
        "            # El grupo 2 captura el √∫ltimo bloque de d√≠gitos\n",
        "            count = int(match.group(2))\n",
        "            data[name] = count\n",
        "        else:\n",
        "            print(f\"Advertencia: No se pudo analizar la l√≠nea: '{line}'\")\n",
        "\n",
        "    if not data:\n",
        "        print(\"Error: No se analizaron datos v√°lidos de la cadena de entrada.\")\n",
        "        return []\n",
        "\n",
        "    # --- 2. Ordenar los datos ---\n",
        "    # Convertir a una lista de tuplas y ordenar por conteo en orden descendente\n",
        "    # Esto facilita la comparaci√≥n de elementos consecutivos.\n",
        "    sorted_items = sorted(data.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "    # Manejar el caso con solo un elemento\n",
        "    if len(sorted_items) <= 1:\n",
        "        print(\"Solo se encontr√≥ un punto de datos, devolvi√©ndolo como un √∫nico cl√∫ster.\")\n",
        "        return [sorted_items]\n",
        "\n",
        "    # --- 3. Calcular diferencias logar√≠tmicas ---\n",
        "    # Extraer los conteos en un array de numpy\n",
        "    counts = np.array([item[1] for item in sorted_items])\n",
        "\n",
        "    # Calcular el logaritmo de los conteos. La transformaci√≥n logar√≠tmica ayuda a normalizar\n",
        "    # grandes diferencias y centrarse en cambios relativos.\n",
        "    # Se agrega un peque√±o epsilon para evitar log(0) si los conteos pueden ser cero.\n",
        "    epsilon = 1e-9\n",
        "    log_counts = np.log(counts + epsilon)\n",
        "\n",
        "    # Calcular las diferencias entre los logaritmos consecutivos de los conteos.\n",
        "    # Dado que los datos est√°n ordenados de forma descendente, una gran diferencia positiva aqu√≠\n",
        "    # indica una gran ca√≠da en el valor del conteo.\n",
        "    # diff[i] = log_counts[i] - log_counts[i+1]\n",
        "    log_diffs = -np.diff(log_counts)\n",
        "\n",
        "    # --- 4. Determinar rupturas de cl√∫ster ---\n",
        "    # Identificar saltos significativos en los conteos logar√≠tmicos utilizando un umbral basado en\n",
        "    # la media y la desviaci√≥n est√°ndar de las diferencias logar√≠tmicas.\n",
        "    # Las diferencias mayores que el umbral sugieren un punto de ruptura para un nuevo cl√∫ster.\n",
        "    if len(log_diffs) > 0:\n",
        "        mean_log_diff = np.mean(log_diffs)\n",
        "        std_log_diff = np.std(log_diffs)\n",
        "        # Establecer el umbral: media + multiplicador * desviaci√≥n est√°ndar\n",
        "        # Ajustar el multiplicador para cambiar la sensibilidad (mayor = menos sensible)\n",
        "        threshold = mean_log_diff + std_dev_multiplier * std_log_diff\n",
        "        if verbose:\n",
        "            print(f\"- Diferencias logar√≠tmicas: {np.round(log_diffs, 2)}\")\n",
        "            print(f\"- Diferencia logar√≠tmica media: {mean_log_diff:.2f}\")\n",
        "            print(f\"- Desviaci√≥n est√°ndar de las diferencias logar√≠tmicas: {std_log_diff:.2f}\")\n",
        "            print(f\"- Umbral (Media + {std_dev_multiplier:.1f}*SD): {threshold:.2f}\")\n",
        "\n",
        "        # Encontrar √≠ndices *donde* la diferencia excede el umbral.\n",
        "        # Esto indica una ruptura *despu√©s* de este √≠ndice en la lista ordenada.\n",
        "        # np.where devuelve una tupla de arrays, necesitamos el primer array.\n",
        "        break_after_indices = np.where(log_diffs >= threshold)[0]\n",
        "        # El nuevo cl√∫ster comienza en el √≠ndice *siguiente*.\n",
        "        break_start_indices = break_after_indices + 1\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"- Puntos de quiembre detectados despu√©s de los elementos en los √≠ndices: {break_after_indices}\")\n",
        "            print(f\"- Los nuevos cl√∫steres comienzan en los √≠ndices: {break_start_indices}\")\n",
        "    else:\n",
        "        # No hay diferencias para calcular si hay <= 1 elemento\n",
        "        break_start_indices = []\n",
        "        print(\"No hay suficientes puntos de datos para calcular diferencias.\")\n",
        "\n",
        "    # --- 5. Agrupar en cl√∫steres ---\n",
        "    clusters = []\n",
        "    start_index = 0\n",
        "    # Iterar a trav√©s de los puntos de ruptura identificados (donde comienzan nuevos cl√∫steres)\n",
        "    for break_idx in break_start_indices:\n",
        "        # Agregar el segmento antes de la ruptura como un cl√∫ster\n",
        "        clusters.append(sorted_items[start_index:break_idx])\n",
        "        # Actualizar el √≠ndice de inicio para el pr√≥ximo cl√∫ster\n",
        "        start_index = break_idx\n",
        "    # Agregar los elementos restantes (desde el √∫ltimo punto de ruptura hasta el final) como el cl√∫ster final\n",
        "    clusters.append(sorted_items[start_index:])\n",
        "\n",
        "    return clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f5657f0",
      "metadata": {
        "id": "1f5657f0"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def plot_clusters(df, target, found_clusters):\n",
        "    df_graph = df.copy()\n",
        "\n",
        "    # 1. Crear un mapeo del nombre de la categor√≠a al ID del cl√∫ster\n",
        "    category_to_cluster_id = {}\n",
        "    for cluster_id, cluster_list in enumerate(found_clusters):\n",
        "        for category_name, _ in cluster_list:  # Solo necesitamos el nombre aqu√≠\n",
        "            category_to_cluster_id[category_name] = cluster_id  # Asignar 0, 1, 2...\n",
        "\n",
        "    # 2. Usar este mapeo para crear la columna 'cluster' en df_graph\n",
        "    df_graph['cluster'] = df_graph[target].map(category_to_cluster_id)\n",
        "\n",
        "\n",
        "    # Opcional: Convertir los IDs de cl√∫ster a tipo string para etiquetas de leyenda m√°s claras\n",
        "    n_clusters = len(df_graph['cluster'].unique())\n",
        "    # Map numerical cluster IDs to textual labels (e.g., \"Cluster 1\", \"Cluster 2\", ...)\n",
        "    cluster_id_to_text = {cluster_id: f\"Cluster {cluster_id + 1}\" for cluster_id in range(n_clusters)}\n",
        "    df_graph['cluster'] = df_graph['cluster'].map(cluster_id_to_text)\n",
        "\n",
        "    # Sort the clusters in descending order based on their IDs\n",
        "    df_graph['cluster'] = pd.Categorical(df_graph['cluster'],\n",
        "                                        categories=[f\"Cluster {i + 1}\" for i in range(n_clusters)],\n",
        "                                        ordered=True)\n",
        "\n",
        "    # Obtener el conteo de cada clase\n",
        "    class_counts = df_graph[target].value_counts().sort_values(ascending=False)\n",
        "\n",
        "    # Obtener la lista ordenada de clases basada en el conteo\n",
        "    ordered_classes_by_count = class_counts.index.tolist()\n",
        "\n",
        "    # Configurar el tama√±o de la figura\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Crear un gr√°fico de barras (usando el argumento data= para mayor claridad)\n",
        "    sns.countplot(y=target, order=ordered_classes_by_count, hue='cluster',\n",
        "                data=df_graph, palette=\"Spectral\", dodge=False)\n",
        "\n",
        "    plt.title(\"Gr√°fico de barras de la distribuci√≥n de clases objetivo coloreado por cl√∫steres (basado en el conteo)\")\n",
        "    plt.ylabel(\"Clases objetivo\")\n",
        "    plt.xlabel(\"Conteo\")\n",
        "    plt.legend(title='ID del cl√∫ster')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ba74686",
      "metadata": {
        "id": "4ba74686"
      },
      "outputs": [],
      "source": [
        "def filter_classes(df, target, range: tuple = (0,None)):\n",
        "    lim_min, lim_max = range\n",
        "    counts = df[target].value_counts()\n",
        "    if lim_min == None:\n",
        "        lim_min  = 0\n",
        "    if lim_max == None:\n",
        "        lim_max  = counts.max()\n",
        "    #print(\"Limites:\", lim_min, lim_max)\n",
        "\n",
        "    selection = []\n",
        "    for idx, count in enumerate(counts):\n",
        "        if lim_min < count <= lim_max:\n",
        "            clase = counts.index[idx]\n",
        "            selection.append(clase)\n",
        "    return selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6927b8c",
      "metadata": {
        "id": "b6927b8c"
      },
      "outputs": [],
      "source": [
        "def gen_rnd_id():\n",
        "    \"\"\"\n",
        "        Genera un n√∫mero aleatorio de 6 c√≠fras que se usar√° para diferenciar imagenes procesados de las originales\n",
        "    \"\"\"\n",
        "    rnd_seed = round(time.time() * 1e10) # Use clock as seed generator\n",
        "    random.seed(rnd_seed)  # Set this value as a new seed (assure better randomization)\n",
        "    sampled_numbers = random.sample(range(int(1e6)), 1)  # Generates a random number\n",
        "    return str(sampled_numbers[0]).zfill(6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b206ffb",
      "metadata": {
        "id": "8b206ffb"
      },
      "outputs": [],
      "source": [
        "def numoji(numero):\n",
        "  \"\"\"\n",
        "  Convierte un n√∫mero entero del 1 al 10 a su emoji correspondiente.\n",
        "\n",
        "  Args:\n",
        "    numero: Un entero entre 1 y 10.\n",
        "\n",
        "  Returns:\n",
        "    Un string con el emoji correspondiente al n√∫mero, o \"0Ô∏è‚É£\" si el n√∫mero\n",
        "    est√° fuera del rango.\n",
        "  \"\"\"\n",
        "  if 1 <= numero <= 10:\n",
        "    emoji_map = {\n",
        "        1: \"1Ô∏è‚É£\",\n",
        "        2: \"2Ô∏è‚É£\",\n",
        "        3: \"3Ô∏è‚É£\",\n",
        "        4: \"4Ô∏è‚É£\",\n",
        "        5: \"5Ô∏è‚É£\",\n",
        "        6: \"6Ô∏è‚É£\",\n",
        "        7: \"7Ô∏è‚É£\",\n",
        "        8: \"8Ô∏è‚É£\",\n",
        "        9: \"9Ô∏è‚É£\",\n",
        "        10: \"üîü\"\n",
        "    }\n",
        "    return emoji_map[numero]\n",
        "  else:\n",
        "    return \"*Ô∏è‚É£\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93d6e93b",
      "metadata": {
        "id": "93d6e93b"
      },
      "outputs": [],
      "source": [
        "def show_test_mode_alert(spacer2):\n",
        "    print()\n",
        "    print(spacer2,(\"‚ö†Ô∏è\"*31))\n",
        "    print(spacer2,\"‚ö†Ô∏è\",(\" \"*46),\"‚ö†Ô∏è\")\n",
        "    print(f\"{spacer2} ‚ö†Ô∏è      üö®     MODO TESTING ACTIVADO      üö®       ‚ö†Ô∏è\")\n",
        "    print(spacer2,\"‚ö†Ô∏è\",(\" \"*46),\"‚ö†Ô∏è\")\n",
        "    print(f\"{spacer2} ‚ö†Ô∏è (+) Se omite la apertura de im√°genes           ‚ö†Ô∏è\")\n",
        "    print(f\"{spacer2} ‚ö†Ô∏è (+) Se omite las creaci√≥n de transformaciones  ‚ö†Ô∏è\")\n",
        "    print(spacer2,\"‚ö†Ô∏è\",(\" \"*46),\"‚ö†Ô∏è\")\n",
        "    print(f\"{spacer2} ‚ö†Ô∏è    ESTE PROCESO NO GENERAR√Å ARCHIVOS REALES    ‚ö†Ô∏è\")\n",
        "    print(spacer2,\"‚ö†Ô∏è\",(\" \"*46),\"‚ö†Ô∏è\")\n",
        "    print(spacer2,(\"‚ö†Ô∏è\"*31))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b3ee624",
      "metadata": {
        "id": "5b3ee624"
      },
      "source": [
        "## Creaci√≥n de directorios y estructuras de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5985a44",
      "metadata": {
        "id": "e5985a44"
      },
      "outputs": [],
      "source": [
        "# Se ha definido una nueva constante con la ubicaci√≥n del dataset aumentado (ser√° almacenadad en YAML)\n",
        "AUG_PATH = SPLITTED_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9895aa32",
      "metadata": {
        "id": "9895aa32"
      },
      "source": [
        "Luego, se aplican las transformaciones para aumentaci√≥n de datos a la copia del dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fe57e92",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "id": "2fe57e92",
        "outputId": "4f9c3c96-b5bf-485b-fba1-1aa4a098e1a7"
      },
      "outputs": [],
      "source": [
        "# Agrega columnas para indicar procesamiento\n",
        "## original -> imagen sin transofrmaciones\n",
        "## augmented -> existen aumentaciones derivadas\n",
        "df_split['augmented'] = False\n",
        "df_split['is_original'] = True\n",
        "\n",
        "# Crea un nuevo dataframe para gesitonar el procesamiento\n",
        "process_split = df_split[df_split['split']=='train'].copy()\n",
        "process_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e71f579d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 976
        },
        "id": "e71f579d",
        "outputId": "de1b5240-7899-4769-ac4a-5ccabab02fcb"
      },
      "outputs": [],
      "source": [
        "# Crea una columna 'ref' que apunta al id de la imagen original\n",
        "process_split['ref'] = process_split.index\n",
        "process_split"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5ab86f5",
      "metadata": {
        "id": "e5ab86f5"
      },
      "source": [
        "## Estrategia de aumentaci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b04ebf55",
      "metadata": {
        "id": "b04ebf55"
      },
      "outputs": [],
      "source": [
        "# Se define el feature a partir del cual se aplicar√°n las aumentaciones\n",
        "target = 'class' # class / group\n",
        "#data = df.copy()\n",
        "data = process_split.copy()\n",
        "#data = df_augmented.copy()\n",
        "#data = processed_split.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54b1a017",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54b1a017",
        "outputId": "d39fd59c-2a39-497c-b63f-5c4381641858"
      },
      "outputs": [],
      "source": [
        "# Resumen distribucion por grupo\n",
        "print(f\"Conteo por '{target}':\")\n",
        "counts = data[target].value_counts()\n",
        "for class_name, count in counts.items():\n",
        "    print(f\" - {class_name}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64604549",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64604549",
        "outputId": "da1fd330-f646-435c-a31d-3ab29768dd01"
      },
      "outputs": [],
      "source": [
        "def find_clusters(data):\n",
        "    # Convierte el diccionario a una cadena multil√≠nea\n",
        "    data_string = dict(data.value_counts())\n",
        "    data_string = \"\\n\".join(f\"{key} {value}\" for key, value in data_string.items())\n",
        "\n",
        "    # --- Ejecuta la funci√≥n de agrupamiento ---\n",
        "    # Puedes ajustar el std_dev_multiplier (por ejemplo, 0.5, 1.0, 1.5)\n",
        "    # para hacer que el agrupamiento sea m√°s o menos sensible a las brechas de conteo.\n",
        "    # Un valor m√°s bajo podr√≠a crear m√°s cl√∫steres.\n",
        "    multiplier = 0.15 # 0.5 / 0.25 / 0.1 proba distintos valores\n",
        "    found_clusters = cluster_by_count_distribution(data_string, std_dev_multiplier=multiplier,verbose=False)\n",
        "\n",
        "    # --- Muestra los resultados ---\n",
        "    print(f\"Cl√∫steres identificados:\")\n",
        "    print(f\" ¬∑ Multiplicador Desviaci√≥n Est√°ndar {multiplier}\\n\")\n",
        "    if found_clusters:\n",
        "        for i, cluster in enumerate(found_clusters):\n",
        "            # Filtrar cl√∫steres potencialmente vac√≠os si el corte result√≥ en uno\n",
        "            if cluster:\n",
        "                print(f\"Cl√∫ster {i + 1}:\")\n",
        "                # Obtener conteos para calcular el rango (opcional para mostrar)\n",
        "                cluster_counts = [count for _, count in cluster]\n",
        "                print(f\"  (Rango de conteo: {min(cluster_counts)} - {max(cluster_counts)})\")\n",
        "                # Imprimir elementos en el cl√∫ster\n",
        "                for name, count in cluster:\n",
        "                    print(f\"  - {name}: {count}\")\n",
        "                print()\n",
        "        return found_clusters\n",
        "    else:\n",
        "        print(\"No se identificaron cl√∫steres.\")\n",
        "        return None\n",
        "\n",
        "found_clusters = find_clusters(data[target])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac9a905b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        },
        "id": "ac9a905b",
        "outputId": "46d2c10d-2529-452c-cbf3-cdabe2f25c4c"
      },
      "outputs": [],
      "source": [
        "plot_clusters(data, target, found_clusters)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e93d82e3",
      "metadata": {
        "id": "e93d82e3"
      },
      "source": [
        "Se identifican las siguientes distribuciones:\n",
        "- **Para target `class`:**\n",
        "    1. **Clase mayoritaria** 'Soybean___healthy' con ~4.5k\n",
        "    1. Grupo 1 con rango de > 4k *(3 clases)*\n",
        "    1. Grupo 2 con rango de ~1k - 2k ***ascendente***\n",
        "    1. Grupo 3 con rango de ~500 - 1k\n",
        "        1. Mitad sueprior ***ascendente*** con rango de >800k\n",
        "        1. Mitad inferior ***constante*** ~800 *(8 clases)*\n",
        "    1. Grupo 4 con rango de ~400 - 500 *(2 clases)*\n",
        "    1. Grupo 5 con rango de ~400 *(3 clases)* ***ascendente***\n",
        "    1. Grupo 6 con rango de ~300 *(3 clases)* ***constante***\n",
        "    1. Clase aislada de ~200\n",
        "    1. **Clase minoritaria** 'Potato___healthy' con ~100\n",
        "\n",
        "- **Para target `group`:**\n",
        "    1. **Clase mayoritaria** 'Tomato' con ~14.5k\n",
        "    1. Grupo 1 con rango de ~4k *(2 clases)*\n",
        "    1. Grupo 2 con rango de ~3k *(2 clases)*\n",
        "    1. Grupo 3 con rango de ~2k *(4 clases)*\n",
        "    1. Grupo 4 con rango de ~1.5k *(2 clases)*\n",
        "    1. Grupo 5 con rango de ~1.2k *(2 clases)*\n",
        "    1. **Clase minoritaria** 'Raspberry' con ~300"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "195ed49b",
      "metadata": {
        "id": "195ed49b"
      },
      "source": [
        "**Estrategia 1:**\n",
        "- Se dejan sin procesar los primeros 3 grupos *(>4k)* **‚á¢ =**\n",
        "- Para los grupos 1 y 2 *(2k‚Äì4k)* se aplica **‚á¢ x2**\n",
        "- Para los grupos 3 a 6 *(500‚Äì2k)* se aplica **‚á¢ x3**\n",
        "- Para la clase minotirtaria *(<500)* se aplica **‚á¢ x6**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d5c242a",
      "metadata": {
        "id": "2d5c242a"
      },
      "outputs": [],
      "source": [
        "# ESTRATEGIA PARA AUMENTACI√ìN POR 'GRUPO' (estrategia 1)\n",
        "# Clases a las que se le aplicar√° aumentaci√≥n de datos\n",
        "# Entre 4000 y 2000\n",
        "group1 = {\n",
        "    'classes': filter_classes(df_split,target,(2000,4000)),\n",
        "    'increase': 2 # 2x data augmentation (original + transf)\n",
        "}\n",
        "# Menores a 2000\n",
        "group2 = {\n",
        "    'classes': filter_classes(df_split,target,(500,2000)),\n",
        "    'increase': 3 # 3x data augmentation (original +  2 transf)\n",
        "}\n",
        "\n",
        "# Clase m√≠nima (Raspberry)\n",
        "group3 = {\n",
        "    'classes': filter_classes(df_split,target,(0,500)),\n",
        "    'increase': 6 # 6x data augmentation (original +  5 transf)\n",
        "}\n",
        "\n",
        "estrategy1 = [group1, group2, group3]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88457edd",
      "metadata": {
        "id": "88457edd"
      },
      "source": [
        "**Estrategia 2:**\n",
        "\n",
        "A diferencia de la estrategia anterior, adem√°s de la aumentanci√≥n, se aplicar√° la t√©cnica de  submuestreo (*undersampling*) para las clases mayoritarias.\n",
        "\n",
        "Se intentar√° balancear todas las clases entorno a 2k samples.\n",
        "\n",
        "- Submuestreo del grupo 1 *(>4k)* **‚á¢ √∑2**\n",
        "- Para el grupo 2 *(1k-2k)* se aumentar√° **‚á¢ x2** y luego undersampling **‚á¢ =2k**\n",
        "- Para el grupos 3 *(500‚Äì1k)* se aplica **‚á¢ x2**\n",
        "- Para el grupos 4 *(400-500)* se aplica **‚á¢ x4**\n",
        "- Para el grupos 5 *(~400)* se aplica **‚á¢ x5**\n",
        "- Para el grupos 6 *(~300)* se aplica **‚á¢ x6**\n",
        "- Para las clases minoritarias:\n",
        "    - Cluster 7 *(~200)* se aplica **‚á¢ x10** *(aumentando variabilidad)*\n",
        "    - Cluster 8 *(~100)* se aplica **‚á¢ x20** *(aumentando variabilidad)*\n",
        "\n",
        "Para las clases minoritarias se modificar√° la funci√≥n de transformaci√≥n incrementando la probabilidad para aumentar la variabilidad en la generaci√≥n y evitar as√≠ el overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de5cf8c4",
      "metadata": {
        "id": "de5cf8c4"
      },
      "outputs": [],
      "source": [
        "# ESTRATEGIA PARA AUMENTACI√ìN POR 'CLASE' (estrategia 2)\n",
        "# Clases a las que se le aplicar√° aumentaci√≥n de datos\n",
        "# Entre 4000 y 2000\n",
        "\n",
        "# Grupo 1 (>4k): Requiere submuestreo (√∑2), no aumentaci√≥n. Se maneja por separado.\n",
        "# Rango original: 1000 a 2000 muestras\n",
        "group1 = {\n",
        "    'classes': filter_classes(process_split, target, (3000, 10000)), # Clases por encima de 3000\n",
        "    'increase': 0,\n",
        "    'limit': 2000 # ~ undersampling √∑2\n",
        "}\n",
        "\n",
        "# Grupo 2: Aumentaci√≥n x2 (target original * 2)\n",
        "# Rango original: 1000 a 2000 muestras\n",
        "group2 = {\n",
        "    'classes': filter_classes(process_split, target, (1000, 3000)), # Clases entre 1000 y 3000 (exclusive el 2000)\n",
        "    'increase': 2, # 2x data augmentation (original + 1 transf).\n",
        "    'limit': 2000 # Post-tope a 2k\n",
        "}\n",
        "\n",
        "# Grupo 3: Aumentaci√≥n x2\n",
        "# Rango original: 500 a 1000 muestras\n",
        "group3 = {\n",
        "    'classes': filter_classes(process_split, target, (550, 1000)), # Clases entre 500 y 1000 (exclusive el 1000)\n",
        "    'increase': 2 # 2x data augmentation (original + 1 transf)\n",
        "}\n",
        "\n",
        "# Grupo 4: Aumentaci√≥n x4\n",
        "# Rango original: 400 a 500 muestras\n",
        "group4 = {\n",
        "    'classes': filter_classes(process_split, target, (450, 550)), # Clases entre 400 y 500 (exclusive el 500)\n",
        "    'increase': 4 # 4x data augmentation (original + 3 transf)\n",
        "}\n",
        "\n",
        "# Grupo 5: Aumentaci√≥n x5\n",
        "# Rango original: ~400 muestras (interpretado como 300 a 400)\n",
        "group5 = {\n",
        "    'classes': filter_classes(process_split, target, (300, 450)), # Clases entre 300 y 400 (exclusive el 400)\n",
        "    'increase': 5 # 5x data augmentation (original + 4 transf)\n",
        "}\n",
        "\n",
        "# Grupo 6: Aumentaci√≥n x6\n",
        "# Rango original: ~300 muestras (interpretado como 200 a 300)\n",
        "group6 = {\n",
        "    'classes': filter_classes(process_split, target, (250, 300)), # Clases entre 200 y 300 (exclusive el 300)\n",
        "    'increase': 6 # 6x data augmentation (original + 5 transf)\n",
        "}\n",
        "\n",
        "# Grupo 7 (Minoritaria 1): Aumentaci√≥n x10\n",
        "# Rango original: ~200 muestras (interpretado como 100 a 200)\n",
        "group7 = {\n",
        "    'classes': filter_classes(process_split, target, (150, 250)), # Clases entre 100 y 200 (exclusive el 200)\n",
        "    'increase': 10, # 10x data augmentation (original + 9 transf)\n",
        "    'algo': 2   # Algoritmo alternativo buscando variabilidad\n",
        "}\n",
        "\n",
        "# Grupo 8 (Minoritaria 2): Aumentaci√≥n x20\n",
        "# Rango original: ~100 muestras (interpretado como 0 a 100)\n",
        "group8 = {\n",
        "    'classes': filter_classes(process_split, target, (0, 150)), # Clases entre 0 y 100 (exclusive el 100)\n",
        "    'increase': 20, # 20x data augmentation (original + 19 transf)\n",
        "    'algo': 2   # Algoritmo alternativo buscando variabilidad\n",
        "}\n",
        "\n",
        "estrategy2 = [group1, group2, group3, group4, group5, group6, group7, group8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de94c83a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de94c83a",
        "outputId": "f8b3b5c8-b235-474e-eecc-f9c37cbc8e38"
      },
      "outputs": [],
      "source": [
        "print(\"Efecto de la aumentaci√≥n de datos para 'train' split\\n\")\n",
        "counts = process_split[target].value_counts()\n",
        "\n",
        "groups = estrategy2 # Estrategia 2\n",
        "for group in groups:\n",
        "    mult = group['increase']\n",
        "    print(f\"Estrategia de aumentaci√≥n {mult}x:\")\n",
        "    for clase in group['classes']:\n",
        "        print(f\" - {clase}: {counts[clase]} -> {counts[clase]*mult}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd858a04",
      "metadata": {
        "id": "bd858a04"
      },
      "source": [
        "## Procesamiento de aumentaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94cf6517",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94cf6517",
        "outputId": "e71cfa2d-eab5-4e72-c485-5df8b341558b"
      },
      "outputs": [],
      "source": [
        "# PROCESAMIENTO DE AUMENTACIONES CON FILTRADO POR CLASE\n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "# SETUP INCIAL:\n",
        "verbose = True # Incluye detalle del proceso y estimaci√≥n de tiempo\n",
        "testing = False # No modifica archivos\n",
        "debugging = False # Incluye detalle de archivos y mensajes de error\n",
        "estrategy = estrategy2 # Estrategia a aplicar: estrategy1 | estrategy2\n",
        "processes = estrategy.copy() # Para evitar modificaci√≥n cruzada\n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "for_processing_classes = [cls for group in groups for cls in group['classes']]\n",
        "for_processing_data = process_split[process_split[target].isin(for_processing_classes)]\n",
        "n_transformations = 0\n",
        "without_transfromations = False\n",
        "for process in processes:\n",
        "    if process['increase'] !=0:\n",
        "        classes, increase = process['classes'], process['increase']\n",
        "        images_in_classes = for_processing_data[for_processing_data[target].isin(classes)]\n",
        "        n_transformations += len(images_in_classes) * (increase - 1)\n",
        "    else:\n",
        "        print(f\"NO AUGMENTATIONS NEEDED FOR:\\n{process}\\n\") if debugging else None\n",
        "        processes.remove(process)\n",
        "\n",
        "print(f\"Iniciando proceso de aumentaci√≥n de datos‚Ä¶\")\n",
        "folder = f'{AUG_PATH}train/'\n",
        "print(f\"Aumentando im√°genes para:\\n >>\", folder)\n",
        "total_files = len(for_processing_data) # Total de archivos del dataset\n",
        "print(f\" - Total de archivos en el dataset 'train': {len(process_split)}\")\n",
        "print(f\" - Total de archivos a procesar: {total_files} ({(total_files/len(process_split)*100):.0f}% del dataset)\")\n",
        "print(f\" - Transformaciones totales: {n_transformations} im√°genes ser√°n creadas\\n\")\n",
        "\n",
        "if n_transformations > 0:\n",
        "    print(f\"Se procesar√°n un total de {len(for_processing_classes)} clases de {len(process_split[target].value_counts())}:\")\n",
        "    for label in for_processing_classes:\n",
        "        print(\" - \",label)\n",
        "    print()\n",
        "    for i, process in enumerate(processes):\n",
        "        print(f\"¬∑ Estrategia {i+1}: {process['increase']}x augmentation para {process['classes']}\")\n",
        "    print(\"\\n¬øDeseas iniciar el procesamiento?\")\n",
        "    time.sleep(1) # Para asegurar que se muestren los prints\n",
        "\n",
        "    # Input de confirmaci√≥n del usuario\n",
        "    confirmacion = input(f\"‚ö†Ô∏è ATENCI√ìN: El proceso puede demorar varios minutos. Se requiere confirmaci√≥n para continuar [Y/N]: \").strip().lower()\n",
        "    if confirmacion != 'y':\n",
        "        print(f\"\\n‚õîÔ∏è La ejecuci√≥n ha sido denegada por el usuario.\")\n",
        "    else:\n",
        "        start_time = time.perf_counter() # Medici√≥n del tiempo\n",
        "        partial_time = start_time #?\n",
        "\n",
        "        # PROCESAMIENTO DE AUMENTACIONES\n",
        "        # Leyenda:\n",
        "        #  count -> archivos originales procesados (total)\n",
        "        #  work -> transformaciones por clase (parcial)\n",
        "        #  task -> n√∫mero de transformaciones por archivo\n",
        "\n",
        "        # Duplica dataframe para almacenar el proceso\n",
        "        processed_split = process_split.copy()\n",
        "\n",
        "        # Realiza el proceso de copiado de archivos para cada grupo\n",
        "        succeeded_process = True\n",
        "        count = 0\n",
        "        total_work_done = 0\n",
        "        avg_time = 0\n",
        "        partial_time = 0\n",
        "        previous_count = 0\n",
        "        spacer1 = '   '\n",
        "        spacer2 = '      '\n",
        "\n",
        "        for i, process in enumerate(processes):\n",
        "            mult = process['increase']\n",
        "            n_augmentations = mult - 1 # se resta 1 porque la imagen original ya existe\n",
        "            classes = process['classes']\n",
        "            print(f\"\\n\\n{numoji(i+1)} Iniciando subproceso de aumentaci√≥n:\")\n",
        "            print(f\" - Estrategia de aumentaci√≥n: {mult}x\")\n",
        "            if process.get('algo'):\n",
        "                algo = process['algo']\n",
        "                print(f\" - Aplicando el algoritmo {algo}\")\n",
        "            else:\n",
        "                algo = 1\n",
        "            print(f\" - Clases a procesar: {len(classes)} grupos\\n   {classes}\")\n",
        "\n",
        "            for clase in classes:\n",
        "                # Filtrado de clases\n",
        "                subprocess_split = process_split[process_split[target]==clase]\n",
        "                total_work = len(subprocess_split) * n_augmentations # Conteo de archivos para la clase\n",
        "                print(f\"\\n{spacer1}üîÑ Procesando clase '{clase}' ({(total_work/total_files*100):.0f}% del total):\")\n",
        "                print(f\"{spacer1}  - Transformaciones a generar: {total_work}\") if verbose else None\n",
        "                work = 0\n",
        "\n",
        "                for index, image_row in subprocess_split.iterrows():\n",
        "                    show_test_mode_alert(spacer2) if work==0 and testing else None\n",
        "                    # 1. Apertura del archivo de imagen original\n",
        "                    original_image = load_image_idx(data=image_row,root=folder) if not testing else None\n",
        "                    #original_image.show() # Debugging\n",
        "\n",
        "                    # Genera m√∫ltiples transformaciones para cada imagen\n",
        "                    try:\n",
        "                        task = n_augmentations\n",
        "                        for n in range(n_augmentations):\n",
        "\n",
        "                            # 2. Procesamiento de la transformacion\n",
        "                            transformed_image = apply_transformations(original_image,algo=algo) if not testing else None\n",
        "                            try:\n",
        "                                # Convierte el array NumPy a un objeto Image de PIL\n",
        "                                if not testing:\n",
        "                                    if transformed_image.ndim == 2:  # Escala de grises\n",
        "                                        processed_image = Image.fromarray(transformed_image.astype(np.uint8), 'L')\n",
        "                                    elif transformed_image.ndim == 3:  # RGB\n",
        "                                        if transformed_image.shape[2] == 3:\n",
        "                                            processed_image = Image.fromarray(transformed_image.astype(np.uint8), 'RGB')\n",
        "                                        else:\n",
        "                                            raise ValueError(f\"{spacer2}‚ùóÔ∏è El array NumPy debe tener 2 (escala de grises) o 3 (RGB) canales.\")\n",
        "                                    else:\n",
        "                                        raise ValueError(f\"{spacer2}‚ùóÔ∏è El array NumPy debe tener 2 o 3 dimensiones.\")\n",
        "                                    #processed_image.show() # Debugging\n",
        "\n",
        "                                # 3. Almacena el archivo\n",
        "                                # Genera el nombre del nuevo archivo\n",
        "                                # se incluye un int random al final (para permitir m√∫ltiples aumentaciones)\n",
        "                                rnd_num = gen_rnd_id() # string de 6 cifras\n",
        "                                name = list(os.path.splitext(image_row.filename))\n",
        "                                name.insert(1,f'-{rnd_num}')\n",
        "                                filename = \"\".join(name)\n",
        "                                # Guarda la transformaci√≥n como JPG\n",
        "                                processed_image.save(f'{folder}{image_row.image_path}{filename}', \"JPEG\") if not testing else None\n",
        "                                print(f\"{spacer2}üîπ {filename}\") if debugging else None\n",
        "\n",
        "                                # 4. Registra la nueva imagen en el DataFrame\n",
        "                                new_row = image_row.copy() # Copia las etiquetas\n",
        "                                # Actualiza los valores correspondientes\n",
        "                                new_row['filename'] = filename\n",
        "                                new_row['is_original'] = False\n",
        "                                new_row['augmented'] = True\n",
        "                                # Agrega la nueva fila al DataFrame processed_split\n",
        "                                processed_split = pd.concat([processed_split, pd.DataFrame([new_row])], ignore_index=True)\n",
        "\n",
        "                                task -= 1 # cuenta una transformaci√≥n finalizada\n",
        "                                work += 1 # cuenta una aumentaci√≥n realizada (por clase)\n",
        "                                if work % 100 == 0:\n",
        "                                    print(f'{spacer2}üî® ¬ª Progreso subproceso: {work/total_work*100:.2f}%') if verbose else None\n",
        "\n",
        "                            except FileExistsError:\n",
        "                                print(f\"{spacer2}‚ùå Error: El archivo de destino '{destination_folder}' ya existe.\\n\") if debugging else None\n",
        "                                continue\n",
        "                            except Exception as e:\n",
        "                                print(f\"{spacer2}‚ùå Error al guardar la imagen como JPG: {e}\") if debugging else None\n",
        "                                continue\n",
        "                    finally:\n",
        "                        # 5. Actualiza el registro de la imagen original\n",
        "                        processed_split.loc[index, 'augmented'] = True\n",
        "                        #print(processed_split.iloc[index])\n",
        "\n",
        "                        count += 1 # cuenta un archivo finalizado\n",
        "                        if task == 0:\n",
        "                            pass\n",
        "                        else:\n",
        "                            print(f\"{spacer2}‚ÄºÔ∏è ALERTA: No se ha completado la tarea para {image_row.filename}\", \"[Activar opci√≥n 'debugging' para m√°s detalle]\" if not debugging else None)\n",
        "\n",
        "                print(f'{spacer2}üî® ¬ª Progreso subproceso: 100%') if verbose else None\n",
        "                total_work_done += work # registro del trabajo realizado\n",
        "\n",
        "                # L√ìGICA TEMPORAL\n",
        "                if verbose:\n",
        "                    # Medici√≥n de tiempo pasado y estimaci√≥n del faltante\n",
        "                    previous_time = partial_time\n",
        "                    partial_time = time.perf_counter() # Medici√≥n del tiempo\n",
        "                    divisor = count - previous_count\n",
        "                    elapsed_time = (partial_time - start_time)\n",
        "                    task_time = elapsed_time/divisor if divisor > 0 else 0\n",
        "                    avg_time = elapsed_time/total_work_done if total_work_done > 0 else 0 # Tiempo medio para estimaci√≥n\n",
        "\n",
        "                    print(f'{spacer2}‚è±Ô∏è ¬∑ Tiempo de subproceso {humanize.naturaldelta(task_time)}')\n",
        "                    if previous_count and verbose:\n",
        "                        if task_time > avg_time*1.1:\n",
        "                            print(f\"{spacer2}üëéüèª La tarea ha demorado m√°s de lo esperado‚Ä¶\")\n",
        "                        elif task_time < avg_time*0.75:\n",
        "                            print(f\"{spacer2}üëçüèª ¬°Tarea completada en tiempo record!\")\n",
        "\n",
        "                if work == total_work:\n",
        "                    print(f\"{spacer1}‚úÖ Subproceso de transformaciones completado.\")\n",
        "                    succeeded = True\n",
        "                else:\n",
        "                    print(f\"{spacer1}‚ÄºÔ∏è ALERTA: No se pudo completar correctamente el subproceso de transformaciones.\", \"[Activar opci√≥n 'debugging' para m√°s detalle]\" if not debugging else None)\n",
        "                    succeeded = False\n",
        "                if verbose:\n",
        "                    #print(f'\\n{spacer1}üïó Ejecuci√≥n total {elapsed_time/60:.1f} min')\n",
        "                    print(f'\\n{spacer1}üïó Ejecuci√≥n total {humanize.naturaldelta(elapsed_time)}')\n",
        "                    #print(f'{spacer1}üîÆ Estimaci√≥n de tiempo restante {avg_time * (n_transformations - total_work_done) / 60:.2f} min')\n",
        "                    print(f'{spacer1}üîÆ Estimaci√≥n de tiempo restante {humanize.naturaldelta(avg_time * (n_transformations - total_work_done))}')\n",
        "                    print(f'{spacer1}‚ùáÔ∏è ¬ª Progreso general: {count/total_files*100:.3f}%') if (count/total_files) < 0.99 else None\n",
        "                previous_count = count\n",
        "\n",
        "                succeeded_process *= succeeded # Actualiza el estado del proceso\n",
        "                # (S√≥lo es 'True' si todos los subprocesos se completan correctamente)\n",
        "\n",
        "        print(f'{spacer1}‚ùáÔ∏è ¬ª Progreso general: 100%') if not testing else None\n",
        "        end_time = time.perf_counter() # Medici√≥n del tiempo\n",
        "        total_time = end_time - start_time\n",
        "        if succeeded_process:\n",
        "            print(\"\\nüåü El proceso de aumentaci√≥n del dataset ha finalizado con √©xito.\")\n",
        "        else:\n",
        "            print(\"\\nüö´ No se pudo completar satisfactoriamente el proceso de aumentaci√≥n del dataset.\")\n",
        "        if verbose:\n",
        "            print(f'{spacer1}üïó Ejecuci√≥n completada en {humanize.naturaldelta(total_time)}')\n",
        "            print(f'{spacer1}üî¢ ¬∑ Se procesaron {total_work_done} transformaciones para un total de {total_files} im√°genes.')\n",
        "            print(f'{spacer1}‚è±Ô∏è ¬∑ Tiempo medio por transofrmaci√≥n {(total_time/total_work_done):.3f} sec')\n",
        "\n",
        "else:\n",
        "    print(\"üåü La estrategia actual no requiere realizar ninguna transformaci√≥n.\")\n",
        "    without_transfromations = True\n",
        "    processed_split = process_split.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66596957",
      "metadata": {
        "id": "66596957"
      },
      "source": [
        "# Verificaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc73c695",
      "metadata": {
        "id": "cc73c695"
      },
      "source": [
        "### Archivos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "462cf624",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "462cf624",
        "outputId": "d99ab73b-a4e2-4aa1-fbe4-09d8e3894f02"
      },
      "outputs": [],
      "source": [
        "resut_aug_dataframe = len(processed_split) - len(process_split)\n",
        "print(f\"Luego del proceso de aumentaci√≥n de datos se genraron {resut_aug_dataframe} archivos nuevos.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4aee39a7",
      "metadata": {
        "id": "4aee39a7"
      },
      "outputs": [],
      "source": [
        "def contar_archivos(ruta):\n",
        "    total = 0\n",
        "    with os.scandir(ruta) as it:\n",
        "        for entry in it:\n",
        "            if entry.is_file():\n",
        "                total += 1\n",
        "            elif entry.is_dir():\n",
        "                total += contar_archivos(entry.path)\n",
        "    return total"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb7a4f37",
      "metadata": {
        "id": "cb7a4f37"
      },
      "source": [
        "### Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37817aab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37817aab",
        "outputId": "2049de46-7a21-4319-8f9c-32fb8c840bad"
      },
      "outputs": [],
      "source": [
        "processed_split.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab5e118d",
      "metadata": {
        "id": "ab5e118d"
      },
      "outputs": [],
      "source": [
        "if not without_transfromations:\n",
        "    processed_split[(processed_split['is_original'] == False)].sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd3c1e23",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "cd3c1e23",
        "outputId": "516cec36-9ebe-4e54-9e98-07ae75f2e6db"
      },
      "outputs": [],
      "source": [
        "# Verificamos el √∫ltimo archivo procesado para asegurar que el dataframe fue editado correctamente\n",
        "processed_split[(processed_split['group'] == 'Raspberry') & (processed_split['ref'] == 27181)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6b76619",
      "metadata": {
        "id": "b6b76619"
      },
      "source": [
        "## Distribuci√≥n por clase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb42147b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb42147b",
        "outputId": "4e50742e-c93e-4ea9-9211-4da7a9dc1d8a"
      },
      "outputs": [],
      "source": [
        "### Resumen distribucion por grupo\n",
        "print(f\"Conteo por {target}:\")\n",
        "print(processed_split[target].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f94ff79",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f94ff79",
        "outputId": "07c10678-5516-44cf-b46d-c334be271895"
      },
      "outputs": [],
      "source": [
        "print(\"Efecto de la aumentaci√≥n de datos por clase:\\n\")\n",
        "counts_entrada = df_split[target].value_counts()\n",
        "counts_salida = processed_split[target].value_counts()\n",
        "\n",
        "classes = counts_salida.index\n",
        "for class_ in classes:\n",
        "    print(f\" - {class_}: {counts_entrada[class_]} -> {counts_salida[class_]} (+{(counts_salida[class_]/counts_entrada[class_]-1)*100:.0f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d61944fa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d61944fa",
        "outputId": "93b31895-fe01-4f9f-af35-7a8599627c66"
      },
      "outputs": [],
      "source": [
        "# Se define el feature a partir del cual se aplicar√°n las aumentaciones\n",
        "#target = 'class' # class / group\n",
        "data = processed_split.copy()\n",
        "found_clusters = find_clusters(data[target])\n",
        "plot_clusters(data, target, found_clusters)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd368525",
      "metadata": {
        "id": "dd368525"
      },
      "source": [
        "## Merge dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c206ea89",
      "metadata": {
        "id": "c206ea89"
      },
      "outputs": [],
      "source": [
        "# MERGE DEL DATAFRAME AUMENTADO CON EL ORIGINAL\n",
        "# 1. Se pararan los datos de archivos procesados\n",
        "for_update = processed_split[processed_split['is_original']==True]\n",
        "new_rows = processed_split[processed_split['is_original']==False]\n",
        "\n",
        "# 2. Se actualiza el Dataframe original con los nuevos valores\n",
        "df_split.update(for_update)\n",
        "\n",
        "# 3. Se agregan las aumentaciones con el dataframe original\n",
        "df_augmented = pd.concat([df_split, new_rows], axis=0)\n",
        "df_augmented.update(for_update)\n",
        "\n",
        "# 4. Ajuste de formato\n",
        "df_augmented['ref'] = df_augmented['ref'].astype('Int64')\n",
        "\n",
        "# 5. Eliminaci√≥n de posibles entrads duplicadas\n",
        "df_augmented.drop_duplicates(subset='filename', keep='first', inplace=True)\n",
        "\n",
        "# 6. (Opcional) Reordenar por √≠ndice si quer√©s mantener orden\n",
        "# df_merged = df_merged.sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d2f9f33",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "7d2f9f33",
        "outputId": "36710701-3499-451e-a43f-107eef31b77d"
      },
      "outputs": [],
      "source": [
        "df_augmented.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1aac14e3",
      "metadata": {
        "id": "1aac14e3"
      },
      "source": [
        "# Undersampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8078910a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8078910a",
        "outputId": "a69feed5-0fe2-47e3-9671-f06f66cb6b07"
      },
      "outputs": [],
      "source": [
        "counts = df_augmented[target].value_counts()\n",
        "undersample_works = []\n",
        "undersample_count = 0\n",
        "undersample_count_group = 0\n",
        "\n",
        "for i, process in enumerate(estrategy):\n",
        "    if process.get('limit'):\n",
        "        print(f\"{numoji(i+1)} Se aplicar√° submuestreo hasta {process['limit']} im√°genes para las siguientes clases:\")\n",
        "        for clase in process['classes']:\n",
        "            print(f\" - {clase}: {counts[clase]} -> {process['limit']}\")\n",
        "            if counts[clase] > process['limit']:\n",
        "                undersample_count_group += counts[clase] - process['limit']\n",
        "            else:\n",
        "                print(f\"    ‚ÄºÔ∏è La clase '{clase}' posee {counts[clase]} im√°genes, por lo que no es necesario aplicar submuestreo.\")\n",
        "        if undersample_count_group > 0:\n",
        "            print(\"Reducci√≥n para el grupo: \",undersample_count_group)\n",
        "            undersample_count += undersample_count_group\n",
        "            undersample_works.append(process)\n",
        "        else:\n",
        "            print(\"    üö® CUIDADO: Aunque est√° configurado un proceso de undersampling, la cantidad de muestras para estas clases es menor al l√≠mite establecido.\")\n",
        "        print()\n",
        "if undersample_count:\n",
        "    print(f\"‚úÖ Luego del proceso de undersampling se eliminar√°n un total de {undersample_count} im√°genes.\")\n",
        "else:\n",
        "    print(f\"üåü No se ha configurado ning√∫n proceso de undersampling para la estrategia seleccionada.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d0518a5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d0518a5",
        "outputId": "f9518634-1518-485c-8186-b36136dc2a34"
      },
      "outputs": [],
      "source": [
        "# PROCESAMIENTO DE UNDERSAMPLING CON FILTRADO POR CLASE\n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "# SETUP INCIAL:\n",
        "#verbose = True # Incluye detalle del proceso y estimaci√≥n de tiempo\n",
        "testing = False # No modifica archivos\n",
        "debugging = False # Incluye detalle de archivos y mensajes de error\n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "undersample_split = df_augmented[df_augmented['split']=='train']\n",
        "undersample_split['for_remove'] = False\n",
        "spacer1 = '    '\n",
        "spacer2 = '      '\n",
        "remove_data_work = pd.DataFrame(columns=undersample_split.columns)\n",
        "indices_to_remove_all = []\n",
        "\n",
        "for i, work in enumerate(undersample_works):\n",
        "    print(f'{numoji(i+1)} Iniciando trabajo de submuestreo para {len(work)} clases:')\n",
        "    limit = work['limit']\n",
        "    print(f' ¬∑ L√≠mite establecido para undersampling: {limit}\\n')\n",
        "\n",
        "    for clase in work['classes']:\n",
        "        indices_to_remove_class = []\n",
        "        class_indices = undersample_split[undersample_split['class'] == clase].index\n",
        "\n",
        "        removing_message = lambda remove_data: f\"{spacer1}> Ser√°n eliminadas: {len(remove_data)} im√°genes\"\n",
        "        data = undersample_split.loc[class_indices]\n",
        "        data_orig = data[data['is_original'] == True]\n",
        "        data_aug = data[data['is_original'] == False]\n",
        "        print(f'  - {clase}: contiene {len(data)} im√°genes ({len(data_orig)}+{len(data_aug)})')\n",
        "\n",
        "        if len(data_orig) > limit:\n",
        "            print(f\"{spacer1}Data original supera el l√≠mite\") if debugging else None\n",
        "            print(removing_message(data_orig.iloc[limit:]))\n",
        "            remove_orig_indices = data_orig.index[limit:].tolist()\n",
        "            indices_to_remove_class.extend(remove_orig_indices)\n",
        "            if len(data_aug) != 0:\n",
        "                # Verifica que no se hayan generado aumentaciones inncesarias\n",
        "                # En caso afirmativo, las elimina tambi√©n\n",
        "                remove_aug_indices = data_aug.index.tolist()\n",
        "                indices_to_remove_class.extend(remove_aug_indices)\n",
        "        elif len(data_aug) > limit:\n",
        "            print(f\"{spacer1}Data aumentada supera el l√≠mite\") if debugging else None\n",
        "            print(removing_message(data_aug.iloc[limit:]))\n",
        "            remove_orig_indices = data_aug.index[limit:].tolist()\n",
        "            indices_to_remove_class.extend(remove_orig_indices)\n",
        "        elif len(data_orig)+len(data_aug) > limit:\n",
        "            print(f\"{spacer1}Al unir ambos conjuntos se supera el l√≠mite\") if debugging else None\n",
        "            keep_aug_count = limit - len(data_orig)\n",
        "            print(f\"{spacer1}Se deben incluir\", keep_aug_count) if debugging else None\n",
        "            print(removing_message(data_aug.iloc[keep_aug_count:]))\n",
        "            remove_aug_indices = data_aug.index[keep_aug_count:].tolist()\n",
        "            indices_to_remove_class.extend(remove_aug_indices)\n",
        "        else:\n",
        "            print(f\"No es necesario aplicar undersampling. La cantidad de im√°genes {len(data)} no supera el l√≠mite establecido.\")\n",
        "\n",
        "        if indices_to_remove_class:\n",
        "             print(f\"{spacer1}Total marcado para remover en clase '{clase}': {len(indices_to_remove_class)}\") if debugging else None\n",
        "             indices_to_remove_all.extend(indices_to_remove_class)\n",
        "    print()\n",
        "\n",
        "#====================================================================================================\n",
        "# --- Post-procesamiento despu√©s de iterar a trav√©s de todos los trabajos ---\n",
        "# Elimina duplicados entre diferentes trabajos/clases\n",
        "indices_to_remove_all = list(set(indices_to_remove_all))\n",
        "print(f\"Total de im√°genes marcadas para eliminaci√≥n: {len(indices_to_remove_all)}\")\n",
        "print(\"¬øDeseas iniciar el procesamiento?\\n\\n\")\n",
        "time.sleep(1) # Para asegurar que se muestren los prints\n",
        "\n",
        "# Input de confirmaci√≥n del usuario\n",
        "confirmacion = input(f\"‚ö†Ô∏è ATENCI√ìN: El proceso puede demorar varios minutos. Se requiere confirmaci√≥n para continuar [Y/N]: \").strip().lower()\n",
        "if confirmacion != 'y':\n",
        "    print(f\"\\n‚õîÔ∏è La ejecuci√≥n ha sido denegada por el usuario.\")\n",
        "else:\n",
        "\n",
        "    folder = f'{AUG_PATH}train/'\n",
        "    error_log = []\n",
        "    # --- Aplica los cambios al DataFrame principal ---\n",
        "    print(\"üîÑ INICIANDO PROCESO DE ELIMINACI√ìN (para interrumpir presione: Ctrl+Q)\")\n",
        "    if not testing and indices_to_remove_all:  # Solo modifica si no est√° en modo de prueba y hay √≠ndices\n",
        "        print(f\"Actualizando el dataframe...\")\n",
        "        time.sleep(5) # Pausa por precauci√≥n para que el usuario pueda interrumpir\n",
        "        undersample_split.loc[indices_to_remove_all, 'for_remove'] = True\n",
        "        print(\"¬°Actualizaci√≥n del dataframe completada!\") if debugging else None\n",
        "    elif testing:\n",
        "        show_test_mode_alert(spacer1)\n",
        "    else:\n",
        "        print(\"‚ÄºÔ∏è No se encontraron filas marcadas para eliminaci√≥n. [ERROR 1]\")\n",
        "\n",
        "\n",
        "    # --- Crea el DataFrame final de filas marcadas para eliminaci√≥n ---\n",
        "    print(\"Preparando archivos para su eliminaci√≥n...\")\n",
        "    remove_data_work = undersample_split[undersample_split['for_remove'] == True].copy()\n",
        "    deleted_count = 0\n",
        "    not_found_count = 0\n",
        "    error_count = 0\n",
        "    alert_counter = 0\n",
        "\n",
        "    # Verificar si las columnas requeridas existen antes de iterar\n",
        "    required_cols = ['filename', 'image_path']\n",
        "    if not all(col in remove_data_work.columns for col in required_cols):\n",
        "        print(f\"‚ùå ERROR: El DataFrame 'remove_data_work' no contiene las columnas requeridas: {required_cols}\")\n",
        "        raise ValueError\n",
        "    else:\n",
        "\n",
        "        # Iterar a trav√©s de las filas del DataFrame\n",
        "        for index, row in remove_data_work.iterrows():\n",
        "            try:\n",
        "                # Extraer el nombre del archivo y la ruta del directorio de la fila\n",
        "                # Usar str() para manejar posibles tipos no cadena de manera segura, aunque deber√≠an ser cadenas\n",
        "                filename = str(row['filename'])\n",
        "                image_dir = str(row['image_path'])\n",
        "\n",
        "                # Verificaci√≥n b√°sica para valores vac√≠os/invalidos\n",
        "                if not filename or not image_dir or pd.isna(row['filename']) or pd.isna(row['image_path']):\n",
        "                    print(f\"  ‚ùóÔ∏è Fila {index} omitida - 'filename' o 'image_path' inv√°lido/vac√≠o (Filename: '{filename}', Path: '{image_dir}')\")\n",
        "                    error_count += 1\n",
        "                    continue  # Saltar a la siguiente fila\n",
        "\n",
        "                # Construir la ruta completa al archivo\n",
        "                full_file_path = os.path.join(folder, image_dir, filename)\n",
        "\n",
        "                # Verificar si el archivo existe antes de intentar eliminarlo\n",
        "                if os.path.exists(full_file_path):\n",
        "                    os.remove(full_file_path) if not testing else None\n",
        "                    if debugging & (index % 1000 == 0):\n",
        "                        print(f\"{spacer2}‚ö†Ô∏è Muestra ejemplos de archivos eliminados (cada 1000 outputs)\") if alert_counter % 50 == 0 else None\n",
        "                        print(f\"{spacer2}üîπ ELIMINADO: {filename}\")\n",
        "                        alert_counter += 1\n",
        "                    deleted_count += 1\n",
        "                else:\n",
        "                    # La ruta del archivo derivada del DataFrame no existe\n",
        "                    print(f\"{spacer2}üî∏ NO ENCONTRADO: {filename}\") if debugging else None\n",
        "                    error_log.append((folder, image_dir, filename, full_file_path))\n",
        "                    not_found_count += 1\n",
        "\n",
        "            except OSError as e:\n",
        "                # Manejar posibles errores del sistema operativo durante la eliminaci√≥n (por ejemplo, permisos)\n",
        "                print(f\"{spacer2}‚ùóÔ∏è ERROR eliminando {full_file_path}: {e}\")\n",
        "                error_count += 1\n",
        "                continue\n",
        "            except KeyError as e:\n",
        "                # Manejar el caso donde la columna 'filename' o 'image_path' no existe (deber√≠a ser detectado antes)\n",
        "                print(f\"{spacer2}‚ùóÔ∏è ERROR: Falta la columna {e} en la fila {index}.\")\n",
        "                error_count += 1\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                # Capturar cualquier otro error inesperado para esta fila\n",
        "                print(f\"{spacer2}‚ùóÔ∏è ERROR INESPERADO procesando la fila {index} ({full_file_path}): {e}\")\n",
        "                error_count += 1\n",
        "                continue\n",
        "\n",
        "    work_done = len(remove_data_work)\n",
        "    undersampled_split = undersample_split[undersample_split['for_remove'] == False].copy()\n",
        "    pending_for_undersample = undersample_split[undersample_split['for_remove'] == True].copy()\n",
        "    if debugging or not_found_count or error_count:\n",
        "        print(\"\\n--- Resumen de Eliminaci√≥n ---\")\n",
        "        print(f\"Archivos eliminados exitosamente: {deleted_count}\")\n",
        "        print(f\"Archivos no encontrados en la ruta especificada: {not_found_count}\")\n",
        "        print(f\"Errores durante el proceso: {error_count}\")\n",
        "        print(f\"Total de filas procesadas: {work_done}\")\n",
        "        print(\"-----------------------------\")\n",
        "\n",
        "    if error_count:\n",
        "        print(f\"\\n{spacer1}‚ÄºÔ∏è ERROR: No se pudo completar la eliminaci√≥n para {error_count} archivos\", \"[Activar opci√≥n 'debugging' para m√°s detalle]\" if not debugging else None)\n",
        "    elif not_found_count:\n",
        "        print(\"\\n‚ÄºÔ∏è No se encontraron archivos indicados para eliminaci√≥n. [ERROR 2]\", \"[Activar opci√≥n 'debugging' para m√°s detalle]\" if not debugging else None)\n",
        "    else:\n",
        "        # Se crea el conjunto de datos submuestreado final:\n",
        "        if work_done != deleted_count:\n",
        "            print(\"\\n‚ÄºÔ∏è La cantidad de archivos para eliminar y efectivamente eliminados no coincide.\")\n",
        "            if debugging:\n",
        "                print(\"Deb√≠an removerse:\",len(indices_to_remove_all))\n",
        "                print(\"Se procesaron:\", deleted_count)\n",
        "                print(\"Se registraron en el dataframe:\",len(undersampled_split))\n",
        "        else:\n",
        "            print(\"\\n‚úÖ Eliminaci√≥n de archivos completada exitosamente.\")\n",
        "        print(f\"El conjunto de datos submuestreado final tiene {len(df_augmented)} im√°genes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18d9dbad",
      "metadata": {
        "id": "18d9dbad"
      },
      "source": [
        "## Verificaci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fe1eb17",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fe1eb17",
        "outputId": "606fdcfa-1a16-4a67-8f2a-3f8ea266b6cc"
      },
      "outputs": [],
      "source": [
        "print(f'Quedaron {len(error_log)} sin eliminar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b00bce9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "1b00bce9",
        "outputId": "53bf919f-3200-4eed-9d25-ea16c0501f17"
      },
      "outputs": [],
      "source": [
        "# Verificaci√≥n de entradas duplicadas\n",
        "filename_counts = df_augmented['filename'].value_counts()\n",
        "filename_counts.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ff7c3bd",
      "metadata": {
        "id": "0ff7c3bd"
      },
      "outputs": [],
      "source": [
        "for log in error_log:\n",
        "    if filename_counts.loc[log[2]]-1:\n",
        "        pass\n",
        "    else:\n",
        "        print(False)\n",
        "        print(log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b144e987",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b144e987",
        "outputId": "68ad0b10-e5cc-49f4-f6d6-d750598352b8"
      },
      "outputs": [],
      "source": [
        "error_log[-10:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "584cb713",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "584cb713",
        "outputId": "43fc467c-d110-44f0-855b-e95485e5a7ef"
      },
      "outputs": [],
      "source": [
        "# Se  verifica la distribucion\n",
        "data = undersampled_split.copy()\n",
        "found_clusters = find_clusters(data[target])\n",
        "plot_clusters(data, target, found_clusters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a776f90a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "a776f90a",
        "outputId": "62a997eb-95a7-46a5-9aaf-266f6e4e7e9d"
      },
      "outputs": [],
      "source": [
        "undersampled_split[undersampled_split['for_remove']==True]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c33da8c",
      "metadata": {
        "id": "1c33da8c"
      },
      "outputs": [],
      "source": [
        "del undersampled_split['for_remove']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46b638dc",
      "metadata": {
        "id": "46b638dc"
      },
      "outputs": [],
      "source": [
        "# MERGE DEL DATAFRAME REDUCIDO CON EL AUMENTADO\n",
        "# 1. Obt√©n las filas de df_augmented donde 'split' NO sea 'train'\n",
        "df_non_train = df_augmented[df_augmented['split'] != 'train'].copy()\n",
        "\n",
        "# 2. Combina ambos dataframes\n",
        "df_undersampled = pd.concat([df_non_train, undersampled_split])\n",
        "\n",
        "# Opcional: Restablece el √≠ndice del nuevo dataframe para un √≠ndice limpio y secuencial\n",
        "#df_undersampled = df_undersampled.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44b4b72e",
      "metadata": {
        "id": "44b4b72e"
      },
      "source": [
        "## Distribuci√≥n por clase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98f36875",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98f36875",
        "outputId": "b2974e8d-2c0d-430a-d5c4-fe520576e1f0"
      },
      "outputs": [],
      "source": [
        "### Resumen distribucion por grupo\n",
        "print(f\"Conteo por {target}:\")\n",
        "print(df_undersampled[target].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "554650a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "554650a6",
        "outputId": "390a31f2-0bb0-4290-968f-f35bcccfc203"
      },
      "outputs": [],
      "source": [
        "print(\"Efecto de la aumentaci√≥n de datos por clase:\\n\")\n",
        "counts_entrada = df_split[target].value_counts()\n",
        "counts_salida = df_undersampled[target].value_counts()\n",
        "\n",
        "classes = counts_salida.index\n",
        "for class_ in classes:\n",
        "    prop = counts_salida[class_]/counts_entrada[class_]\n",
        "    print(f\" - {class_}: {counts_entrada[class_]} -> {counts_salida[class_]} ({'+' if prop >1 else ''}{(prop-1)*100:.0f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "494b3ef2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "494b3ef2",
        "outputId": "3b7592d9-1ab5-4cc0-f5dc-b45c2f45a8d9"
      },
      "outputs": [],
      "source": [
        "# Se  verifica la distribucion\n",
        "data = df_undersampled.copy()\n",
        "found_clusters = find_clusters(data[target])\n",
        "plot_clusters(data, target, found_clusters)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zuJLwiu3kQTy",
      "metadata": {
        "id": "zuJLwiu3kQTy"
      },
      "source": [
        "----\n",
        "# Training model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lYYhbtz_kbqm",
      "metadata": {
        "id": "lYYhbtz_kbqm"
      },
      "source": [
        "## Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R5KPXKQ9lzs5",
      "metadata": {
        "id": "R5KPXKQ9lzs5"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vaH4dNL-kRZe",
      "metadata": {
        "id": "vaH4dNL-kRZe"
      },
      "outputs": [],
      "source": [
        "# Data laoders setup\n",
        "def load_from_directory(data_folder):\n",
        "    \"\"\"\n",
        "    Carga un dataset de im√°genes desde un directorio espec√≠fico.\n",
        "\n",
        "    Args:\n",
        "        data_folder (str): Ruta al directorio que contiene las im√°genes.\n",
        "\n",
        "    Returns:\n",
        "        tf.data.Dataset: Dataset de TensorFlow con las im√°genes y etiquetas.\n",
        "    \"\"\"\n",
        "    dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        data_folder,  # Ruta al directorio de datos\n",
        "        labels=\"inferred\",  # Las etiquetas se infieren autom√°ticamente desde los nombres de las carpetas\n",
        "        label_mode=\"categorical\",  # Las etiquetas se codifican como categor√≠as (one-hot encoding)\n",
        "        class_names=None,  # Las clases se detectan autom√°ticamente\n",
        "        color_mode=\"rgb\",  # Las im√°genes se cargan en modo RGB\n",
        "        batch_size=128,  # Tama√±o de lote para el entrenamiento\n",
        "        image_size=(256, 256),  # Redimensiona las im√°genes a 128x128 p√≠xeles\n",
        "        shuffle=True,  # Mezcla las im√°genes aleatoriamente\n",
        "        seed=42,  # No se utiliza una semilla espec√≠fica para la aleatorizaci√≥n\n",
        "        validation_split=None,  # No se realiza una divisi√≥n de validaci√≥n aqu√≠\n",
        "        subset=None,  # No se especifica un subconjunto (train/validation)\n",
        "        interpolation=\"bilinear\",  # M√©todo de interpolaci√≥n para redimensionar las im√°genes\n",
        "        follow_links=False,  # No sigue enlaces simb√≥licos\n",
        "        crop_to_aspect_ratio=False  # No recorta las im√°genes para ajustar la relaci√≥n de aspecto\n",
        "    )\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k21oXc7-kTqf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k21oXc7-kTqf",
        "outputId": "e9adf73a-ef67-4399-99c4-dcf523a9fb37"
      },
      "outputs": [],
      "source": [
        "# Carga el dataset de im√°genes desde el directorio especificado\n",
        "train_images = \"\"; test_images = \"\"; valid_images = \"\"\n",
        "\n",
        "print(\"Cargando datasets desde el directorio‚Ä¶\\n\")\n",
        "for split in splits:\n",
        "    data_folder = f'{SPLITTED_PATH}{split}/'\n",
        "\n",
        "    # Carga el conjunto de datos desde el directorio especificado\n",
        "    # Utiliza la funci√≥n de TensorFlow para crear un dataset de im√°genes\n",
        "    match split:\n",
        "        case 'train':\n",
        "            print(f\"Cargando dataset de entrenamiento desde:\\n > {data_folder}\")\n",
        "            train_images = load_from_directory(data_folder)\n",
        "        case 'test':\n",
        "            print(f\"Cargando dataset de test desde:\\n > {data_folder}\")\n",
        "            test_images = load_from_directory(data_folder)\n",
        "        case 'valid':\n",
        "            print(f\"Cargando dataset de validaci√≥n desde:\\n > {data_folder}\")\n",
        "            valid_images = load_from_directory(data_folder)\n",
        "        case _: # En caso de no coincidir con ninguno de los splits\n",
        "            print(f\"‚ö†Ô∏è El split '{split}' no es reconocido. No se cargar√° ning√∫n dataset.\")\n",
        "            continue # Salta al siguiente split\n",
        "    print(f\"‚úÖ Dataset cargado exitosamente.\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S5wkBadHlOcN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5wkBadHlOcN",
        "outputId": "7578c5f5-1209-40aa-8ff5-8b0dbf4d6ffb"
      },
      "outputs": [],
      "source": [
        "print(\"Resumen de los datasets cargados:\")\n",
        "print(f\" - Total de im√°genes en el dataset de entrenamiento: {len(train_images)}\")\n",
        "print(f\" - Total de im√°genes en el dataset de validaci√≥n: {len(valid_images)}\")\n",
        "print(f\" - Total de im√°genes en el dataset de test: {len(test_images)}\")\n",
        "print(f\"Total de im√°genes cargadas: {len(train_images) + len(test_images) + len(valid_images)}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vXPimzgmm78F",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXPimzgmm78F",
        "outputId": "41a6232b-e762-4fb6-872a-850abb4a33f4"
      },
      "outputs": [],
      "source": [
        "print(f\"Clases detectadas:\")\n",
        "[print(\" -\",clase) for clase in train_images.class_names]\n",
        "print(f\"Total de clases: {len(train_images.class_names)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UGKhzCLJm-gx",
      "metadata": {
        "id": "UGKhzCLJm-gx"
      },
      "source": [
        "----\n",
        "## Arquitectura del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xTKxLilrnADt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 974
        },
        "id": "xTKxLilrnADt",
        "outputId": "819bf763-a48a-43bf-ff1e-b47283aaf054"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras import Input\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "# Bloque 1\n",
        "model.add(Input(shape=(256, 256, 3)))\n",
        "model.add(layers.Rescaling(1./255))\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.1))\n",
        "\n",
        "# Bloque 2\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.1))\n",
        "\n",
        "# Bloque 3\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.1))\n",
        "\n",
        "# Bloque 4\n",
        "model.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.1))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "# Capa densa intermedia\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dropout(0.25))\n",
        "\n",
        "# Capa de salida con 38 neuronas y softmax para multiclase\n",
        "model.add(layers.Dense(38, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5lJh9l29nEg9",
      "metadata": {
        "id": "5lJh9l29nEg9"
      },
      "source": [
        "## Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tWE8J2-snD2L",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWE8J2-snD2L",
        "outputId": "7b10f3ed-655e-498b-aa97-d529d59ac85c"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "start_time = time.perf_counter()\n",
        "\n",
        "# Definimos el callback para guardar el mejor modelo seg√∫n la m√©trica elegida\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath='best_model.keras',   # Se generar√° una carpeta con este nombre\n",
        "    monitor='val_loss',            # M√©trica a monitorear ('val_accuracy' es otra opci√≥n)\n",
        "    save_best_only=True,           # Guarda solo si hay mejora\n",
        "    save_weights_only=False,       # Guarda la arquitectura + pesos\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Ajusta el modelo a tu criterio\n",
        "with tf.device('/GPU:0'):\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "history = model.fit(\n",
        "    train_images,\n",
        "    validation_data=test_images,\n",
        "    epochs=10,\n",
        "    callbacks=[checkpoint_callback]  # Incorporamos el callback\n",
        ")\n",
        "\n",
        "end_time = time.perf_counter()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Tiempo de entrenamiento: {elapsed_time:.2f} segundos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jQnG78SFnJ10",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQnG78SFnJ10",
        "outputId": "c9f478d1-c7a7-43b3-a574-3412eb1cb861"
      },
      "outputs": [],
      "source": [
        "print(f\"El entrenamiento tom√≥ {elapsed_time:.2f} segundos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5OiC0aJnnLtu",
      "metadata": {
        "id": "5OiC0aJnnLtu"
      },
      "source": [
        "## Guardando resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iy4o3hiNnNjx",
      "metadata": {
        "id": "iy4o3hiNnNjx"
      },
      "outputs": [],
      "source": [
        "#Recording History in json & pickle\n",
        "import json\n",
        "with open('training_hist.json','w') as f:\n",
        "  json.dump(history.history,f)\n",
        "\n",
        "import pickle\n",
        "with open('training_hist.pkl', 'wb') as f:\n",
        "    pickle.dump(history.history, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HHUmDV4HnQ_Z",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHUmDV4HnQ_Z",
        "outputId": "80175c66-9ec5-49b9-e6e6-6b678467f322"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "experiment = 'experimento_n' # Completar n√∫mero de experimento\n",
        "files = ['best_model.keras','training_hist.json','training_hist.pkl']\n",
        "destino=f\"/content/drive/MyDrive/CV2-PlantVillage/{experiment}/\"\n",
        "\n",
        "def check_folder(folder):\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "        print(f\"Folder '{folder}' created successfully.\")\n",
        "    else:\n",
        "        print(f\"Folder '{folder}' already exists.\")\n",
        "\n",
        "check_folder(destino)\n",
        "\n",
        "for file in files:\n",
        "    try:\n",
        "        origen=f\"/content/{file}\"\n",
        "        !cp -r \"$origen\" \"$destino\"\n",
        "    except:\n",
        "        print(f\"Error al copiar el archivo '{file}'\")\n",
        "    finally:\n",
        "        print(f\"Archivo '{file}' copiado exitosamente.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0MCZ-KXTnrn3",
      "metadata": {
        "id": "0MCZ-KXTnrn3"
      },
      "source": [
        "---\n",
        "# Gr√°ficos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Aa1ve2usnucW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "Aa1ve2usnucW",
        "outputId": "d4c4d58b-f0dc-4e1c-8746-6550d5aadf18"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = [i for i in range(1,11)]\n",
        "plt.plot(epochs,history.history['accuracy'],color='red',label='Training Accuracy')\n",
        "plt.plot(epochs,history.history['val_accuracy'],color='blue',label='Validation Accuracy')\n",
        "plt.xlabel('No. of Epochs')\n",
        "plt.title('Visualization of Accuracy Result')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J-GidBpGnxCm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-GidBpGnxCm",
        "outputId": "93dc61b5-9c31-409b-e73b-03e3d3ffab1b"
      },
      "outputs": [],
      "source": [
        "#Validation set Accuracy\n",
        "model = tf.keras.models.load_model('best_model.keras')\n",
        "val_loss, val_acc = model.evaluate(test_images)\n",
        "print('Validation accuracy:', val_acc)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
