{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LkNA9xnI2TN"
      },
      "source": [
        "# Notebook para pruebas de train en Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importaci√≥n de librer√≠as\n",
        "# Gesti√≥n de archivos y reporte\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Manipulaci√≥n y an√°lisis de datos\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Machine Learning\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargamos el dataframe desde el .CSV y definimos 'id' como √≠ndice\n",
        "try:\n",
        "    df_split = pd.read_csv('/content/drive/MyDrive/CV2-PlantVillage/dataframe_splitted.csv').set_index('id')\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ö†Ô∏è Error: El archivo 'dataframe.csv' no se encontr√≥ en la ubicaci√≥n actual: {os.getcwd()}\")\n",
        "    print(\"üö® Se crear√° nuevamente al correr las celdas de 'Importaci√≥n de im√°genes' üö®.\")\n",
        "    df_split = None\n",
        "except Exception as e:\n",
        "    print(f\"Ocurri√≥ un error al leer el archivo CSV: {e}\")\n",
        "    df_split = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_split.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Descarga de dataset de Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "DATASET_PATH = kagglehub.dataset_download(\"abdallahalidev/plantvillage-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", DATASET_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ruta de acceso al dataset\n",
        "ROOT_DIR = f'{DATASET_PATH}/plantvillage dataset/color'\n",
        "DATASET_PATH = None\n",
        "SPLITTED_PATH = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "towcap63I2TR"
      },
      "source": [
        "### Funciones necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pGOlUuiI2TR"
      },
      "outputs": [],
      "source": [
        "import os, re\n",
        "\n",
        "# Busca la carpeta ra√≠z del dataset en el directorio donde fue descargado\n",
        "def find_path(folder):\n",
        "    match = re.search(fr\"^(.*?)/{folder}/\", DATASET_PATH)\n",
        "    if match:\n",
        "        prefix = match.group(1)\n",
        "        path = os.path.join(prefix, f\"{folder}/\")\n",
        "        return path\n",
        "    else:\n",
        "        print(f'No se ha podido encontrar la carpeta \"{folder}\" en {DATASET_PATH}')\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9lAywfCI2TS"
      },
      "outputs": [],
      "source": [
        "# Carga de imagenes en memoria y visualizaci√≥n\n",
        "def load_image(data: pd.DataFrame, index: int, root: str=ROOT_DIR):\n",
        "    \"\"\"\n",
        "    Carga una imagen PIL desde una fila espec√≠fica de un DataFrame.\n",
        "\n",
        "    Args:\n",
        "        dataframe (pandas.DataFrame): El DataFrame que contiene las rutas de las im√°genes.\n",
        "        index (int): El √≠ndice de la fila en el DataFrame para cargar la imagen.\n",
        "        root_dir (str): El directorio ra√≠z donde se encuentran las im√°genes.\n",
        "\n",
        "    Returns:\n",
        "        PIL.Image.Image: La imagen cargada como un objeto PIL.Image, o None si ocurre un error.\n",
        "    \"\"\"\n",
        "    if index < 0 or index >= len(data):\n",
        "        print(\"√çndice fuera de rango.\")\n",
        "        return None\n",
        "\n",
        "    row = data.iloc[index]\n",
        "    relative_path = row['image_path']\n",
        "    filename = row['filename']\n",
        "    full_path = os.path.join(root, relative_path, filename)\n",
        "\n",
        "    try:\n",
        "        img = Image.open(full_path)\n",
        "        return img\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Archivo no encontrado: {full_path}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error al cargar la imagen: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGy5o6efI2TS"
      },
      "source": [
        "#### Descarga de dataset de Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-9XsvjnI2TS",
        "outputId": "338f6c83-54b8-4be9-e46e-f8e278436138"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "DATASET_PATH = kagglehub.dataset_download(\"abdallahalidev/plantvillage-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", DATASET_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pF3OLlHNI2TS"
      },
      "source": [
        "Decidimos en principio trabajar con el dataset con im√°genes a color por ser el que contiene mayor informaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRRlGvUNI2TS"
      },
      "outputs": [],
      "source": [
        "# Ruta de acceso al dataset\n",
        "ROOT_DIR = f'{DATASET_PATH}/plantvillage dataset/color'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgCX9HtUI2TU"
      },
      "source": [
        "# Dataset split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lutpWiCCI2TU"
      },
      "source": [
        "#### Funciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qW8UJHPFI2TU"
      },
      "outputs": [],
      "source": [
        "def dataset_already_exists(path_to_check: str) -> bool | None:\n",
        "    \"\"\"\n",
        "    Verifica si el directorio especificado existe y est√° vac√≠o.\n",
        "\n",
        "    Args:\n",
        "        path_to_check (str): Ruta del directorio a verificar.\n",
        "\n",
        "    Returns:\n",
        "        bool: True si el directorio existe y est√° vac√≠o, False en caso contrario.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path_to_check):\n",
        "        # El directorio no existe -> Crea el directorio\n",
        "        #print(f\"‚òëÔ∏è El directorio no existe, a√∫n no ha sido creado:\\n > {path_to_check}\") # Debugging\n",
        "        return False # No realiza ninguna acci√≥n\n",
        "    else:\n",
        "        # Verificar si el directorio est√° vac√≠o\n",
        "        try:\n",
        "            # Explora el contenido del directorio\n",
        "            content = os.listdir(path_to_check)\n",
        "            #print(content) # Debugging\n",
        "\n",
        "            # Si el directorio est√° vac√≠o, se puede eliminar directamente\n",
        "            #       -> Elimina sin confirmaci√≥n\n",
        "            if not content:\n",
        "                os.rmdir(path_to_check) # Elimina el directorio vac√≠o\n",
        "                print(f\"‚òëÔ∏è El directorio estaba vac√≠o y se ha eliminado de forma autom√°tica:\\n > {path_to_check}\\n\")\n",
        "                return False\n",
        "\n",
        "            # Si el directorio contiene s√≥lo archivos ocultos (de sistema)\n",
        "            #       -> Elimina sin confirmaci√≥n\n",
        "            elif all([file.startswith('.') for file in content]):\n",
        "                shutil.rmtree(path_to_check) # Elimina el directorio y su contenido\n",
        "                print(f\"‚òëÔ∏è El directorio s√≥lo conten√≠a archivos ocutlos, por lo que se ha eliminado de forma autom√°tica:\\n > {path_to_check}\\n\")\n",
        "                return False\n",
        "\n",
        "            # Si hay archivos visibles en el directorio (dataset ya existe)\n",
        "            #       -> Solicita permiso para eliminarlos\n",
        "            else:\n",
        "                # Input de confirmaci√≥n del usuario\n",
        "                confirmacion = input(f\"‚ö†Ô∏è El directorio especificado ya existe y contiene archivos. ¬øDeseas eliminar todo su contenido y el directorio en s√≠? [Y/N]: '{path_to_check}'\").strip().lower()\n",
        "                # Verifica la respuesta del usuario\n",
        "                if confirmacion == 'y':\n",
        "                    shutil.rmtree(path_to_check) # Elimina el directorio y su contenido\n",
        "                    print(f\"‚úÖ El directorio y su contenido han sido eliminados exitosamente:\\n > {path_to_check}\\n\")\n",
        "                    return False\n",
        "                else:\n",
        "                    print(f\"‚õîÔ∏è La eliminaci√≥n del directorio ha sido denegada por el usuario:\\n  > {path_to_check}\")\n",
        "                    return True\n",
        "\n",
        "        except OSError as e:\n",
        "            print(f\"‚ùå Error al eliminar el directorio vac√≠o en {path_to_check}: {e}\\n\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"‚ÄºÔ∏è Ocurri√≥ un error inesperado al intentar eliminar el directorio vac√≠o en {path_to_check}: {e}\\n\")\n",
        "            return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CM6sntHI2TV"
      },
      "outputs": [],
      "source": [
        "def create_ignore_function(df, train_label, filename_col='filename'):\n",
        "    \"\"\"\n",
        "    Crea y devuelve la funci√≥n 'ignore_files' que tiene acceso al DataFrame\n",
        "    y sabe qu√© archivos mantener.\n",
        "    \"\"\"\n",
        "    # Crea un conjunto (set) con los nombres de archivo que S√ç queremos copiar (ej: split == 'train')\n",
        "    # Usa este conjunto para hacer la b√∫squeda de forma mucho m√°s r√°pida\n",
        "    files_to_keep = set(df[df['split'] == train_label][filename_col])\n",
        "    #print(f\"Archivos a mantener (split='{train_label}'): {files_to_keep}\") # Debugging\n",
        "\n",
        "    def ignore_files(current_dir, files_in_current_dir):\n",
        "        \"\"\"\n",
        "        Funci√≥n que ser√° llamada por shutil.copytree.\n",
        "        Decide qu√© archivos/directorios ignorar en el directorio actual.\n",
        "        \"\"\"\n",
        "        ignore_list = []\n",
        "        for item in files_in_current_dir:\n",
        "            # Construye la ruta completa para verificar si es archivo o directorio\n",
        "            full_path = os.path.join(current_dir, item)\n",
        "\n",
        "            # Aplicar la l√≥gica de ignorar SOLO los ARCHIVOS de la lista\n",
        "            if os.path.isfile(full_path):\n",
        "                # Si el nombre del archivo NO est√° en el conjunto de archivos a mantener,\n",
        "                # entonces lo agrega a la lista de ignorados.\n",
        "                if item not in files_to_keep:\n",
        "                    # print(f\"Ignorando archivo: {item} (en {current_dir})\") # Debugging\n",
        "                    ignore_list.append(item)\n",
        "\n",
        "        # print(f\"Directorio: {current_dir}, Ignorando: {ignore_list}\") # Debugging\n",
        "        return ignore_list\n",
        "\n",
        "    # Devuelve la funci√≥n 'ignore_files' configurada\n",
        "    return ignore_files\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VozJJQWwI2TV"
      },
      "source": [
        "## Divis√≥n de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33MELJ_HcfEm"
      },
      "source": [
        "Se importa CSV con asignaci√≥n de splits precalculada y se dividen las imagenes a las carpetas correspondientes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-04T23:16:42.374657587Z",
          "start_time": "2025-04-04T21:56:49.128260Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "SX6i49jJI2TV",
        "outputId": "5e0d5cda-eb2d-4d65-ecc5-c162180292c1"
      },
      "outputs": [],
      "source": [
        "df_split.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_0pIcbLI2TV"
      },
      "source": [
        "### Construcci√≥n de carpetas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJRQoQkoI2TW",
        "outputId": "349acd6a-2e2d-4d61-a7f0-3e95b38ae973"
      },
      "outputs": [],
      "source": [
        "# Guarda directorio del dataset dividido\n",
        "path = find_path(\"plantvillage-dataset\")\n",
        "SPLITTED_PATH = f\"{path}splitted/\" if path else None\n",
        "SPLITTED_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1WFZkCgI2TW",
        "outputId": "30359209-2931-4666-b77f-ee90ba1ecba0"
      },
      "outputs": [],
      "source": [
        "splits = df_split['split'].value_counts().index.tolist()\n",
        "splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqH2MgdxI2TW",
        "outputId": "8642f2c5-b779-4956-b913-150950e47185"
      },
      "outputs": [],
      "source": [
        "verfication = True # Ejecuta el proceso de verificaci√≥n (punto 2)\n",
        "\n",
        "print(f\"Se inicia proceso de copiado del dataset‚Ä¶\")\n",
        "total_files = len(df_split) # Total de archivos del dataset\n",
        "print(f\" - Total de archivos en el dataset: {total_files}\")\n",
        "\n",
        "# Realiza el proceso de copiado de archivos para cada split\n",
        "succeeded_process = True\n",
        "for split in splits:\n",
        "    # Crea las rutas de origen y destino\n",
        "    # (Ejemplo: 'train', 'test', 'valid')\n",
        "    print(f\"\\n\\nIniciando proceso para '{split}' split ‚Ä¶\")\n",
        "    source_folder = f'{ROOT_DIR}/'\n",
        "    destination_folder = f'{SPLITTED_PATH}{split}/'\n",
        "    total_split = len(df_split[df_split['split'] == split]) # Total de archivos del split\n",
        "    if dataset_already_exists(destination_folder): # Verifica si el directorio existe y est√° vac√≠o\n",
        "        print(\"  ‚®Ø El directorio ya existe y contiene archivos, a petici√≥n del usuario se omite el proceso de copiado.\")\n",
        "        continue # Si el directorio ya existe, no se hace nada+\n",
        "    else:\n",
        "        print(f\"üîÑ Procesando split '{split.upper()}' ({(total_split/total_files*100):.2f}):\")\n",
        "        print(f\"  - Total de archivos a copiar: {total_split}\")\n",
        "    succeeded = False\n",
        "\n",
        "    try:\n",
        "        print(f\"1. Creando estructura de subcarpetas:\")\n",
        "        # 1. Crea la funci√≥n para ignorar espec√≠fica para el split a procesar\n",
        "        ignore_function = create_ignore_function(df_split, train_label=split, filename_col='filename')\n",
        "        print(f\"    ‚úî Funci√≥n de filtro creada para el split \")\n",
        "\n",
        "        # 2. Con copytree copia todo el \"√°rbol\" de directorios (careptas y subcarpetas)\n",
        "        # Fitrando con ignore_function todos aquellos archivos que no pertenecen al split deseado\n",
        "        print(f\"    ‚àû Copiando contenido del dataset (puede demorar hasta un minuto).\")\n",
        "        shutil.copytree(source_folder, destination_folder, ignore=ignore_function)\n",
        "        print(f\"    ‚úî Proceso de copiado del split finalizado.\")\n",
        "\n",
        "        if verfication:\n",
        "            # Verifica qu√© se haya copiado adecuadamente (opcional pero √∫til)\n",
        "            print(f\"2. Se inicia proceso de verificaci√≥n‚Ä¶\")\n",
        "            copied_files = []\n",
        "            for root, dirs, files_in_dest in os.walk(destination_folder):\n",
        "                for name in files_in_dest:\n",
        "                    copied_files.append(os.path.join(os.path.relpath(root, destination_folder), name).replace('\\\\', '/')) # Normalizar path\n",
        "                    #print(f\"  - {os.path.join(root, name)}\") # Debuggin\n",
        "            print(f\"    ‚úî Se crearon un total de {len(os.listdir(destination_folder))} carpetas (para las clases).\")\n",
        "            print(f\"    ‚úî Se copiaron un total de {len(copied_files)} archivos ({len(copied_files)/total_split*100:.2f}%)\")\n",
        "            # Agregar confirmaci√≥n de igualdad cantidad split == copiados\n",
        "            if len(copied_files) == total_split:\n",
        "                print(f\"‚úÖ Se complet√≥ satisfactoriamente el subproceso de copiado para el split.\\n\")\n",
        "                succeeded = True\n",
        "            else:\n",
        "                print(f\" ‚ùå Error: No se pudo copiar correctamente el split '{split.upper()}'\\n\")\n",
        "                succeeded = False\n",
        "        else:\n",
        "            succeeded = True # Si la verificaci√≥n est√° desactivada, se asume que el proceso fue exitoso\n",
        "\n",
        "    except FileExistsError:\n",
        "        print(f\"Error: La carpeta de destino '{destination_folder}' ya existe.\\n\")\n",
        "        pass\n",
        "    except Exception as e:\n",
        "        print(f\"Ocurri√≥ un error inesperado: {e}\\n\")\n",
        "\n",
        "    succeeded_process *= succeeded # Actualiza el estado del proceso\n",
        "    # (S√≥lo es 'True' si todos los splits se copian correctamente)\n",
        "\n",
        "if succeeded_process:\n",
        "    print(\"\\n\\nüåü El proceso de copiado del dataset ha finalizado con √©xito.\\n\")\n",
        "else:\n",
        "    print(\"\\n\\nüö´ No se pudo completar satisfactoriamente el proceso de copiado del dataset.\\nVerificar que se haya completado la eliminaci√≥n de las carpetas.\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bH1NHp2I2TW"
      },
      "source": [
        "----\n",
        "# Training model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdJ4L63nI2TW"
      },
      "source": [
        "## Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GR1f2R6BI2TW"
      },
      "outputs": [],
      "source": [
        "# Data laoders setup\n",
        "def load_from_directory(data_folder):\n",
        "    \"\"\"\n",
        "    Carga un dataset de im√°genes desde un directorio espec√≠fico.\n",
        "\n",
        "    Args:\n",
        "        data_folder (str): Ruta al directorio que contiene las im√°genes.\n",
        "\n",
        "    Returns:\n",
        "        tf.data.Dataset: Dataset de TensorFlow con las im√°genes y etiquetas.\n",
        "    \"\"\"\n",
        "    dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        data_folder,  # Ruta al directorio de datos\n",
        "        labels=\"inferred\",  # Las etiquetas se infieren autom√°ticamente desde los nombres de las carpetas\n",
        "        label_mode=\"categorical\",  # Las etiquetas se codifican como categor√≠as (one-hot encoding)\n",
        "        class_names=None,  # Las clases se detectan autom√°ticamente\n",
        "        color_mode=\"rgb\",  # Las im√°genes se cargan en modo RGB\n",
        "        batch_size=128,  # Tama√±o de lote para el entrenamiento\n",
        "        image_size=(256, 256),  # Redimensiona las im√°genes a 128x128 p√≠xeles\n",
        "        shuffle=True,  # Mezcla las im√°genes aleatoriamente\n",
        "        seed=42,  # No se utiliza una semilla espec√≠fica para la aleatorizaci√≥n\n",
        "        validation_split=None,  # No se realiza una divisi√≥n de validaci√≥n aqu√≠\n",
        "        subset=None,  # No se especifica un subconjunto (train/validation)\n",
        "        interpolation=\"bilinear\",  # M√©todo de interpolaci√≥n para redimensionar las im√°genes\n",
        "        follow_links=False,  # No sigue enlaces simb√≥licos\n",
        "        crop_to_aspect_ratio=False  # No recorta las im√°genes para ajustar la relaci√≥n de aspecto\n",
        "    )\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CX4meyPHI2TX",
        "outputId": "c35bc5eb-8168-4787-84e5-97e8824538bb"
      },
      "outputs": [],
      "source": [
        "# Carga el dataset de im√°genes desde el directorio especificado\n",
        "train_images = \"\"; test_images = \"\"; valid_images = \"\"\n",
        "\n",
        "print(\"Cargando datasets desde el directorio‚Ä¶\\n\")\n",
        "for split in splits:\n",
        "    data_folder = f'{SPLITTED_PATH}{split}/'\n",
        "\n",
        "    # Carga el conjunto de datos desde el directorio especificado\n",
        "    # Utiliza la funci√≥n de TensorFlow para crear un dataset de im√°genes\n",
        "    match split:\n",
        "        case 'train':\n",
        "            print(f\"Cargando dataset de entrenamiento desde:\\n > {data_folder}\")\n",
        "            train_images = load_from_directory(data_folder)\n",
        "        case 'test':\n",
        "            print(f\"Cargando dataset de test desde:\\n > {data_folder}\")\n",
        "            test_images = load_from_directory(data_folder)\n",
        "        case 'valid':\n",
        "            print(f\"Cargando dataset de validaci√≥n desde:\\n > {data_folder}\")\n",
        "            valid_images = load_from_directory(data_folder)\n",
        "        case _: # En caso de no coincidir con ninguno de los splits\n",
        "            print(f\"‚ö†Ô∏è El split '{split}' no es reconocido. No se cargar√° ning√∫n dataset.\")\n",
        "            continue # Salta al siguiente split\n",
        "    print(f\"‚úÖ Dataset cargado exitosamente.\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egwVG5_yI2TX",
        "outputId": "43e83f7b-a7f1-43c6-f8ab-161eab89b64e"
      },
      "outputs": [],
      "source": [
        "print(\"Resumen de los datasets cargados:\")\n",
        "print(f\" - Total de im√°genes en el dataset de entrenamiento: {len(train_images)}\")\n",
        "print(f\" - Total de im√°genes en el dataset de validaci√≥n: {len(valid_images)}\")\n",
        "print(f\" - Total de im√°genes en el dataset de test: {len(test_images)}\")\n",
        "print(f\"Total de im√°genes cargadas: {len(train_images) + len(test_images) + len(valid_images)}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9Ck8YCbI2TX",
        "outputId": "603a5617-a811-412e-88e5-dfa789e8a293"
      },
      "outputs": [],
      "source": [
        "print(f\"Clases detectadas:\")\n",
        "[print(\" -\",clase) for clase in train_images.class_names]\n",
        "print(f\"Total de clases: {len(train_images.class_names)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kviEs_CI2TX"
      },
      "source": [
        "----\n",
        "## Arquitectura del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 974
        },
        "id": "zGwMznw0I2TY",
        "outputId": "f86e6013-b33f-45e7-f753-6fb402c34fc6"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras import Input\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "# Bloque 1\n",
        "model.add(Input(shape=(256, 256, 3)))\n",
        "model.add(layers.Rescaling(1./255))\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.1))\n",
        "\n",
        "# Bloque 2\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.1))\n",
        "\n",
        "# Bloque 3\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.1))\n",
        "\n",
        "# Bloque 4\n",
        "model.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.1))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "# Capa densa intermedia\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dropout(0.25))\n",
        "\n",
        "# Capa de salida con 38 neuronas y softmax para multiclase\n",
        "model.add(layers.Dense(38, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKzMCoyQI2TY"
      },
      "source": [
        "## Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feTD6PRAI2TY",
        "outputId": "19852a81-7ffe-40b7-b162-c76f46d17beb"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "start_time = time.perf_counter()\n",
        "\n",
        "# Definimos el callback para guardar el mejor modelo seg√∫n la m√©trica elegida\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath='best_model.keras',   # Se generar√° una carpeta con este nombre\n",
        "    monitor='val_loss',            # M√©trica a monitorear ('val_accuracy' es otra opci√≥n)\n",
        "    save_best_only=True,           # Guarda solo si hay mejora\n",
        "    save_weights_only=False,       # Guarda la arquitectura + pesos\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Ajusta el modelo a tu criterio\n",
        "with tf.device('/GPU:0'):\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "history = model.fit(\n",
        "    train_images,\n",
        "    validation_data=test_images,\n",
        "    epochs=10,\n",
        "    callbacks=[checkpoint_callback]  # Incorporamos el callback\n",
        ")\n",
        "\n",
        "end_time = time.perf_counter()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Tiempo de entrenamiento: {elapsed_time:.2f} segundos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQAOhynxI2TY"
      },
      "source": [
        "## Guardando resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "it_ATa-HI2TY"
      },
      "outputs": [],
      "source": [
        "#Recording History in json & pickle\n",
        "import json\n",
        "with open('training_hist.json','w') as f:\n",
        "  json.dump(history.history,f)\n",
        "\n",
        "import pickle\n",
        "with open('training_hist.pkl', 'wb') as f:\n",
        "    pickle.dump(history.history, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4UbPwaXI2TY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "experiment = 'experimento_1' # Completar n√∫mero de experimento\n",
        "files = ['best_model.keras','training_hist.json','training_hist.pkl']\n",
        "destino=f\"/content/drive/MyDrive/CV2-PlantVillage/{experiment}/\"\n",
        "\n",
        "def check_folder(folder):\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "        print(f\"Folder '{folder}' created successfully.\")\n",
        "    else:\n",
        "        print(f\"Folder '{folder}' already exists.\")\n",
        "\n",
        "check_folder(destino)\n",
        "\n",
        "for file in files:\n",
        "    try:\n",
        "        origen=f\"/content/{file}\"\n",
        "        !cp -r \"$origen\" \"$destino\"\n",
        "    except:\n",
        "        print(f\"Error al copiar el archivo '{file}'\")\n",
        "    finally:\n",
        "        print(f\"Archivo '{file}' copiado exitosamente.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wajVEmO1I2TY"
      },
      "source": [
        "---\n",
        "# Gr√°ficos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvY2nDDMI2TZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = [i for i in range(1,11)]\n",
        "plt.plot(epochs,history.history['accuracy'],color='red',label='Training Accuracy')\n",
        "plt.plot(epochs,history.history['val_accuracy'],color='blue',label='Validation Accuracy')\n",
        "plt.xlabel('No. of Epochs')\n",
        "plt.title('Visualization of Accuracy Result')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_YzKxnDI2TZ"
      },
      "outputs": [],
      "source": [
        "#Validation set Accuracy\n",
        "model = tf.keras.models.load_model('best_model.keras')\n",
        "val_loss, val_acc = model.evaluate(test_images)\n",
        "print('Validation accuracy:', val_acc)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
