{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3LkNA9xnI2TN"
   },
   "source": [
    "# Notebook para pruebas de train en Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNYbLTzEI2TP"
   },
   "source": [
    "## Inicializaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-NSvKsHgI2TP",
    "ExecuteTime": {
     "end_time": "2025-04-22T02:07:35.759731Z",
     "start_time": "2025-04-22T02:07:34.061766Z"
    }
   },
   "source": [
    "# Importaci√≥n de librer√≠as\n",
    "# Gesti√≥n de archivos y reporte\n",
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "import yaml\n",
    "\n",
    "# Manipulaci√≥n y an√°lisis de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Procesamiento de im√°genes\n",
    "from PIL import Image\n",
    "\n",
    "# Machine Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 23:07:34.360630: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-21 23:07:34.401133: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745287654.416027 1367172 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745287654.422415 1367172 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745287654.459966 1367172 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745287654.459977 1367172 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745287654.459978 1367172 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745287654.459980 1367172 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-21 23:07:34.468310: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcfGqLvSI2TQ"
   },
   "source": [
    "### Carga de datos almacenados"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xOGPlIM0I2TR",
    "ExecuteTime": {
     "end_time": "2025-04-22T02:07:35.820182Z",
     "start_time": "2025-04-22T02:07:35.762674Z"
    }
   },
   "source": [
    "# Cargamos el dataframe desde el .CSV y definimos 'id' como √≠ndice\n",
    "try:\n",
    "    df_split = pd.read_csv('dataframe_splitted.csv').set_index('id')\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ö†Ô∏è Error: El archivo 'dataframe.csv' no se encontr√≥ en la ubicaci√≥n actual: {os.getcwd()}\")\n",
    "    print(\"üö® Se crear√° nuevamente al correr las celdas de 'Importaci√≥n de im√°genes' üö®.\")\n",
    "    df_split = None\n",
    "except Exception as e:\n",
    "    print(f\"Ocurri√≥ un error al leer el archivo CSV: {e}\")\n",
    "    df_split = None"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "df_split.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "fMvdw25gcpYF",
    "outputId": "c4cab1a0-668d-4911-e1a1-fdc585e2172a",
    "ExecuteTime": {
     "end_time": "2025-04-22T02:07:35.857212Z",
     "start_time": "2025-04-22T02:07:35.852709Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               image_path                                           filename  \\\n",
       "id                                                                             \n",
       "0   Strawberry___healthy/  8f558908-aa1b-4a86-855a-5094c2392e5a___RS_HL 1...   \n",
       "1   Strawberry___healthy/  b8e9ed27-8e37-4214-9206-f8c0ef21cf4d___RS_HL 4...   \n",
       "2   Strawberry___healthy/  abdd34a0-ab02-41e0-95a3-a014ab863ec2___RS_HL 1...   \n",
       "3   Strawberry___healthy/  d1aee44a-b6bb-45b9-b7b6-5d553add8fd1___RS_HL 2...   \n",
       "4   Strawberry___healthy/  3d28c3ea-8419-4e09-addd-211e3828e39f___RS_HL 1...   \n",
       "\n",
       "                   class       group      tag  split  \n",
       "id                                                    \n",
       "0   Strawberry___healthy  Strawberry  healthy  train  \n",
       "1   Strawberry___healthy  Strawberry  healthy   test  \n",
       "2   Strawberry___healthy  Strawberry  healthy  train  \n",
       "3   Strawberry___healthy  Strawberry  healthy  train  \n",
       "4   Strawberry___healthy  Strawberry  healthy  train  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>filename</th>\n",
       "      <th>class</th>\n",
       "      <th>group</th>\n",
       "      <th>tag</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Strawberry___healthy/</td>\n",
       "      <td>8f558908-aa1b-4a86-855a-5094c2392e5a___RS_HL 1...</td>\n",
       "      <td>Strawberry___healthy</td>\n",
       "      <td>Strawberry</td>\n",
       "      <td>healthy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Strawberry___healthy/</td>\n",
       "      <td>b8e9ed27-8e37-4214-9206-f8c0ef21cf4d___RS_HL 4...</td>\n",
       "      <td>Strawberry___healthy</td>\n",
       "      <td>Strawberry</td>\n",
       "      <td>healthy</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Strawberry___healthy/</td>\n",
       "      <td>abdd34a0-ab02-41e0-95a3-a014ab863ec2___RS_HL 1...</td>\n",
       "      <td>Strawberry___healthy</td>\n",
       "      <td>Strawberry</td>\n",
       "      <td>healthy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Strawberry___healthy/</td>\n",
       "      <td>d1aee44a-b6bb-45b9-b7b6-5d553add8fd1___RS_HL 2...</td>\n",
       "      <td>Strawberry___healthy</td>\n",
       "      <td>Strawberry</td>\n",
       "      <td>healthy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Strawberry___healthy/</td>\n",
       "      <td>3d28c3ea-8419-4e09-addd-211e3828e39f___RS_HL 1...</td>\n",
       "      <td>Strawberry___healthy</td>\n",
       "      <td>Strawberry</td>\n",
       "      <td>healthy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XfgL3b2rI2TR",
    "outputId": "95dffcab-2073-4c33-bb0b-a40c9229f56a",
    "ExecuteTime": {
     "end_time": "2025-04-22T02:07:35.964098Z",
     "start_time": "2025-04-22T02:07:35.961276Z"
    }
   },
   "source": [
    "# Cargar variables desde el archivo YAML\n",
    "try:\n",
    "    # Verificar si el archivo YAML existe\n",
    "    yaml_filename = \"constants.yaml\"\n",
    "    with open(yaml_filename, \"r\") as yaml_file:\n",
    "        constants_data = yaml.safe_load(yaml_file)\n",
    "\n",
    "    # Acceder a las variables\n",
    "    ROOT_DIR = constants_data.get(\"ROOT_DIR\")\n",
    "    DATASET_PATH = constants_data.get(\"DATASET_PATH\")\n",
    "    SPLITTED_PATH = constants_data.get(\"SPLITTED_PATH\")\n",
    "\n",
    "    print(f\"‚úÖ Se han cargado las variables de configuraci√≥n desde '{yaml_filename}'\")\n",
    "    print(f\" - ROOT_DIR: {ROOT_DIR}\")\n",
    "    print(f\" - DATASET_PATH: {DATASET_PATH}\")\n",
    "    print(f\" - SPLITTED_PATH: {SPLITTED_PATH}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: El archivo 'constants.yaml' no se encontr√≥ en la ubicaci√≥n actual: {os.getcwd()}\")\n",
    "    print(\"Se crear√° nuevamente al correr el notebook.\")\n",
    "    ROOT_DIR = None\n",
    "    DATASET_PATH = None\n",
    "    SPLITTED_PATH = None\n",
    "except Exception as e:\n",
    "    print(f\"Ocurri√≥ un error al leer el archivo YAML: {e}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Se han cargado las variables de configuraci√≥n desde 'constants.yaml'\n",
      " - ROOT_DIR: /home/juan/.cache/kagglehub/datasets/abdallahalidev/plantvillage-dataset/versions/3/plantvillage dataset/color\n",
      " - DATASET_PATH: /home/juan/.cache/kagglehub/datasets/abdallahalidev/plantvillage-dataset/versions/3\n",
      " - SPLITTED_PATH: /home/juan/.cache/kagglehub/datasets/abdallahalidev/plantvillage-dataset/splitted/\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "towcap63I2TR"
   },
   "source": [
    "### Funciones necesarias"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5pGOlUuiI2TR",
    "ExecuteTime": {
     "end_time": "2025-04-22T02:07:36.010812Z",
     "start_time": "2025-04-22T02:07:36.008809Z"
    }
   },
   "source": [
    "import os, re\n",
    "\n",
    "# Busca la carpeta ra√≠z del dataset en el directorio donde fue descargado\n",
    "def find_path(folder):\n",
    "    match = re.search(fr\"^(.*?)/{folder}/\", DATASET_PATH)\n",
    "    if match:\n",
    "        prefix = match.group(1)\n",
    "        path = os.path.join(prefix, f\"{folder}/\")\n",
    "        return path\n",
    "    else:\n",
    "        print(f'No se ha podido encontrar la carpeta \"{folder}\" en {DATASET_PATH}')\n",
    "        return None"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "t9lAywfCI2TS",
    "ExecuteTime": {
     "end_time": "2025-04-22T02:07:36.037580Z",
     "start_time": "2025-04-22T02:07:36.035243Z"
    }
   },
   "source": [
    "# Carga de imagenes en memoria y visualizaci√≥n\n",
    "def load_image(data: pd.DataFrame, index: int, root: str=ROOT_DIR):\n",
    "    \"\"\"\n",
    "    Carga una imagen PIL desde una fila espec√≠fica de un DataFrame.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): El DataFrame que contiene las rutas de las im√°genes.\n",
    "        index (int): El √≠ndice de la fila en el DataFrame para cargar la imagen.\n",
    "        root_dir (str): El directorio ra√≠z donde se encuentran las im√°genes.\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image.Image: La imagen cargada como un objeto PIL.Image, o None si ocurre un error.\n",
    "    \"\"\"\n",
    "    if index < 0 or index >= len(data):\n",
    "        print(\"√çndice fuera de rango.\")\n",
    "        return None\n",
    "\n",
    "    row = data.iloc[index]\n",
    "    relative_path = row['image_path']\n",
    "    filename = row['filename']\n",
    "    full_path = os.path.join(root, relative_path, filename)\n",
    "\n",
    "    try:\n",
    "        img = Image.open(full_path)\n",
    "        return img\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Archivo no encontrado: {full_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar la imagen: {e}\")\n",
    "        return None"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGy5o6efI2TS"
   },
   "source": [
    "#### Descarga de dataset de Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0-9XsvjnI2TS",
    "outputId": "e0f7ea5b-7232-4ea1-9aef-15a0326a91b6",
    "ExecuteTime": {
     "end_time": "2025-04-22T02:07:36.849344Z",
     "start_time": "2025-04-22T02:07:36.089274Z"
    }
   },
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "DATASET_PATH = kagglehub.dataset_download(\"abdallahalidev/plantvillage-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", DATASET_PATH)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.8), please consider upgrading to the latest version (0.3.11).\n",
      "Path to dataset files: /home/juan/.cache/kagglehub/datasets/abdallahalidev/plantvillage-dataset/versions/3\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pF3OLlHNI2TS"
   },
   "source": [
    "Decidimos en principio trabajar con el dataset con im√°genes a color por ser el que contiene mayor informaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uRRlGvUNI2TS",
    "ExecuteTime": {
     "end_time": "2025-04-22T02:07:36.857160Z",
     "start_time": "2025-04-22T02:07:36.855588Z"
    }
   },
   "source": [
    "# Ruta de acceso al dataset\n",
    "ROOT_DIR = f'{DATASET_PATH}/plantvillage dataset/color'"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SgCX9HtUI2TU"
   },
   "source": [
    "# Dataset split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lutpWiCCI2TU"
   },
   "source": [
    "#### Funciones"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qW8UJHPFI2TU",
    "ExecuteTime": {
     "end_time": "2025-04-22T02:07:36.914999Z",
     "start_time": "2025-04-22T02:07:36.911715Z"
    }
   },
   "source": [
    "def dataset_already_exists(path_to_check: str) -> bool | None:\n",
    "    \"\"\"\n",
    "    Verifica si el directorio especificado existe y est√° vac√≠o.\n",
    "\n",
    "    Args:\n",
    "        path_to_check (str): Ruta del directorio a verificar.\n",
    "\n",
    "    Returns:\n",
    "        bool: True si el directorio existe y est√° vac√≠o, False en caso contrario.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path_to_check):\n",
    "        # El directorio no existe -> Crea el directorio\n",
    "        #print(f\"‚òëÔ∏è El directorio no existe, a√∫n no ha sido creado:\\n > {path_to_check}\") # Debugging\n",
    "        return False # No realiza ninguna acci√≥n\n",
    "    else:\n",
    "        # Verificar si el directorio est√° vac√≠o\n",
    "        try:\n",
    "            # Explora el contenido del directorio\n",
    "            content = os.listdir(path_to_check)\n",
    "            #print(content) # Debugging\n",
    "\n",
    "            # Si el directorio est√° vac√≠o, se puede eliminar directamente\n",
    "            #       -> Elimina sin confirmaci√≥n\n",
    "            if not content:\n",
    "                os.rmdir(path_to_check) # Elimina el directorio vac√≠o\n",
    "                print(f\"‚òëÔ∏è El directorio estaba vac√≠o y se ha eliminado de forma autom√°tica:\\n > {path_to_check}\\n\")\n",
    "                return False\n",
    "\n",
    "            # Si el directorio contiene s√≥lo archivos ocultos (de sistema)\n",
    "            #       -> Elimina sin confirmaci√≥n\n",
    "            elif all([file.startswith('.') for file in content]):\n",
    "                shutil.rmtree(path_to_check) # Elimina el directorio y su contenido\n",
    "                print(f\"‚òëÔ∏è El directorio s√≥lo conten√≠a archivos ocutlos, por lo que se ha eliminado de forma autom√°tica:\\n > {path_to_check}\\n\")\n",
    "                return False\n",
    "\n",
    "            # Si hay archivos visibles en el directorio (dataset ya existe)\n",
    "            #       -> Solicita permiso para eliminarlos\n",
    "            else:\n",
    "                # Input de confirmaci√≥n del usuario\n",
    "                confirmacion = input(f\"‚ö†Ô∏è El directorio especificado ya existe y contiene archivos. ¬øDeseas eliminar todo su contenido y el directorio en s√≠? [Y/N]: '{path_to_check}'\").strip().lower()\n",
    "                # Verifica la respuesta del usuario\n",
    "                if confirmacion == 'y':\n",
    "                    shutil.rmtree(path_to_check) # Elimina el directorio y su contenido\n",
    "                    print(f\"‚úÖ El directorio y su contenido han sido eliminados exitosamente:\\n > {path_to_check}\\n\")\n",
    "                    return False\n",
    "                else:\n",
    "                    print(f\"‚õîÔ∏è La eliminaci√≥n del directorio ha sido denegada por el usuario:\\n  > {path_to_check}\")\n",
    "                    return True\n",
    "\n",
    "        except OSError as e:\n",
    "            print(f\"‚ùå Error al eliminar el directorio vac√≠o en {path_to_check}: {e}\\n\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"‚ÄºÔ∏è Ocurri√≥ un error inesperado al intentar eliminar el directorio vac√≠o en {path_to_check}: {e}\\n\")\n",
    "            return None"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2CM6sntHI2TV",
    "ExecuteTime": {
     "end_time": "2025-04-22T02:07:36.960581Z",
     "start_time": "2025-04-22T02:07:36.958224Z"
    }
   },
   "source": [
    "def create_ignore_function(df, train_label, filename_col='filename'):\n",
    "    \"\"\"\n",
    "    Crea y devuelve la funci√≥n 'ignore_files' que tiene acceso al DataFrame\n",
    "    y sabe qu√© archivos mantener.\n",
    "    \"\"\"\n",
    "    # Crea un conjunto (set) con los nombres de archivo que S√ç queremos copiar (ej: split == 'train')\n",
    "    # Usa este conjunto para hacer la b√∫squeda de forma mucho m√°s r√°pida\n",
    "    files_to_keep = set(df[df['split'] == train_label][filename_col])\n",
    "    #print(f\"Archivos a mantener (split='{train_label}'): {files_to_keep}\") # Debugging\n",
    "\n",
    "    def ignore_files(current_dir, files_in_current_dir):\n",
    "        \"\"\"\n",
    "        Funci√≥n que ser√° llamada por shutil.copytree.\n",
    "        Decide qu√© archivos/directorios ignorar en el directorio actual.\n",
    "        \"\"\"\n",
    "        ignore_list = []\n",
    "        for item in files_in_current_dir:\n",
    "            # Construye la ruta completa para verificar si es archivo o directorio\n",
    "            full_path = os.path.join(current_dir, item)\n",
    "\n",
    "            # Aplicar la l√≥gica de ignorar SOLO los ARCHIVOS de la lista\n",
    "            if os.path.isfile(full_path):\n",
    "                # Si el nombre del archivo NO est√° en el conjunto de archivos a mantener,\n",
    "                # entonces lo agrega a la lista de ignorados.\n",
    "                if item not in files_to_keep:\n",
    "                    # print(f\"Ignorando archivo: {item} (en {current_dir})\") # Debugging\n",
    "                    ignore_list.append(item)\n",
    "\n",
    "        # print(f\"Directorio: {current_dir}, Ignorando: {ignore_list}\") # Debugging\n",
    "        return ignore_list\n",
    "\n",
    "    # Devuelve la funci√≥n 'ignore_files' configurada\n",
    "    return ignore_files\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VozJJQWwI2TV"
   },
   "source": [
    "## Divis√≥n de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33MELJ_HcfEm"
   },
   "source": [
    "Se importa CSV con asignaci√≥n de splits precalculada y se dividen las imagenes a las carpetas correspondientes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "SX6i49jJI2TV",
    "outputId": "ce3bf272-4f6a-4632-aebb-8cea74ddc305",
    "ExecuteTime": {
     "end_time": "2025-04-22T02:07:37.014444Z",
     "start_time": "2025-04-22T02:07:37.009554Z"
    }
   },
   "source": [
    "df_split.sample(5)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                      image_path  \\\n",
       "id                                                 \n",
       "27754                          Tomato___healthy/   \n",
       "31801    Tomato___Tomato_Yellow_Leaf_Curl_Virus/   \n",
       "16079  Orange___Haunglongbing_(Citrus_greening)/   \n",
       "29622         Cherry_(including_sour)___healthy/   \n",
       "33428    Tomato___Tomato_Yellow_Leaf_Curl_Virus/   \n",
       "\n",
       "                                                filename  \\\n",
       "id                                                         \n",
       "27754  4127b1a7-3442-4847-96cb-7ea68495039e___GH_HL L...   \n",
       "31801  07621782-2709-4cfb-b4ff-1f1bad24df64___YLCV_GC...   \n",
       "16079  8b92b2d2-a852-4a35-b351-942f23eefce9___CREC_HL...   \n",
       "29622  db2c8b52-1f7d-4bb7-8cf1-81f241091122___JR_HL 9...   \n",
       "33428  b3fe3d8c-f0e0-405c-a67d-ac57db6be043___YLCV_GC...   \n",
       "\n",
       "                                          class                    group  \\\n",
       "id                                                                         \n",
       "27754                          Tomato___healthy                   Tomato   \n",
       "31801    Tomato___Tomato_Yellow_Leaf_Curl_Virus                   Tomato   \n",
       "16079  Orange___Haunglongbing_(Citrus_greening)                   Orange   \n",
       "29622         Cherry_(including_sour)___healthy  Cherry_(including_sour)   \n",
       "33428    Tomato___Tomato_Yellow_Leaf_Curl_Virus                   Tomato   \n",
       "\n",
       "                                   tag  split  \n",
       "id                                             \n",
       "27754                          healthy  train  \n",
       "31801    Tomato_Yellow_Leaf_Curl_Virus   test  \n",
       "16079  Haunglongbing_(Citrus_greening)  train  \n",
       "29622                          healthy  train  \n",
       "33428    Tomato_Yellow_Leaf_Curl_Virus  train  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>filename</th>\n",
       "      <th>class</th>\n",
       "      <th>group</th>\n",
       "      <th>tag</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27754</th>\n",
       "      <td>Tomato___healthy/</td>\n",
       "      <td>4127b1a7-3442-4847-96cb-7ea68495039e___GH_HL L...</td>\n",
       "      <td>Tomato___healthy</td>\n",
       "      <td>Tomato</td>\n",
       "      <td>healthy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31801</th>\n",
       "      <td>Tomato___Tomato_Yellow_Leaf_Curl_Virus/</td>\n",
       "      <td>07621782-2709-4cfb-b4ff-1f1bad24df64___YLCV_GC...</td>\n",
       "      <td>Tomato___Tomato_Yellow_Leaf_Curl_Virus</td>\n",
       "      <td>Tomato</td>\n",
       "      <td>Tomato_Yellow_Leaf_Curl_Virus</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16079</th>\n",
       "      <td>Orange___Haunglongbing_(Citrus_greening)/</td>\n",
       "      <td>8b92b2d2-a852-4a35-b351-942f23eefce9___CREC_HL...</td>\n",
       "      <td>Orange___Haunglongbing_(Citrus_greening)</td>\n",
       "      <td>Orange</td>\n",
       "      <td>Haunglongbing_(Citrus_greening)</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29622</th>\n",
       "      <td>Cherry_(including_sour)___healthy/</td>\n",
       "      <td>db2c8b52-1f7d-4bb7-8cf1-81f241091122___JR_HL 9...</td>\n",
       "      <td>Cherry_(including_sour)___healthy</td>\n",
       "      <td>Cherry_(including_sour)</td>\n",
       "      <td>healthy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33428</th>\n",
       "      <td>Tomato___Tomato_Yellow_Leaf_Curl_Virus/</td>\n",
       "      <td>b3fe3d8c-f0e0-405c-a67d-ac57db6be043___YLCV_GC...</td>\n",
       "      <td>Tomato___Tomato_Yellow_Leaf_Curl_Virus</td>\n",
       "      <td>Tomato</td>\n",
       "      <td>Tomato_Yellow_Leaf_Curl_Virus</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_0pIcbLI2TV"
   },
   "source": [
    "### Construcci√≥n de carpetas"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rJRQoQkoI2TW",
    "outputId": "02c46336-e464-423e-ab2f-36b131858f06",
    "ExecuteTime": {
     "end_time": "2025-04-22T02:07:37.072966Z",
     "start_time": "2025-04-22T02:07:37.070961Z"
    }
   },
   "source": [
    "# Guarda directorio del dataset dividido\n",
    "path = find_path(\"plantvillage-dataset\")\n",
    "SPLITTED_PATH = f\"{path}splitted/\" if path else None\n",
    "SPLITTED_PATH"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/juan/.cache/kagglehub/datasets/abdallahalidev/plantvillage-dataset/splitted/'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I1WFZkCgI2TW",
    "outputId": "ede75a4f-b92c-49b4-f9e9-a7a044aeb736",
    "ExecuteTime": {
     "end_time": "2025-04-22T02:07:37.124329Z",
     "start_time": "2025-04-22T02:07:37.119995Z"
    }
   },
   "source": [
    "splits = df_split['split'].value_counts().index.tolist()\n",
    "splits"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'test']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CqH2MgdxI2TW",
    "outputId": "94467777-9bf8-4c63-ca7d-857fc0ff7530",
    "ExecuteTime": {
     "end_time": "2025-04-22T02:07:39.048193Z",
     "start_time": "2025-04-22T02:07:37.167819Z"
    }
   },
   "source": [
    "verfication = True # Ejecuta el proceso de verificaci√≥n (punto 2)\n",
    "\n",
    "print(f\"Se inicia proceso de copiado del dataset‚Ä¶\")\n",
    "total_files = len(df_split) # Total de archivos del dataset\n",
    "print(f\" - Total de archivos en el dataset: {total_files}\")\n",
    "\n",
    "# Realiza el proceso de copiado de archivos para cada split\n",
    "succeeded_process = True\n",
    "for split in splits:\n",
    "    # Crea las rutas de origen y destino\n",
    "    # (Ejemplo: 'train', 'test', 'valid')\n",
    "    print(f\"\\n\\nIniciando proceso para '{split}' split ‚Ä¶\")\n",
    "    source_folder = f'{ROOT_DIR}/'\n",
    "    destination_folder = f'{SPLITTED_PATH}{split}/'\n",
    "    total_split = len(df_split[df_split['split'] == split]) # Total de archivos del split\n",
    "    if dataset_already_exists(destination_folder): # Verifica si el directorio existe y est√° vac√≠o\n",
    "        print(\"  ‚®Ø El directorio ya existe y contiene archivos, a petici√≥n del usuario se omite el proceso de copiado.\")\n",
    "        continue # Si el directorio ya existe, no se hace nada+\n",
    "    else:\n",
    "        print(f\"üîÑ Procesando split '{split.upper()}' ({(total_split/total_files*100):.2f}):\")\n",
    "        print(f\"  - Total de archivos a copiar: {total_split}\")\n",
    "    succeeded = False\n",
    "\n",
    "    try:\n",
    "        print(f\"1. Creando estructura de subcarpetas:\")\n",
    "        # 1. Crea la funci√≥n para ignorar espec√≠fica para el split a procesar\n",
    "        ignore_function = create_ignore_function(df_split, train_label=split, filename_col='filename')\n",
    "        print(f\"    ‚úî Funci√≥n de filtro creada para el split \")\n",
    "\n",
    "        # 2. Con copytree copia todo el \"√°rbol\" de directorios (careptas y subcarpetas)\n",
    "        # Fitrando con ignore_function todos aquellos archivos que no pertenecen al split deseado\n",
    "        print(f\"    ‚àû Copiando contenido del dataset (puede demorar hasta un minuto).\")\n",
    "        shutil.copytree(source_folder, destination_folder, ignore=ignore_function)\n",
    "        print(f\"    ‚úî Proceso de copiado del split finalizado.\")\n",
    "\n",
    "        if verfication:\n",
    "            # Verifica qu√© se haya copiado adecuadamente (opcional pero √∫til)\n",
    "            print(f\"2. Se inicia proceso de verificaci√≥n‚Ä¶\")\n",
    "            copied_files = []\n",
    "            for root, dirs, files_in_dest in os.walk(destination_folder):\n",
    "                for name in files_in_dest:\n",
    "                    copied_files.append(os.path.join(os.path.relpath(root, destination_folder), name).replace('\\\\', '/')) # Normalizar path\n",
    "                    #print(f\"  - {os.path.join(root, name)}\") # Debuggin\n",
    "            print(f\"    ‚úî Se crearon un total de {len(os.listdir(destination_folder))} carpetas (para las clases).\")\n",
    "            print(f\"    ‚úî Se copiaron un total de {len(copied_files)} archivos ({len(copied_files)/total_split*100:.2f}%)\")\n",
    "            # Agregar confirmaci√≥n de igualdad cantidad split == copiados\n",
    "            if len(copied_files) == total_split:\n",
    "                print(f\"‚úÖ Se complet√≥ satisfactoriamente el subproceso de copiado para el split.\\n\")\n",
    "                succeeded = True\n",
    "            else:\n",
    "                print(f\" ‚ùå Error: No se pudo copiar correctamente el split '{split.upper()}'\\n\")\n",
    "                succeeded = False\n",
    "        else:\n",
    "            succeeded = True # Si la verificaci√≥n est√° desactivada, se asume que el proceso fue exitoso\n",
    "\n",
    "    except FileExistsError:\n",
    "        print(f\"Error: La carpeta de destino '{destination_folder}' ya existe.\\n\")\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurri√≥ un error inesperado: {e}\\n\")\n",
    "\n",
    "    succeeded_process *= succeeded # Actualiza el estado del proceso\n",
    "    # (S√≥lo es 'True' si todos los splits se copian correctamente)\n",
    "\n",
    "if succeeded_process:\n",
    "    print(\"\\n\\nüåü El proceso de copiado del dataset ha finalizado con √©xito.\\n\")\n",
    "else:\n",
    "    print(\"\\n\\nüö´ No se pudo completar satisfactoriamente el proceso de copiado del dataset.\\nVerificar que se haya completado la eliminaci√≥n de las carpetas.\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se inicia proceso de copiado del dataset‚Ä¶\n",
      " - Total de archivos en el dataset: 54305\n",
      "\n",
      "\n",
      "Iniciando proceso para 'train' split ‚Ä¶\n",
      "‚õîÔ∏è La eliminaci√≥n del directorio ha sido denegada por el usuario:\n",
      "  > /home/juan/.cache/kagglehub/datasets/abdallahalidev/plantvillage-dataset/splitted/train/\n",
      "  ‚®Ø El directorio ya existe y contiene archivos, a petici√≥n del usuario se omite el proceso de copiado.\n",
      "\n",
      "\n",
      "Iniciando proceso para 'test' split ‚Ä¶\n",
      "‚õîÔ∏è La eliminaci√≥n del directorio ha sido denegada por el usuario:\n",
      "  > /home/juan/.cache/kagglehub/datasets/abdallahalidev/plantvillage-dataset/splitted/test/\n",
      "  ‚®Ø El directorio ya existe y contiene archivos, a petici√≥n del usuario se omite el proceso de copiado.\n",
      "\n",
      "\n",
      "üåü El proceso de copiado del dataset ha finalizado con √©xito.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bH1NHp2I2TW"
   },
   "source": [
    "----\n",
    "# Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdJ4L63nI2TW"
   },
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_J_0FGEDI2TW",
    "ExecuteTime": {
     "end_time": "2025-04-22T02:07:39.052286Z",
     "start_time": "2025-04-22T02:07:39.050766Z"
    }
   },
   "source": [
    "import tensorflow as tf"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GR1f2R6BI2TW",
    "ExecuteTime": {
     "end_time": "2025-04-22T02:07:39.097844Z",
     "start_time": "2025-04-22T02:07:39.095795Z"
    }
   },
   "source": [
    "# Data laoders setup\n",
    "def load_from_directory(data_folder):\n",
    "    \"\"\"\n",
    "    Carga un dataset de im√°genes desde un directorio espec√≠fico.\n",
    "\n",
    "    Args:\n",
    "        data_folder (str): Ruta al directorio que contiene las im√°genes.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: Dataset de TensorFlow con las im√°genes y etiquetas.\n",
    "    \"\"\"\n",
    "    dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        data_folder,  # Ruta al directorio de datos\n",
    "        labels=\"inferred\",  # Las etiquetas se infieren autom√°ticamente desde los nombres de las carpetas\n",
    "        label_mode=\"categorical\",  # Las etiquetas se codifican como categor√≠as (one-hot encoding)\n",
    "        class_names=None,  # Las clases se detectan autom√°ticamente\n",
    "        color_mode=\"rgb\",  # Las im√°genes se cargan en modo RGB\n",
    "        batch_size=32,  # Tama√±o de lote para el entrenamiento\n",
    "        image_size=(256, 256),  # Redimensiona las im√°genes a 128x128 p√≠xeles\n",
    "        shuffle=True,  # Mezcla las im√°genes aleatoriamente\n",
    "        seed=42,  # No se utiliza una semilla espec√≠fica para la aleatorizaci√≥n\n",
    "        validation_split=None,  # No se realiza una divisi√≥n de validaci√≥n aqu√≠\n",
    "        subset=None,  # No se especifica un subconjunto (train/validation)\n",
    "        interpolation=\"bilinear\",  # M√©todo de interpolaci√≥n para redimensionar las im√°genes\n",
    "        follow_links=False,  # No sigue enlaces simb√≥licos\n",
    "        crop_to_aspect_ratio=False  # No recorta las im√°genes para ajustar la relaci√≥n de aspecto\n",
    "    )\n",
    "\n",
    "    return dataset"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CX4meyPHI2TX",
    "outputId": "a702e80b-d527-41c9-f336-b868e8f9c35d",
    "ExecuteTime": {
     "end_time": "2025-04-22T02:07:39.142117Z",
     "start_time": "2025-04-22T02:07:39.140556Z"
    }
   },
   "source": [
    "# # Carga el dataset de im√°genes desde el directorio especificado\n",
    "# train_images = \"\"; test_images = \"\"; valid_images = \"\"\n",
    "# \n",
    "# print(\"Cargando datasets desde el directorio‚Ä¶\\n\")\n",
    "# for split in splits:\n",
    "#     data_folder = f'{SPLITTED_PATH}{split}/'\n",
    "# \n",
    "#     # Carga el conjunto de datos desde el directorio especificado\n",
    "#     # Utiliza la funci√≥n de TensorFlow para crear un dataset de im√°genes\n",
    "#     match split:\n",
    "#         case 'train':\n",
    "#             print(f\"Cargando dataset de entrenamiento desde:\\n > {data_folder}\")\n",
    "#             train_images = load_from_directory(data_folder)\n",
    "#         case 'test':\n",
    "#             print(f\"Cargando dataset de test desde:\\n > {data_folder}\")\n",
    "#             test_images = load_from_directory(data_folder)\n",
    "#         case 'valid':\n",
    "#             print(f\"Cargando dataset de validaci√≥n desde:\\n > {data_folder}\")\n",
    "#             valid_images = load_from_directory(data_folder)\n",
    "#         case _: # En caso de no coincidir con ninguno de los splits\n",
    "#             print(f\"‚ö†Ô∏è El split '{split}' no es reconocido. No se cargar√° ning√∫n dataset.\")\n",
    "#             continue # Salta al siguiente split\n",
    "#     print(f\"‚úÖ Dataset cargado exitosamente.\\n\")"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "egwVG5_yI2TX",
    "outputId": "68fef03c-8323-49ff-8601-e081f059f2ce",
    "ExecuteTime": {
     "end_time": "2025-04-22T02:07:39.186039Z",
     "start_time": "2025-04-22T02:07:39.184572Z"
    }
   },
   "source": [
    "# print(\"Resumen de los datasets cargados:\")\n",
    "# print(f\" - Total de im√°genes en el dataset de entrenamiento: {len(train_images)}\")\n",
    "# print(f\" - Total de im√°genes en el dataset de validaci√≥n: {len(valid_images)}\")\n",
    "# print(f\" - Total de im√°genes en el dataset de test: {len(test_images)}\")\n",
    "# print(f\"Total de im√°genes cargadas: {len(train_images) + len(test_images) + len(valid_images)}\\n\")"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x9Ck8YCbI2TX",
    "outputId": "23f66f0a-192c-43a5-d8c4-84139be9f111",
    "ExecuteTime": {
     "end_time": "2025-04-22T02:07:39.228637Z",
     "start_time": "2025-04-22T02:07:39.227365Z"
    }
   },
   "source": [
    "# print(f\"Clases detectadas:\")\n",
    "# [print(\" -\",clase) for clase in train_images.class_names]\n",
    "# print(f\"Total de clases: {len(train_images.class_names)}\")"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kviEs_CI2TX"
   },
   "source": [
    "----\n",
    "# Arquitectura del modelo"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 984
    },
    "id": "zGwMznw0I2TY",
    "outputId": "54f7a798-300a-44f9-fe4b-8c9a514748c8",
    "ExecuteTime": {
     "end_time": "2025-04-22T02:07:39.272238Z",
     "start_time": "2025-04-22T02:07:39.270534Z"
    }
   },
   "source": [
    "# from tensorflow.keras import datasets, layers, models\n",
    "# from tensorflow.keras import Input # For WARNING\n",
    "# \n",
    "# model = models.Sequential()\n",
    "# \n",
    "# # Bloque 1\n",
    "# model.add(Input(shape=(256, 256, 3))) # WARNING\n",
    "# model.add(layers.Rescaling(1./255)) # FALTABA MINMAX SCALLING\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Dropout(0.1))\n",
    "# \n",
    "# # Bloque 2\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Dropout(0.1))\n",
    "# \n",
    "# # Bloque 3\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Dropout(0.1))\n",
    "# \n",
    "# # Bloque 4\n",
    "# model.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Dropout(0.1))\n",
    "# \n",
    "# model.add(layers.Flatten())\n",
    "# \n",
    "# # Capa densa intermedia\n",
    "# model.add(layers.Dense(512, activation='relu'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Dropout(0.5))\n",
    "# model.add(layers.Dropout(0.25))\n",
    "# \n",
    "# # Capa de salida con 38 neuronas y softmax para multiclase\n",
    "# model.add(layers.Dense(38, activation='softmax'))\n",
    "# \n",
    "# model.summary()"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKzMCoyQI2TY"
   },
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5i7lvmd8K3x6",
    "ExecuteTime": {
     "end_time": "2025-04-22T02:07:39.315670Z",
     "start_time": "2025-04-22T02:07:39.314345Z"
    }
   },
   "source": [
    "import time"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "feTD6PRAI2TY",
    "outputId": "469125aa-5e8c-4786-d56d-a395deefefef",
    "ExecuteTime": {
     "end_time": "2025-04-22T02:07:39.358994Z",
     "start_time": "2025-04-22T02:07:39.357400Z"
    }
   },
   "source": [
    "# from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# start_time = time.perf_counter()\n",
    "\n",
    "# # Definimos el callback para guardar el mejor modelo seg√∫n la m√©trica elegida\n",
    "# checkpoint_callback = ModelCheckpoint(\n",
    "#     filepath='best_model.keras',   # Se generar√° una carpeta con este nombre\n",
    "#     monitor='val_loss',            # M√©trica a monitorear ('val_accuracy' es otra opci√≥n)\n",
    "#     save_best_only=True,           # Guarda solo si hay mejora\n",
    "#     save_weights_only=False,       # Guarda la arquitectura + pesos\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # Ajusta el modelo a tu criterio\n",
    "# with tf.device('/GPU:0'):\n",
    "#     model.compile(\n",
    "#         optimizer='adam',\n",
    "#         loss='categorical_crossentropy', #loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), <-- ERROR\n",
    "#         metrics=['accuracy']\n",
    "#     )\n",
    "\n",
    "# history = model.fit(\n",
    "#     train_images,\n",
    "#     validation_data=test_images,\n",
    "#     epochs=10,\n",
    "#     callbacks=[checkpoint_callback]  # Incorporamos el callback\n",
    "# )\n",
    "\n",
    "# end_time = time.perf_counter()\n",
    "# elapsed_time = end_time - start_time\n",
    "# print(f\"Tiempo de entrenamiento: {elapsed_time:.2f} segundos\")"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vMwOyEtQI2TY",
    "outputId": "7b499dca-5c52-4da0-e475-e8c0ff5f79e3",
    "ExecuteTime": {
     "end_time": "2025-04-22T02:07:39.401728Z",
     "start_time": "2025-04-22T02:07:39.400277Z"
    }
   },
   "source": "# print(f\"El entrenamiento tom√≥ {elapsed_time:.2f} segundos\")",
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQAOhynxI2TY"
   },
   "source": [
    "## Guardando resultados"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "it_ATa-HI2TY",
    "ExecuteTime": {
     "end_time": "2025-04-22T02:07:39.444596Z",
     "start_time": "2025-04-22T02:07:39.443111Z"
    }
   },
   "source": [
    "# #Recording History in json & pickle\n",
    "# import json\n",
    "# with open('training_hist.json','w') as f:\n",
    "#   json.dump(history.history,f)\n",
    "# \n",
    "# import pickle\n",
    "# with open('training_hist.pkl', 'wb') as f:\n",
    "#     pickle.dump(history.history, f)"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L4UbPwaXI2TY",
    "outputId": "0d43a419-f945-4f2a-f22f-bd973dee67fe",
    "ExecuteTime": {
     "end_time": "2025-04-22T02:07:39.487799Z",
     "start_time": "2025-04-22T02:07:39.486265Z"
    }
   },
   "source": [
    "# import os\n",
    "# \n",
    "# experiment = 'experimento_1' # Completar n√∫mero de experimento\n",
    "# files = ['best_model.keras','training_hist.json','training_hist.pkl']\n",
    "# destino=f\"/content/drive/MyDrive/CV2-PlantVillage/{experiment}/\"\n",
    "# \n",
    "# def check_folder(folder):\n",
    "#     if not os.path.exists(folder):\n",
    "#         os.makedirs(folder)\n",
    "#         print(f\"Folder '{folder}' created successfully.\")\n",
    "#     else:\n",
    "#         print(f\"Folder '{folder}' already exists.\")\n",
    "# \n",
    "# check_folder(destino)\n",
    "# \n",
    "# for file in files:\n",
    "#     try:\n",
    "#         origen=f\"/content/{file}\"\n",
    "#         !cp -r \"$origen\" \"$destino\"\n",
    "#     except:\n",
    "#         print(f\"Error al copiar el archivo '{file}'\")\n",
    "#     finally:\n",
    "#         print(f\"Archivo '{file}' copiado exitosamente.\")"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wajVEmO1I2TY"
   },
   "source": [
    "---\n",
    "# Gr√°ficos"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "kvY2nDDMI2TZ",
    "outputId": "566ea28f-625b-492a-bc28-7095b91607e6",
    "ExecuteTime": {
     "end_time": "2025-04-22T02:07:39.530485Z",
     "start_time": "2025-04-22T02:07:39.529201Z"
    }
   },
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# \n",
    "# epochs = [i for i in range(1,11)]\n",
    "# plt.plot(epochs,history.history['accuracy'],color='red',label='Training Accuracy')\n",
    "# plt.plot(epochs,history.history['val_accuracy'],color='blue',label='Validation Accuracy')\n",
    "# plt.xlabel('No. of Epochs')\n",
    "# plt.title('Visualization of Accuracy Result')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "t_YzKxnDI2TZ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "da16d33f-32ca-49d4-e5c6-44c27edbaddd",
    "ExecuteTime": {
     "end_time": "2025-04-22T02:07:39.573774Z",
     "start_time": "2025-04-22T02:07:39.572383Z"
    }
   },
   "source": [
    "# #Validation set Accuracy\n",
    "# model = tf.keras.models.load_model('best_model.keras')\n",
    "# val_loss, val_acc = model.evaluate(test_images)\n",
    "# print('Validation accuracy:', val_acc)"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install ray[tune] tensorflow\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K3XR3mNcuil-",
    "outputId": "06a7856d-9f5c-4f3b-8006-d6a7ed27749e",
    "ExecuteTime": {
     "end_time": "2025-04-22T02:07:40.921357Z",
     "start_time": "2025-04-22T02:07:39.618697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (2.19.0)\r\n",
      "Requirement already satisfied: ray[tune] in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (2.40.0)\r\n",
      "Requirement already satisfied: click>=7.0 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from ray[tune]) (8.1.7)\r\n",
      "Requirement already satisfied: filelock in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from ray[tune]) (3.13.1)\r\n",
      "Requirement already satisfied: jsonschema in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from ray[tune]) (4.19.2)\r\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from ray[tune]) (1.1.0)\r\n",
      "Requirement already satisfied: packaging in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from ray[tune]) (24.1)\r\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from ray[tune]) (5.29.1)\r\n",
      "Requirement already satisfied: pyyaml in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from ray[tune]) (6.0.1)\r\n",
      "Requirement already satisfied: aiosignal in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from ray[tune]) (1.3.1)\r\n",
      "Requirement already satisfied: frozenlist in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from ray[tune]) (1.5.0)\r\n",
      "Requirement already satisfied: requests in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from ray[tune]) (2.32.3)\r\n",
      "Requirement already satisfied: pandas in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from ray[tune]) (2.2.2)\r\n",
      "Requirement already satisfied: tensorboardX>=1.9 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from ray[tune]) (2.6.2.2)\r\n",
      "Requirement already satisfied: pyarrow>=9.0.0 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from ray[tune]) (18.1.0)\r\n",
      "Requirement already satisfied: fsspec in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from ray[tune]) (2024.6.1)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from tensorflow) (2.1.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from tensorflow) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from tensorflow) (25.2.10)\r\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from tensorflow) (0.6.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from tensorflow) (0.2.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from tensorflow) (18.1.1)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from tensorflow) (3.4.0)\r\n",
      "Requirement already satisfied: setuptools in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from tensorflow) (75.1.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from tensorflow) (1.16.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from tensorflow) (2.5.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from tensorflow) (4.12.2)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from tensorflow) (1.17.0)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from tensorflow) (1.68.1)\r\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from tensorflow) (2.19.0)\r\n",
      "Requirement already satisfied: keras>=3.5.0 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from tensorflow) (3.9.0)\r\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from tensorflow) (1.26.4)\r\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from tensorflow) (3.12.1)\r\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from tensorflow) (0.5.1)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\r\n",
      "Requirement already satisfied: rich in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\r\n",
      "Requirement already satisfied: namex in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\r\n",
      "Requirement already satisfied: optree in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.14.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from requests->ray[tune]) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from requests->ray[tune]) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from requests->ray[tune]) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from requests->ray[tune]) (2025.1.31)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.7)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (2.2.3)\r\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from jsonschema->ray[tune]) (23.1.0)\r\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from jsonschema->ray[tune]) (2023.7.1)\r\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from jsonschema->ray[tune]) (0.30.2)\r\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from jsonschema->ray[tune]) (0.10.6)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from pandas->ray[tune]) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from pandas->ray[tune]) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from pandas->ray[tune]) (2023.3)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.3)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\r\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "source": [
    "from ray.air import session\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "def train_fn(config):\n",
    "\n",
    "    def load_from_directory(data_folder):\n",
    "        return tf.keras.preprocessing.image_dataset_from_directory(\n",
    "            data_folder,\n",
    "            labels=\"inferred\",\n",
    "            label_mode=\"categorical\",\n",
    "            batch_size=32,\n",
    "            image_size=(256, 256),\n",
    "            shuffle=True,\n",
    "            seed=42\n",
    "        )\n",
    "\n",
    "  # Carga el dataset de im√°genes desde el directorio especificado\n",
    "    train_images = \"\"; test_images = \"\"; valid_images = \"\"\n",
    "\n",
    "    print(\"Cargando datasets desde el directorio‚Ä¶\\n\")\n",
    "    for split in splits:\n",
    "        data_folder = f'{SPLITTED_PATH}{split}/'\n",
    "\n",
    "        # Carga el conjunto de datos desde el directorio especificado\n",
    "        # Utiliza la funci√≥n de TensorFlow para crear un dataset de im√°genes\n",
    "        match split:\n",
    "            case 'train':\n",
    "                print(f\"Cargando dataset de entrenamiento desde:\\n > {data_folder}\")\n",
    "                train_images = load_from_directory(data_folder)\n",
    "            case 'test':\n",
    "                print(f\"Cargando dataset de test desde:\\n > {data_folder}\")\n",
    "                test_images = load_from_directory(data_folder)\n",
    "            case 'valid':\n",
    "                print(f\"Cargando dataset de validaci√≥n desde:\\n > {data_folder}\")\n",
    "                valid_images = load_from_directory(data_folder)\n",
    "            case _: # En caso de no coincidir con ninguno de los splits\n",
    "                print(f\"‚ö†Ô∏è El split '{split}' no es reconocido. No se cargar√° ning√∫n dataset.\")\n",
    "                continue # Salta al siguiente split\n",
    "        print(f\"‚úÖ Dataset cargado exitosamente.\\n\")\n",
    "\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(256, 256, 3)),\n",
    "        layers.Rescaling(1./255),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(config[\"dropout_rate\"]),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(38, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=config[\"learning_rate\"]),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    model.fit(train_images, validation_data=test_images, epochs=3, verbose=0)\n",
    "\n",
    "    val_loss, val_acc = model.evaluate(test_images, verbose=0)\n",
    "    session.report({\"loss\": val_loss, \"accuracy\": val_acc})\n",
    "\n"
   ],
   "metadata": {
    "id": "T75IHCnouywK",
    "ExecuteTime": {
     "end_time": "2025-04-22T03:05:26.479403Z",
     "start_time": "2025-04-22T03:05:26.475846Z"
    }
   },
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "source": [
    "from ray import tune\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "def create_model(filters1=32, filters2=64, filters3=128, filters4=256,\n",
    "                 dropout_rate=0.1, dense_units=512, learning_rate=1e-3):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Bloque 1\n",
    "    model.add(Input(shape=(256, 256, 3)))\n",
    "    model.add(layers.Rescaling(1./255))\n",
    "    model.add(layers.Conv2D(filters1, (3, 3), activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "\n",
    "    # Bloque 2\n",
    "    model.add(layers.Conv2D(filters2, (3, 3), activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "\n",
    "    # Bloque 3\n",
    "    model.add(layers.Conv2D(filters3, (3, 3), activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "\n",
    "    # Bloque 4\n",
    "    model.add(layers.Conv2D(filters4, (3, 3), activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Capa densa intermedia\n",
    "    model.add(layers.Dense(dense_units, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "\n",
    "    # Capa de salida con 38 neuronas y softmax para multiclase\n",
    "    model.add(layers.Dense(38, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n"
   ],
   "metadata": {
    "id": "oXRIjzTsummk",
    "ExecuteTime": {
     "end_time": "2025-04-22T03:05:29.247924Z",
     "start_time": "2025-04-22T03:05:29.244598Z"
    }
   },
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "source": [
    "search_space = {\n",
    "    \"filters1\": tune.choice([32]),\n",
    "    \"filters2\": tune.choice([64]),\n",
    "    \"filters3\": tune.choice([128]),\n",
    "    \"filters4\": tune.choice([256]),\n",
    "    \"dropout_rate\": tune.uniform(0.05, 0.15),\n",
    "    \"dense_units\": tune.choice([512]),\n",
    "    \"learning_rate\": tune.loguniform(1e-6, 5e-4)\n",
    "}\n"
   ],
   "metadata": {
    "id": "qNFYl9N2u2VK",
    "ExecuteTime": {
     "end_time": "2025-04-22T03:30:11.319974Z",
     "start_time": "2025-04-22T03:30:11.317963Z"
    }
   },
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "source": [
    "import ray\n",
    "\n",
    "from ray.tune import run\n",
    "\n",
    "\n",
    "\n",
    "analysis = run(\n",
    "    train_fn,\n",
    "    config=search_space,\n",
    "    num_samples=10,  # Number of trials (i.e., different sets of hyperparameters)\n",
    "    resources_per_trial = {\"gpu\": 1,\"cpu\": 16},  # Allocating 2 CPUs and 1 GPU per trial\n",
    "    max_concurrent_trials=1,\n",
    "    verbose=1,\n",
    "    metric=\"loss\",   # üëà the name of the metric you reported\n",
    "    mode=\"min\"       # üëà \"min\" for minimizing loss, \"max\" for accuracy etc.\n",
    ")\n",
    "\n",
    "print(\"Best hyperparameters found were: \", analysis.best_config)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pVQOyCMeu6Jx",
    "outputId": "457f0878-4107-4e1f-f558-54938bee595c"
   },
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-04-22 01:23:40</td></tr>\n",
       "<tr><td>Running for: </td><td>00:53:27.61        </td></tr>\n",
       "<tr><td>Memory:      </td><td>9.7/14.8 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 0/32 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name          </th><th>status    </th><th>loc                    </th><th style=\"text-align: right;\">  dense_units</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  filters1</th><th style=\"text-align: right;\">  filters2</th><th style=\"text-align: right;\">  filters3</th><th style=\"text-align: right;\">  filters4</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  accuracy</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_fn_19d2d_00000</td><td>TERMINATED</td><td>192.168.100.111:1379426</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">     0.142099 </td><td style=\"text-align: right;\">        32</td><td style=\"text-align: right;\">        64</td><td style=\"text-align: right;\">       128</td><td style=\"text-align: right;\">       256</td><td style=\"text-align: right;\">    2.69557e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         329.585</td><td style=\"text-align: right;\">0.64086 </td><td style=\"text-align: right;\">  0.820643</td></tr>\n",
       "<tr><td>train_fn_19d2d_00001</td><td>TERMINATED</td><td>192.168.100.111:1380611</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">     0.0698269</td><td style=\"text-align: right;\">        32</td><td style=\"text-align: right;\">        64</td><td style=\"text-align: right;\">       128</td><td style=\"text-align: right;\">       256</td><td style=\"text-align: right;\">    0.000218329</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         315.354</td><td style=\"text-align: right;\">0.677385</td><td style=\"text-align: right;\">  0.819538</td></tr>\n",
       "<tr><td>train_fn_19d2d_00002</td><td>TERMINATED</td><td>192.168.100.111:1381899</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">     0.0807287</td><td style=\"text-align: right;\">        32</td><td style=\"text-align: right;\">        64</td><td style=\"text-align: right;\">       128</td><td style=\"text-align: right;\">       256</td><td style=\"text-align: right;\">    1.71275e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         314.787</td><td style=\"text-align: right;\">1.44194 </td><td style=\"text-align: right;\">  0.639168</td></tr>\n",
       "<tr><td>train_fn_19d2d_00003</td><td>TERMINATED</td><td>192.168.100.111:1383064</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">     0.0992878</td><td style=\"text-align: right;\">        32</td><td style=\"text-align: right;\">        64</td><td style=\"text-align: right;\">       128</td><td style=\"text-align: right;\">       256</td><td style=\"text-align: right;\">    0.000139859</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         316.015</td><td style=\"text-align: right;\">0.550488</td><td style=\"text-align: right;\">  0.838321</td></tr>\n",
       "<tr><td>train_fn_19d2d_00004</td><td>TERMINATED</td><td>192.168.100.111:1384238</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">     0.0643009</td><td style=\"text-align: right;\">        32</td><td style=\"text-align: right;\">        64</td><td style=\"text-align: right;\">       128</td><td style=\"text-align: right;\">       256</td><td style=\"text-align: right;\">    1.41564e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         319.05 </td><td style=\"text-align: right;\">1.50273 </td><td style=\"text-align: right;\">  0.629224</td></tr>\n",
       "<tr><td>train_fn_19d2d_00005</td><td>TERMINATED</td><td>192.168.100.111:1385414</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">     0.0709845</td><td style=\"text-align: right;\">        32</td><td style=\"text-align: right;\">        64</td><td style=\"text-align: right;\">       128</td><td style=\"text-align: right;\">       256</td><td style=\"text-align: right;\">    3.68248e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         315.873</td><td style=\"text-align: right;\">0.584358</td><td style=\"text-align: right;\">  0.831507</td></tr>\n",
       "<tr><td>train_fn_19d2d_00006</td><td>TERMINATED</td><td>192.168.100.111:1386590</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">     0.0783668</td><td style=\"text-align: right;\">        32</td><td style=\"text-align: right;\">        64</td><td style=\"text-align: right;\">       128</td><td style=\"text-align: right;\">       256</td><td style=\"text-align: right;\">    0.000190948</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         315.611</td><td style=\"text-align: right;\">0.625622</td><td style=\"text-align: right;\">  0.827824</td></tr>\n",
       "<tr><td>train_fn_19d2d_00007</td><td>TERMINATED</td><td>192.168.100.111:1387749</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">     0.0659855</td><td style=\"text-align: right;\">        32</td><td style=\"text-align: right;\">        64</td><td style=\"text-align: right;\">       128</td><td style=\"text-align: right;\">       256</td><td style=\"text-align: right;\">    3.04429e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         316.74 </td><td style=\"text-align: right;\">1.27155 </td><td style=\"text-align: right;\">  0.66992 </td></tr>\n",
       "<tr><td>train_fn_19d2d_00008</td><td>TERMINATED</td><td>192.168.100.111:1388908</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">     0.050925 </td><td style=\"text-align: right;\">        32</td><td style=\"text-align: right;\">        64</td><td style=\"text-align: right;\">       128</td><td style=\"text-align: right;\">       256</td><td style=\"text-align: right;\">    3.91988e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         319.328</td><td style=\"text-align: right;\">1.14191 </td><td style=\"text-align: right;\">  0.703895</td></tr>\n",
       "<tr><td>train_fn_19d2d_00009</td><td>TERMINATED</td><td>192.168.100.111:1390088</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">     0.135315 </td><td style=\"text-align: right;\">        32</td><td style=\"text-align: right;\">        64</td><td style=\"text-align: right;\">       128</td><td style=\"text-align: right;\">       256</td><td style=\"text-align: right;\">    3.53835e-06</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         316.258</td><td style=\"text-align: right;\">1.23111 </td><td style=\"text-align: right;\">  0.679495</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(pid=1379426)\u001B[0m 2025-04-22 00:30:14.052309: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001B[36m(pid=1379426)\u001B[0m 2025-04-22 00:30:14.067142: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001B[36m(pid=1379426)\u001B[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001B[36m(pid=1379426)\u001B[0m E0000 00:00:1745292614.077391 1379426 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001B[36m(pid=1379426)\u001B[0m E0000 00:00:1745292614.080527 1379426 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001B[36m(pid=1379426)\u001B[0m W0000 00:00:1745292614.089675 1379426 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1379426)\u001B[0m W0000 00:00:1745292614.089686 1379426 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1379426)\u001B[0m W0000 00:00:1745292614.089687 1379426 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1379426)\u001B[0m W0000 00:00:1745292614.089689 1379426 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1379426)\u001B[0m 2025-04-22 00:30:14.092306: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001B[36m(pid=1379426)\u001B[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1379426)\u001B[0m Cargando datasets desde el directorio‚Ä¶\n",
      "\u001B[36m(train_fn pid=1379426)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1379426)\u001B[0m Cargando dataset de entrenamiento desde:\n",
      "\u001B[36m(train_fn pid=1379426)\u001B[0m  > /home/juan/.cache/kagglehub/datasets/abdallahalidev/plantvillage-dataset/splitted/train/\n",
      "\u001B[36m(train_fn pid=1379426)\u001B[0m Found 109718 files belonging to 38 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1379426)\u001B[0m I0000 00:00:1745292617.096159 1379518 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6103 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1379426)\u001B[0m ‚úÖ Dataset cargado exitosamente.\n",
      "\u001B[36m(train_fn pid=1379426)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1379426)\u001B[0m Cargando dataset de test desde:\n",
      "\u001B[36m(train_fn pid=1379426)\u001B[0m  > /home/juan/.cache/kagglehub/datasets/abdallahalidev/plantvillage-dataset/splitted/test/\n",
      "\u001B[36m(train_fn pid=1379426)\u001B[0m Found 10861 files belonging to 38 classes.\n",
      "\u001B[36m(train_fn pid=1379426)\u001B[0m ‚úÖ Dataset cargado exitosamente.\n",
      "\u001B[36m(train_fn pid=1379426)\u001B[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1379426)\u001B[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001B[36m(train_fn pid=1379426)\u001B[0m I0000 00:00:1745292618.573728 1379625 service.cc:152] XLA service 0x789a94018310 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001B[36m(train_fn pid=1379426)\u001B[0m I0000 00:00:1745292618.573852 1379625 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Laptop GPU, Compute Capability 8.9\n",
      "\u001B[36m(train_fn pid=1379426)\u001B[0m 2025-04-22 00:30:18.589962: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001B[36m(train_fn pid=1379426)\u001B[0m I0000 00:00:1745292618.643378 1379625 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "\u001B[36m(train_fn pid=1379426)\u001B[0m 2025-04-22 00:30:19.224135: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_661', 84 bytes spill stores, 84 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1379426)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1379426)\u001B[0m 2025-04-22 00:30:19.377496: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_716', 28 bytes spill stores, 28 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1379426)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1379426)\u001B[0m I0000 00:00:1745292621.099882 1379625 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001B[36m(train_fn pid=1379426)\u001B[0m 2025-04-22 00:32:05.283318: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_661', 64 bytes spill stores, 64 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1379426)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1379426)\u001B[0m 2025-04-22 00:32:05.473139: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_716', 48 bytes spill stores, 48 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1379426)\u001B[0m \n",
      "\u001B[36m(pid=1380611)\u001B[0m 2025-04-22 00:35:46.657821: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001B[36m(pid=1380611)\u001B[0m 2025-04-22 00:35:46.668736: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001B[36m(pid=1380611)\u001B[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001B[36m(pid=1380611)\u001B[0m E0000 00:00:1745292946.679548 1380611 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001B[36m(pid=1380611)\u001B[0m E0000 00:00:1745292946.682955 1380611 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001B[36m(pid=1380611)\u001B[0m W0000 00:00:1745292946.692494 1380611 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1380611)\u001B[0m W0000 00:00:1745292946.692506 1380611 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1380611)\u001B[0m W0000 00:00:1745292946.692508 1380611 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1380611)\u001B[0m W0000 00:00:1745292946.692510 1380611 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1380611)\u001B[0m 2025-04-22 00:35:46.695342: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001B[36m(pid=1380611)\u001B[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1380611)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1380611)\u001B[0m Cargando datasets desde el directorio‚Ä¶\n",
      "\u001B[36m(train_fn pid=1380611)\u001B[0m Cargando dataset de entrenamiento desde:\n",
      "\u001B[36m(train_fn pid=1380611)\u001B[0m  > /home/juan/.cache/kagglehub/datasets/abdallahalidev/plantvillage-dataset/splitted/train/\n",
      "\u001B[36m(train_fn pid=1380611)\u001B[0m Found 109718 files belonging to 38 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1380611)\u001B[0m I0000 00:00:1745292950.273066 1380697 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6133 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1380611)\u001B[0m ‚úÖ Dataset cargado exitosamente.\n",
      "\u001B[36m(train_fn pid=1380611)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1380611)\u001B[0m Cargando dataset de test desde:\n",
      "\u001B[36m(train_fn pid=1380611)\u001B[0m  > /home/juan/.cache/kagglehub/datasets/abdallahalidev/plantvillage-dataset/splitted/test/\n",
      "\u001B[36m(train_fn pid=1380611)\u001B[0m Found 10861 files belonging to 38 classes.\n",
      "\u001B[36m(train_fn pid=1380611)\u001B[0m ‚úÖ Dataset cargado exitosamente.\n",
      "\u001B[36m(train_fn pid=1380611)\u001B[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1380611)\u001B[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001B[36m(train_fn pid=1380611)\u001B[0m I0000 00:00:1745292951.975525 1380806 service.cc:152] XLA service 0x7d5410018620 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001B[36m(train_fn pid=1380611)\u001B[0m I0000 00:00:1745292951.975721 1380806 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Laptop GPU, Compute Capability 8.9\n",
      "\u001B[36m(train_fn pid=1380611)\u001B[0m 2025-04-22 00:35:51.993133: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001B[36m(train_fn pid=1380611)\u001B[0m I0000 00:00:1745292952.049727 1380806 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "\u001B[36m(train_fn pid=1380611)\u001B[0m 2025-04-22 00:35:53.005626: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_716', 28 bytes spill stores, 28 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1380611)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1380611)\u001B[0m 2025-04-22 00:35:53.011651: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_661', 84 bytes spill stores, 84 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1380611)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1380611)\u001B[0m I0000 00:00:1745292954.667128 1380806 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001B[36m(train_fn pid=1380611)\u001B[0m 2025-04-22 00:37:34.363541: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_661', 64 bytes spill stores, 64 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1380611)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1380611)\u001B[0m 2025-04-22 00:37:34.592772: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_716', 48 bytes spill stores, 48 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1380611)\u001B[0m \n",
      "\u001B[36m(pid=1381899)\u001B[0m 2025-04-22 00:41:04.875768: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001B[36m(pid=1381899)\u001B[0m 2025-04-22 00:41:04.887368: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001B[36m(pid=1381899)\u001B[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001B[36m(pid=1381899)\u001B[0m E0000 00:00:1745293264.898154 1381899 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001B[36m(pid=1381899)\u001B[0m E0000 00:00:1745293264.901395 1381899 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001B[36m(pid=1381899)\u001B[0m W0000 00:00:1745293264.911032 1381899 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1381899)\u001B[0m W0000 00:00:1745293264.911044 1381899 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1381899)\u001B[0m W0000 00:00:1745293264.911046 1381899 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1381899)\u001B[0m W0000 00:00:1745293264.911047 1381899 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1381899)\u001B[0m 2025-04-22 00:41:04.913959: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001B[36m(pid=1381899)\u001B[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1381899)\u001B[0m Cargando datasets desde el directorio‚Ä¶\n",
      "\u001B[36m(train_fn pid=1381899)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1381899)\u001B[0m Cargando dataset de entrenamiento desde:\n",
      "\u001B[36m(train_fn pid=1381899)\u001B[0m  > /home/juan/.cache/kagglehub/datasets/abdallahalidev/plantvillage-dataset/splitted/train/\n",
      "\u001B[36m(train_fn pid=1381899)\u001B[0m Found 109718 files belonging to 38 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1381899)\u001B[0m I0000 00:00:1745293268.691658 1381985 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6133 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1381899)\u001B[0m ‚úÖ Dataset cargado exitosamente.\n",
      "\u001B[36m(train_fn pid=1381899)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1381899)\u001B[0m Cargando dataset de test desde:\n",
      "\u001B[36m(train_fn pid=1381899)\u001B[0m  > /home/juan/.cache/kagglehub/datasets/abdallahalidev/plantvillage-dataset/splitted/test/\n",
      "\u001B[36m(train_fn pid=1381899)\u001B[0m Found 10861 files belonging to 38 classes.\n",
      "\u001B[36m(train_fn pid=1381899)\u001B[0m ‚úÖ Dataset cargado exitosamente.\n",
      "\u001B[36m(train_fn pid=1381899)\u001B[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1381899)\u001B[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001B[36m(train_fn pid=1381899)\u001B[0m I0000 00:00:1745293270.296493 1382090 service.cc:152] XLA service 0x713af00040f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001B[36m(train_fn pid=1381899)\u001B[0m I0000 00:00:1745293270.296599 1382090 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Laptop GPU, Compute Capability 8.9\n",
      "\u001B[36m(train_fn pid=1381899)\u001B[0m 2025-04-22 00:41:10.313592: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001B[36m(train_fn pid=1381899)\u001B[0m I0000 00:00:1745293270.372832 1382090 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "\u001B[36m(train_fn pid=1381899)\u001B[0m 2025-04-22 00:41:11.272015: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_661', 84 bytes spill stores, 84 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1381899)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1381899)\u001B[0m 2025-04-22 00:41:11.515070: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_716', 28 bytes spill stores, 28 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1381899)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1381899)\u001B[0m I0000 00:00:1745293273.160578 1382090 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001B[36m(train_fn pid=1381899)\u001B[0m 2025-04-22 00:42:52.417819: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_661', 64 bytes spill stores, 64 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1381899)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1381899)\u001B[0m 2025-04-22 00:42:52.704312: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_716', 48 bytes spill stores, 48 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1381899)\u001B[0m \n",
      "\u001B[36m(pid=1383064)\u001B[0m 2025-04-22 00:46:22.659081: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001B[36m(pid=1383064)\u001B[0m 2025-04-22 00:46:22.669910: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001B[36m(pid=1383064)\u001B[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001B[36m(pid=1383064)\u001B[0m E0000 00:00:1745293582.680794 1383064 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001B[36m(pid=1383064)\u001B[0m E0000 00:00:1745293582.684109 1383064 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001B[36m(pid=1383064)\u001B[0m W0000 00:00:1745293582.693987 1383064 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1383064)\u001B[0m W0000 00:00:1745293582.693998 1383064 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1383064)\u001B[0m W0000 00:00:1745293582.694000 1383064 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1383064)\u001B[0m W0000 00:00:1745293582.694002 1383064 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1383064)\u001B[0m 2025-04-22 00:46:22.696954: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001B[36m(pid=1383064)\u001B[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1383064)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1383064)\u001B[0m Cargando datasets desde el directorio‚Ä¶\n",
      "\u001B[36m(train_fn pid=1383064)\u001B[0m Cargando dataset de entrenamiento desde:\n",
      "\u001B[36m(train_fn pid=1383064)\u001B[0m  > /home/juan/.cache/kagglehub/datasets/abdallahalidev/plantvillage-dataset/splitted/train/\n",
      "\u001B[36m(train_fn pid=1383064)\u001B[0m Found 109718 files belonging to 38 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1383064)\u001B[0m I0000 00:00:1745293586.071678 1383149 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6133 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1383064)\u001B[0m ‚úÖ Dataset cargado exitosamente.\n",
      "\u001B[36m(train_fn pid=1383064)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1383064)\u001B[0m Cargando dataset de test desde:\n",
      "\u001B[36m(train_fn pid=1383064)\u001B[0m  > /home/juan/.cache/kagglehub/datasets/abdallahalidev/plantvillage-dataset/splitted/test/\n",
      "\u001B[36m(train_fn pid=1383064)\u001B[0m Found 10861 files belonging to 38 classes.\n",
      "\u001B[36m(train_fn pid=1383064)\u001B[0m ‚úÖ Dataset cargado exitosamente.\n",
      "\u001B[36m(train_fn pid=1383064)\u001B[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1383064)\u001B[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001B[36m(train_fn pid=1383064)\u001B[0m I0000 00:00:1745293587.685947 1383254 service.cc:152] XLA service 0x7043f4002960 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001B[36m(train_fn pid=1383064)\u001B[0m I0000 00:00:1745293587.686243 1383254 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Laptop GPU, Compute Capability 8.9\n",
      "\u001B[36m(train_fn pid=1383064)\u001B[0m 2025-04-22 00:46:27.703688: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001B[36m(train_fn pid=1383064)\u001B[0m I0000 00:00:1745293587.762352 1383254 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "\u001B[36m(train_fn pid=1383064)\u001B[0m 2025-04-22 00:46:28.797758: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_661', 84 bytes spill stores, 84 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1383064)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1383064)\u001B[0m 2025-04-22 00:46:28.902279: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_716', 28 bytes spill stores, 28 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1383064)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1383064)\u001B[0m I0000 00:00:1745293590.427777 1383254 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001B[36m(train_fn pid=1383064)\u001B[0m 2025-04-22 00:48:10.493225: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_661', 64 bytes spill stores, 64 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1383064)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1383064)\u001B[0m 2025-04-22 00:48:10.653895: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_716', 48 bytes spill stores, 48 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1383064)\u001B[0m \n",
      "\u001B[36m(pid=1384238)\u001B[0m 2025-04-22 00:51:41.680442: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001B[36m(pid=1384238)\u001B[0m 2025-04-22 00:51:41.692282: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001B[36m(pid=1384238)\u001B[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001B[36m(pid=1384238)\u001B[0m E0000 00:00:1745293901.704468 1384238 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001B[36m(pid=1384238)\u001B[0m E0000 00:00:1745293901.708196 1384238 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001B[36m(pid=1384238)\u001B[0m W0000 00:00:1745293901.718281 1384238 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1384238)\u001B[0m W0000 00:00:1745293901.718300 1384238 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1384238)\u001B[0m W0000 00:00:1745293901.718303 1384238 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1384238)\u001B[0m W0000 00:00:1745293901.718305 1384238 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1384238)\u001B[0m 2025-04-22 00:51:41.721211: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001B[36m(pid=1384238)\u001B[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1384238)\u001B[0m Cargando datasets desde el directorio‚Ä¶\n",
      "\u001B[36m(train_fn pid=1384238)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1384238)\u001B[0m Cargando dataset de entrenamiento desde:\n",
      "\u001B[36m(train_fn pid=1384238)\u001B[0m  > /home/juan/.cache/kagglehub/datasets/abdallahalidev/plantvillage-dataset/splitted/train/\n",
      "\u001B[36m(train_fn pid=1384238)\u001B[0m Found 109718 files belonging to 38 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1384238)\u001B[0m I0000 00:00:1745293904.827694 1384325 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6133 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1384238)\u001B[0m ‚úÖ Dataset cargado exitosamente.\n",
      "\u001B[36m(train_fn pid=1384238)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1384238)\u001B[0m Cargando dataset de test desde:\n",
      "\u001B[36m(train_fn pid=1384238)\u001B[0m  > /home/juan/.cache/kagglehub/datasets/abdallahalidev/plantvillage-dataset/splitted/test/\n",
      "\u001B[36m(train_fn pid=1384238)\u001B[0m Found 10861 files belonging to 38 classes.\n",
      "\u001B[36m(train_fn pid=1384238)\u001B[0m ‚úÖ Dataset cargado exitosamente.\n",
      "\u001B[36m(train_fn pid=1384238)\u001B[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1384238)\u001B[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001B[36m(train_fn pid=1384238)\u001B[0m I0000 00:00:1745293906.555790 1384430 service.cc:152] XLA service 0x7c9d28005c70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001B[36m(train_fn pid=1384238)\u001B[0m I0000 00:00:1745293906.555882 1384430 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Laptop GPU, Compute Capability 8.9\n",
      "\u001B[36m(train_fn pid=1384238)\u001B[0m 2025-04-22 00:51:46.572748: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001B[36m(train_fn pid=1384238)\u001B[0m I0000 00:00:1745293906.630878 1384430 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "\u001B[36m(train_fn pid=1384238)\u001B[0m 2025-04-22 00:51:47.580185: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_661', 84 bytes spill stores, 84 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1384238)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1384238)\u001B[0m 2025-04-22 00:51:47.783650: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_716', 28 bytes spill stores, 28 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1384238)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1384238)\u001B[0m I0000 00:00:1745293909.351891 1384430 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001B[36m(train_fn pid=1384238)\u001B[0m 2025-04-22 00:53:29.756784: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_661', 64 bytes spill stores, 64 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1384238)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1384238)\u001B[0m 2025-04-22 00:53:30.182048: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_716', 48 bytes spill stores, 48 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1384238)\u001B[0m \n",
      "\u001B[36m(pid=1385414)\u001B[0m 2025-04-22 00:57:03.755230: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001B[36m(pid=1385414)\u001B[0m 2025-04-22 00:57:03.766231: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001B[36m(pid=1385414)\u001B[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001B[36m(pid=1385414)\u001B[0m E0000 00:00:1745294223.777461 1385414 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001B[36m(pid=1385414)\u001B[0m E0000 00:00:1745294223.780913 1385414 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001B[36m(pid=1385414)\u001B[0m W0000 00:00:1745294223.790876 1385414 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1385414)\u001B[0m W0000 00:00:1745294223.790887 1385414 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1385414)\u001B[0m W0000 00:00:1745294223.790889 1385414 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1385414)\u001B[0m W0000 00:00:1745294223.790891 1385414 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1385414)\u001B[0m 2025-04-22 00:57:03.793895: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001B[36m(pid=1385414)\u001B[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1385414)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1385414)\u001B[0m Cargando datasets desde el directorio‚Ä¶\n",
      "\u001B[36m(train_fn pid=1385414)\u001B[0m Cargando dataset de entrenamiento desde:\n",
      "\u001B[36m(train_fn pid=1385414)\u001B[0m  > /home/juan/.cache/kagglehub/datasets/abdallahalidev/plantvillage-dataset/splitted/train/\n",
      "\u001B[36m(train_fn pid=1385414)\u001B[0m Found 109718 files belonging to 38 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1385414)\u001B[0m I0000 00:00:1745294226.992019 1385500 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6133 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1385414)\u001B[0m ‚úÖ Dataset cargado exitosamente.\n",
      "\u001B[36m(train_fn pid=1385414)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1385414)\u001B[0m Cargando dataset de test desde:\n",
      "\u001B[36m(train_fn pid=1385414)\u001B[0m  > /home/juan/.cache/kagglehub/datasets/abdallahalidev/plantvillage-dataset/splitted/test/\n",
      "\u001B[36m(train_fn pid=1385414)\u001B[0m Found 10861 files belonging to 38 classes.\n",
      "\u001B[36m(train_fn pid=1385414)\u001B[0m ‚úÖ Dataset cargado exitosamente.\n",
      "\u001B[36m(train_fn pid=1385414)\u001B[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1385414)\u001B[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001B[36m(train_fn pid=1385414)\u001B[0m I0000 00:00:1745294228.601355 1385604 service.cc:152] XLA service 0x701d94004710 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001B[36m(train_fn pid=1385414)\u001B[0m I0000 00:00:1745294228.601516 1385604 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Laptop GPU, Compute Capability 8.9\n",
      "\u001B[36m(train_fn pid=1385414)\u001B[0m 2025-04-22 00:57:08.619262: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001B[36m(train_fn pid=1385414)\u001B[0m I0000 00:00:1745294228.678226 1385604 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "\u001B[36m(train_fn pid=1385414)\u001B[0m 2025-04-22 00:57:09.685665: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_661', 84 bytes spill stores, 84 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1385414)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1385414)\u001B[0m 2025-04-22 00:57:09.856898: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_716', 28 bytes spill stores, 28 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1385414)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1385414)\u001B[0m I0000 00:00:1745294231.431009 1385604 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001B[36m(train_fn pid=1385414)\u001B[0m 2025-04-22 00:58:51.355006: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_661', 64 bytes spill stores, 64 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1385414)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1385414)\u001B[0m 2025-04-22 00:58:51.500778: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_716', 48 bytes spill stores, 48 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1385414)\u001B[0m \n",
      "\u001B[36m(pid=1386590)\u001B[0m 2025-04-22 01:02:22.915610: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001B[36m(pid=1386590)\u001B[0m 2025-04-22 01:02:22.926343: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001B[36m(pid=1386590)\u001B[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001B[36m(pid=1386590)\u001B[0m E0000 00:00:1745294542.937199 1386590 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001B[36m(pid=1386590)\u001B[0m E0000 00:00:1745294542.940537 1386590 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001B[36m(pid=1386590)\u001B[0m W0000 00:00:1745294542.950483 1386590 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1386590)\u001B[0m W0000 00:00:1745294542.950494 1386590 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1386590)\u001B[0m W0000 00:00:1745294542.950495 1386590 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1386590)\u001B[0m W0000 00:00:1745294542.950497 1386590 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1386590)\u001B[0m 2025-04-22 01:02:22.953859: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001B[36m(pid=1386590)\u001B[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1386590)\u001B[0m Cargando datasets desde el directorio‚Ä¶\n",
      "\u001B[36m(train_fn pid=1386590)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1386590)\u001B[0m Cargando dataset de entrenamiento desde:\n",
      "\u001B[36m(train_fn pid=1386590)\u001B[0m  > /home/juan/.cache/kagglehub/datasets/abdallahalidev/plantvillage-dataset/splitted/train/\n",
      "\u001B[36m(train_fn pid=1386590)\u001B[0m Found 109718 files belonging to 38 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1386590)\u001B[0m I0000 00:00:1745294546.333564 1386677 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6133 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1386590)\u001B[0m ‚úÖ Dataset cargado exitosamente.\n",
      "\u001B[36m(train_fn pid=1386590)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1386590)\u001B[0m Cargando dataset de test desde:\n",
      "\u001B[36m(train_fn pid=1386590)\u001B[0m  > /home/juan/.cache/kagglehub/datasets/abdallahalidev/plantvillage-dataset/splitted/test/\n",
      "\u001B[36m(train_fn pid=1386590)\u001B[0m Found 10861 files belonging to 38 classes.\n",
      "\u001B[36m(train_fn pid=1386590)\u001B[0m ‚úÖ Dataset cargado exitosamente.\n",
      "\u001B[36m(train_fn pid=1386590)\u001B[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1386590)\u001B[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001B[36m(train_fn pid=1386590)\u001B[0m I0000 00:00:1745294547.962623 1386784 service.cc:152] XLA service 0x7c9470016a20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001B[36m(train_fn pid=1386590)\u001B[0m I0000 00:00:1745294547.962726 1386784 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Laptop GPU, Compute Capability 8.9\n",
      "\u001B[36m(train_fn pid=1386590)\u001B[0m 2025-04-22 01:02:27.979128: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001B[36m(train_fn pid=1386590)\u001B[0m I0000 00:00:1745294548.036492 1386784 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "\u001B[36m(train_fn pid=1386590)\u001B[0m 2025-04-22 01:02:28.642625: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_661', 84 bytes spill stores, 84 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1386590)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1386590)\u001B[0m 2025-04-22 01:02:29.118558: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_716', 28 bytes spill stores, 28 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1386590)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1386590)\u001B[0m I0000 00:00:1745294550.782174 1386784 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001B[36m(train_fn pid=1386590)\u001B[0m 2025-04-22 01:04:10.537778: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_661', 64 bytes spill stores, 64 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1386590)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1386590)\u001B[0m 2025-04-22 01:04:10.692111: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_716', 48 bytes spill stores, 48 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1386590)\u001B[0m \n",
      "\u001B[36m(pid=1387749)\u001B[0m 2025-04-22 01:07:40.896923: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001B[36m(pid=1387749)\u001B[0m 2025-04-22 01:07:40.907413: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001B[36m(pid=1387749)\u001B[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001B[36m(pid=1387749)\u001B[0m E0000 00:00:1745294860.918070 1387749 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001B[36m(pid=1387749)\u001B[0m E0000 00:00:1745294860.921227 1387749 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001B[36m(pid=1387749)\u001B[0m W0000 00:00:1745294860.930641 1387749 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1387749)\u001B[0m W0000 00:00:1745294860.930653 1387749 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1387749)\u001B[0m W0000 00:00:1745294860.930655 1387749 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1387749)\u001B[0m W0000 00:00:1745294860.930656 1387749 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1387749)\u001B[0m 2025-04-22 01:07:40.933524: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001B[36m(pid=1387749)\u001B[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1387749)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1387749)\u001B[0m Cargando datasets desde el directorio‚Ä¶\n",
      "\u001B[36m(train_fn pid=1387749)\u001B[0m Cargando dataset de entrenamiento desde:\n",
      "\u001B[36m(train_fn pid=1387749)\u001B[0m  > /home/juan/.cache/kagglehub/datasets/abdallahalidev/plantvillage-dataset/splitted/train/\n",
      "\u001B[36m(train_fn pid=1387749)\u001B[0m Found 109718 files belonging to 38 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1387749)\u001B[0m I0000 00:00:1745294864.372191 1387835 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6133 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1387749)\u001B[0m ‚úÖ Dataset cargado exitosamente.\n",
      "\u001B[36m(train_fn pid=1387749)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1387749)\u001B[0m Cargando dataset de test desde:\n",
      "\u001B[36m(train_fn pid=1387749)\u001B[0m  > /home/juan/.cache/kagglehub/datasets/abdallahalidev/plantvillage-dataset/splitted/test/\n",
      "\u001B[36m(train_fn pid=1387749)\u001B[0m Found 10861 files belonging to 38 classes.\n",
      "\u001B[36m(train_fn pid=1387749)\u001B[0m ‚úÖ Dataset cargado exitosamente.\n",
      "\u001B[36m(train_fn pid=1387749)\u001B[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1387749)\u001B[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001B[36m(train_fn pid=1387749)\u001B[0m I0000 00:00:1745294865.968007 1387942 service.cc:152] XLA service 0x7f759c019bc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001B[36m(train_fn pid=1387749)\u001B[0m I0000 00:00:1745294865.968078 1387942 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Laptop GPU, Compute Capability 8.9\n",
      "\u001B[36m(train_fn pid=1387749)\u001B[0m 2025-04-22 01:07:45.983507: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001B[36m(train_fn pid=1387749)\u001B[0m I0000 00:00:1745294866.040491 1387942 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "\u001B[36m(train_fn pid=1387749)\u001B[0m 2025-04-22 01:07:46.671062: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_661', 84 bytes spill stores, 84 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1387749)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1387749)\u001B[0m 2025-04-22 01:07:47.204368: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_716', 28 bytes spill stores, 28 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1387749)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1387749)\u001B[0m I0000 00:00:1745294868.968256 1387942 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001B[36m(train_fn pid=1387749)\u001B[0m 2025-04-22 01:09:28.698420: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_661', 64 bytes spill stores, 64 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1387749)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1387749)\u001B[0m 2025-04-22 01:09:28.815728: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_716', 48 bytes spill stores, 48 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1387749)\u001B[0m \n",
      "\u001B[36m(pid=1388908)\u001B[0m 2025-04-22 01:13:00.982233: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001B[36m(pid=1388908)\u001B[0m 2025-04-22 01:13:00.993139: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001B[36m(pid=1388908)\u001B[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001B[36m(pid=1388908)\u001B[0m E0000 00:00:1745295181.004285 1388908 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001B[36m(pid=1388908)\u001B[0m E0000 00:00:1745295181.007663 1388908 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001B[36m(pid=1388908)\u001B[0m W0000 00:00:1745295181.017252 1388908 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1388908)\u001B[0m W0000 00:00:1745295181.017264 1388908 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1388908)\u001B[0m W0000 00:00:1745295181.017266 1388908 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1388908)\u001B[0m W0000 00:00:1745295181.017267 1388908 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1388908)\u001B[0m 2025-04-22 01:13:01.020115: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001B[36m(pid=1388908)\u001B[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1388908)\u001B[0m Cargando datasets desde el directorio‚Ä¶\n",
      "\u001B[36m(train_fn pid=1388908)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1388908)\u001B[0m Cargando dataset de entrenamiento desde:\n",
      "\u001B[36m(train_fn pid=1388908)\u001B[0m  > /home/juan/.cache/kagglehub/datasets/abdallahalidev/plantvillage-dataset/splitted/train/\n",
      "\u001B[36m(train_fn pid=1388908)\u001B[0m Found 109718 files belonging to 38 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1388908)\u001B[0m I0000 00:00:1745295184.471174 1388994 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6133 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1388908)\u001B[0m ‚úÖ Dataset cargado exitosamente.\n",
      "\u001B[36m(train_fn pid=1388908)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1388908)\u001B[0m Cargando dataset de test desde:\n",
      "\u001B[36m(train_fn pid=1388908)\u001B[0m  > /home/juan/.cache/kagglehub/datasets/abdallahalidev/plantvillage-dataset/splitted/test/\n",
      "\u001B[36m(train_fn pid=1388908)\u001B[0m Found 10861 files belonging to 38 classes.\n",
      "\u001B[36m(train_fn pid=1388908)\u001B[0m ‚úÖ Dataset cargado exitosamente.\n",
      "\u001B[36m(train_fn pid=1388908)\u001B[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1388908)\u001B[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001B[36m(train_fn pid=1388908)\u001B[0m I0000 00:00:1745295186.030584 1389104 service.cc:152] XLA service 0x7a5cb8002190 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001B[36m(train_fn pid=1388908)\u001B[0m I0000 00:00:1745295186.030665 1389104 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Laptop GPU, Compute Capability 8.9\n",
      "\u001B[36m(train_fn pid=1388908)\u001B[0m 2025-04-22 01:13:06.046348: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001B[36m(train_fn pid=1388908)\u001B[0m I0000 00:00:1745295186.102104 1389104 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "\u001B[36m(train_fn pid=1388908)\u001B[0m 2025-04-22 01:13:07.051465: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_661', 84 bytes spill stores, 84 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1388908)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1388908)\u001B[0m 2025-04-22 01:13:07.139122: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_716', 28 bytes spill stores, 28 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1388908)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1388908)\u001B[0m I0000 00:00:1745295188.730526 1389104 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001B[36m(train_fn pid=1388908)\u001B[0m 2025-04-22 01:14:49.529966: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_661', 64 bytes spill stores, 64 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1388908)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1388908)\u001B[0m 2025-04-22 01:14:49.596876: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_716', 48 bytes spill stores, 48 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1388908)\u001B[0m \n",
      "\u001B[36m(pid=1390088)\u001B[0m 2025-04-22 01:18:23.018295: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001B[36m(pid=1390088)\u001B[0m 2025-04-22 01:18:23.028889: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001B[36m(pid=1390088)\u001B[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001B[36m(pid=1390088)\u001B[0m E0000 00:00:1745295503.039816 1390088 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001B[36m(pid=1390088)\u001B[0m E0000 00:00:1745295503.043110 1390088 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001B[36m(pid=1390088)\u001B[0m W0000 00:00:1745295503.052785 1390088 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1390088)\u001B[0m W0000 00:00:1745295503.052798 1390088 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1390088)\u001B[0m W0000 00:00:1745295503.052800 1390088 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1390088)\u001B[0m W0000 00:00:1745295503.052801 1390088 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001B[36m(pid=1390088)\u001B[0m 2025-04-22 01:18:23.055700: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001B[36m(pid=1390088)\u001B[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1390088)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1390088)\u001B[0m Cargando datasets desde el directorio‚Ä¶\n",
      "\u001B[36m(train_fn pid=1390088)\u001B[0m Cargando dataset de entrenamiento desde:\n",
      "\u001B[36m(train_fn pid=1390088)\u001B[0m  > /home/juan/.cache/kagglehub/datasets/abdallahalidev/plantvillage-dataset/splitted/train/\n",
      "\u001B[36m(train_fn pid=1390088)\u001B[0m Found 109718 files belonging to 38 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1390088)\u001B[0m I0000 00:00:1745295506.318976 1390176 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6133 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1390088)\u001B[0m ‚úÖ Dataset cargado exitosamente.\n",
      "\u001B[36m(train_fn pid=1390088)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1390088)\u001B[0m Cargando dataset de test desde:\n",
      "\u001B[36m(train_fn pid=1390088)\u001B[0m  > /home/juan/.cache/kagglehub/datasets/abdallahalidev/plantvillage-dataset/splitted/test/\n",
      "\u001B[36m(train_fn pid=1390088)\u001B[0m Found 10861 files belonging to 38 classes.\n",
      "\u001B[36m(train_fn pid=1390088)\u001B[0m ‚úÖ Dataset cargado exitosamente.\n",
      "\u001B[36m(train_fn pid=1390088)\u001B[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(train_fn pid=1390088)\u001B[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001B[36m(train_fn pid=1390088)\u001B[0m I0000 00:00:1745295507.892078 1390283 service.cc:152] XLA service 0x7c95780151f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001B[36m(train_fn pid=1390088)\u001B[0m I0000 00:00:1745295507.892163 1390283 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Laptop GPU, Compute Capability 8.9\n",
      "\u001B[36m(train_fn pid=1390088)\u001B[0m 2025-04-22 01:18:27.910618: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001B[36m(train_fn pid=1390088)\u001B[0m I0000 00:00:1745295507.971646 1390283 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "\u001B[36m(train_fn pid=1390088)\u001B[0m 2025-04-22 01:18:28.690130: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_661', 84 bytes spill stores, 84 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1390088)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1390088)\u001B[0m 2025-04-22 01:18:29.116139: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_716', 28 bytes spill stores, 28 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1390088)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1390088)\u001B[0m I0000 00:00:1745295510.785572 1390283 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001B[36m(train_fn pid=1390088)\u001B[0m 2025-04-22 01:20:10.483789: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_661', 64 bytes spill stores, 64 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1390088)\u001B[0m \n",
      "\u001B[36m(train_fn pid=1390088)\u001B[0m 2025-04-22 01:20:10.617825: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_716', 48 bytes spill stores, 48 bytes spill loads\n",
      "\u001B[36m(train_fn pid=1390088)\u001B[0m \n",
      "2025-04-22 01:23:40,694\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/juan/ray_results/train_fn_2025-04-22_00-30-13' in 0.0032s.\n",
      "2025-04-22 01:23:40,696\tINFO tune.py:1041 -- Total run time: 3207.62 seconds (3207.61 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found were:  {'filters1': 32, 'filters2': 64, 'filters3': 128, 'filters4': 256, 'dropout_rate': 0.09928777465197672, 'dense_units': 512, 'learning_rate': 0.00013985928027592968}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "learning_rate_best_config = 0.00013985928027592968\n",
    "dropout_rate_best_config = 0.09928777465197672"
   ],
   "metadata": {
    "id": "xf9ypO99u8nc"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
